<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>多视图几何（五）</title>
      <link href="/2022/02/28/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E4%BA%94%EF%BC%89/"/>
      <url>/2022/02/28/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E4%BA%94%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter5-Reconstruction-from-Two-Views"><a href="#Chapter5-Reconstruction-from-Two-Views" class="headerlink" title="Chapter5 Reconstruction from Two Views"></a>Chapter5 Reconstruction from Two Views</h1><h2 id="5-1-Problem-Formulation"><a href="#5-1-Problem-Formulation" class="headerlink" title="5.1. Problem Formulation"></a>5.1. Problem Formulation</h2><p>In the last sections, we discussed how to identify point correspondences between two consecutive frames. In this section, we will trackle the next next problem, namely that of <code>reconstructing the 3D geometry of cameras and points</code>.<br>To this end, we will make the following assumptions:</p><ul><li>We assume that we are given a <code>set of corresponding points</code> in two frames taken with the same camera from different vantage points.</li><li>We assume that <code>the scene is static</code>, i.e. none of the observed 3D points moved during the camera motion.</li><li>We also assume that the <code>intrinsic camera (calibration) parameter are known</code>.</li></ul><p>We will first estimate the <code>camera motion</code> from the set of corresponding points. Once we know the relative location and orientation of the cameras, we can reconstruct the 3D location of all corresponding points by <code>triangulation</code>.</p><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/ce7a66d1ad07ba3805814dab6939ac3c.png"></p><p>Goal: Estimate camera motion and 3D scene structure from two views.</p><h2 id="5-2-The-Reconstruction-Problem"><a href="#5-2-The-Reconstruction-Problem" class="headerlink" title="5.2 The Reconstruction Problem"></a>5.2 The Reconstruction Problem</h2><p>In general 3D reconstruction is a challenging problem. If we are given two views with 100 feature points in each of them, then we have 200 point coordinates in 2D. The goal is to estimate</p><ul><li>6 parameters modeling the camera motion $R, T$ and</li><li>$100\times 3$ coordinates for the 3D points $X_j$.</li></ul><p>This could be done by minimizing the <code>projection error</code>:<br>$$<br>E(R, T, X_1, …,X_{100}) &#x3D; \sum_j||x_1^j - \pi(X_j)||^2 + ||x_2^j-\pi(R, T, X_j)||^2.<br>$$</p><p>This amounts to a <code>difficult optimization problem</code> called <code>bundle adjustment</code>.</p><p>Before we look into this problem, we will first study an elegant solution to entirely get rid of the 3D point coordinates. It leads to the well-known 8-point algorithm.</p><h2 id="5-3-Epipolar-Geometry-Some-Notation"><a href="#5-3-Epipolar-Geometry-Some-Notation" class="headerlink" title="5.3 Epipolar Geometry: Some Notation"></a>5.3 Epipolar Geometry: Some Notation</h2><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/7b5fd6359a9d025a7abfc91546279e99.png"></p><p>The projections of a point $X$ onto the two images are denoted by $x_1$ and $x_2$. The <code>optical centers</code> of each camera are denoted by $o_1$ and $o_2$. The intersections of the line $(o_1, o_2)$ with each image plane are called the <code>epipoles</code> $e_1$ and $e_2$. The intersections between the <code>epipolar plane </code> $(o_1, o_2, X)$ and the image planes are called <code>epipolar lines </code> $l_1$ and $l_2$. There is one epipolar plane for each 3D point $X$.</p><h2 id="5-4-The-Epipolar-Constraint"><a href="#5-4-The-Epipolar-Constraint" class="headerlink" title="5.4 The Epipolar Constraint"></a>5.4 The Epipolar Constraint</h2><p>We know that $x_1$ (in homogeneous coordinates) is the projection of a 3D point $X$. Given known camera parameters $(K&#x3D;1)$ and no rotation or translation of the first camera, we merrly have a projection with unknown depth $\lambda_1$. From the first to the second frame we additionally have a camera rotation $R$ and a translation $T$ followed by a projection. This gives the equations:<br>$$<br>\lambda_1 x_1 &#x3D; X, \quad \lambda_2 x_2 &#x3D; RX+T.<br>$$</p><p>Inserting the first equation into the second, we get:<br>$$<br>\lambda_2 x_2 &#x3D; R(\lambda_1 x_1) + T.<br>$$</p><p>Now we remove the translation by multiplying with $\hat{T}$ $(\hat{T}v \equiv T\times v)$:(两边同时乘以$\hat{T}$这个反对称矩阵,而$\hat{T}T$相当于向量$T$自己和自己做叉乘，即为0向量，这样一来，就可以把上式中的加法去掉)<br>$$<br>\lambda_2 \hat{T}x_2 &#x3D; \lambda_1 \hat{T}Rx_1<br>$$</p><p>And projection onto $x_2$ gives the <code>epipolar constraint</code>:(把两边的向量都投影到$x_2$方向上时，即两边的向量同时和$x_2$向量做内积，也就是两边同时乘以向量$x_2^T$，那左边的向量$\hat{T}x_2&#x3D;T\times x_2$本身与$x_2$正交，故内积为标量0，而右边就只剩下$\lambda_1 x_2^T\hat{T}Rx_1$，进而$\lambda_1 \neq 0$表示我们观察的3D物体点不在相机的光心处，所以可以把$\lambda_1$约去，最终就剩下了下面的式子：)<br>$$<br>x_2^T \hat{T}Rx_1 &#x3D; 0 \quad( x_1 和x_2都是2D坐标的齐次形式(x,y,1))<br>$$</p><p>The epipolar constraint:<br>$$<br>x_2^T \hat{T}Rx_1 &#x3D; 0<br>$$<br>provides a <code>relation between the 2D point coordinates</code> of a 3D point in each of the two images <code>and  the camera transformation parameters</code>. The original 3D point coordinates have been removed. The matrix:<br>$$<br>E &#x3D; \hat{T}R \in \mathbb{R}^{3\times 3}<br>$$<br>is called the <code>essential matrix</code>. The <code>epipolar constraint</code> is also known as <code>essential constraint</code> or <code>bilinear constraint</code>. <code>Geometrically</code>, this constraint states that the three vectors $\overrightarrow{o_1 X}$, $\overrightarrow{o_2o_1}$ and $\overrightarrow{o_2X}$ form a plane, i.e. the triple product of those vectors (measuring the volume of the parallelpiped) is zero: In coordinates of the second frame $Rx_1$ gives the direction of the vector $\overrightarrow{o_1X}$; $T$ gives the direction of $\overrightarrow{o_2o_1}$, and $x_2$ is proportional to the vector $\overrightarrow{o_2X}$ such that:<br>$$<br>volume &#x3D; x_2^T (T\times Rx_1) &#x3D; 0<br>$$</p><h2 id="5-5-Properties-of-the-Essential-Matrix-E"><a href="#5-5-Properties-of-the-Essential-Matrix-E" class="headerlink" title="5.5 Properties of the Essential Matrix E"></a>5.5 Properties of the Essential Matrix E</h2><p>The space of all essential matrices is called the <code>essential space</code>:<br>$$<br>\varepsilon \equiv \{\hat{T}R | R\in SO(3), T\in \mathbb{R}^3\} \subset \mathbb{R}^{3\times 3}<br>$$<br><code>Theorem [Huang &amp; Faugeras, 1989] (Characterization of the essential matrix)</code> A nonzero matrix $E\in \mathbb{R}^{3\times 3}$ is an essential matrix if and only if $E$ has a singular value decomposition (SVD)$E &#x3D; U\Sigma V^T$ with:<br>$$<br>\Sigma &#x3D; diag{\sigma, \sigma, 0}<br>$$<br>for some $\sigma &gt; 0$ and $U, V \in SO(3)$.</p><p><code>Theorem (Pose recovery from the essential matrix):</code> There exist exactly two relative poses $(R, T)$ with $R\in SO(3)$ and $T\in \mathbb{R}^3$ corresponding to an essential matrix $E\in \varepsilon$. For $E&#x3D;U\Sigma V^T$ we have:</p><p>$$<br>(\hat{T_1}, R_1) &#x3D; (UR_Z(+\frac{\pi}{2})\Sigma U^T,  UR_Z(+\frac{\pi}{2})\Sigma V^T), \<br>$$</p><p>$$<br>(\hat{T_2}, R_2) &#x3D; (UR_Z(-\frac{\pi}{2})\Sigma U^T,  UR_Z(-\frac{\pi}{2})\Sigma V^T).<br>$$</p><p>(式中我们定义的$R_Z(\theta)&#x3D;\left(\begin{matrix}<br>    cos(\theta) &amp;-sin(\theta) &amp;0 \\<br>    sin(\theta) &amp;cos(\theta) &amp;0\\<br>    0 &amp;0 &amp; 0<br>\end{matrix}\right)$, 所以<br>$R_Z(+\frac{\pi}{2})&#x3D;\left(\begin{matrix}<br>    0 &amp; -1 &amp;0 \\<br>    1 &amp;0 &amp;0\\<br>    0 &amp;0 &amp; 0<br>\end{matrix}\right)$, $R_Z(-\frac{\pi}{2})&#x3D;\left(\begin{matrix}<br>    0 &amp; 1 &amp;0 \\<br>    -1 &amp;0 &amp;0\\<br>    0 &amp;0 &amp; 0<br>\end{matrix}\right)$)</p><p>In general, only one of these gives meaningful (positive) depth values.</p><h2 id="5-6-A-Basic-Reconstruction-Algorithm"><a href="#5-6-A-Basic-Reconstruction-Algorithm" class="headerlink" title="5.6 A Basic Reconstruction Algorithm"></a>5.6 A Basic Reconstruction Algorithm</h2><p>We have seen that the 2D-coordinates of each 3D point are coupled to the camera parameters $R$ and $T$ through an epipolar constraint. In the following, we will derive a 3D reconstruction algorithm which proceeds as follows:</p><ul><li><code>Recover the essential matrix</code> $E$ from the epipolar constraints associated with a set of point pairs.</li><li><code>Extract the relative translation and rotation</code> from the essential matrix $E$. (本质矩阵的前两个特征值相等，且第三个特征值为0)</li></ul><p>In general, the matrix $E recovered from a set of epipolar constrains will not be an essential matrix. One can resolve this problem in two ways:</p><ul><li>Recover some matrix $E\in\mathbb{R}^{3\times 3}$ from the epipolar constraints and then project it onto the essential space.</li><li>Optimize the epipolar constraints in the essential space.</li></ul><p>While the second approach is in principle more accurate it involves a nonlinear constrained optimization. So we will pursue the first approach which is simpler and faster.</p><h2 id="5-7-The-Eight-Point-Linear-Algorithm"><a href="#5-7-The-Eight-Point-Linear-Algorithm" class="headerlink" title="5.7 The Eight-Point Linear Algorithm"></a>5.7 The Eight-Point Linear Algorithm</h2><p>First we rewrite the epipolar constrainr as a scalar product in the elements of the matrix $E$ and the coordinates of the points $\textbf{x}_1 $ and $\textbf{x}_2$. Let </p><p>$$<br>E^s &#x3D; (\theta_{11}, \theta_{21}, \theta_{31},<br>       \theta_{12}, \theta_{22}, \theta_{32},<br>       \theta_{13}, \theta_{23}, \theta_{33})^T \in \mathbb{R}^9<br>$$</p><p>be the vector of elements of $E$ and</p><p>$$<br>a &#x3D; \textbf{x}_1 \otimes \textbf{x}_2<br>$$</p><p>the <code>Kronecker product</code> of the vectors $\textbf{x}_i &#x3D; (x_i, y_i, z_i)$, defined as $a &#x3D; (x_1x_2, x_1y_2, x_1z_2, y_1x_2, y_1y_2, y_1z_2, z_1x_2, z_1y_2, z_1z_2)^T \in \mathbb{R}^9$.</p><p>Then the epipolar constraint can be written as:</p><p>$$<br>\textbf{x}_2^TE\textbf{x}_1 &#x3D; a^TE^s &#x3D; 0.<br>$$</p><p>For $n$ point pairs, we can combine this into the linear system:</p><p>$$<br>\chi E^s &#x3D; 0, \quad \chi &#x3D; (a^1, a^2, …, a^n)^T.<br>$$</p><p>According to </p><p>$$<br>\chi E^s &#x3D; 0, \quad with \chi &#x3D; (a^1,a^2, …, a^n)^T.<br>$$</p><p>we see that the vector of coefficients of the essential matrix $E$ defines the <code>null space of the matrix</code> $\chi$. In order for the abover system to have a unique solution (up to a scaling factor and ruling out the trivial solution $E&#x3D;0$), the <code>rank of the matrix</code> $\chi$ <code>needs to be exactly 8</code>. Therefore we need at least 8 point pairs.</p><p>In certain <code>degenerate cases</code>, the solution for the essential matrix is not unique even if we have 8 or more point pairs. One such example is the case that all points lie on the a line or on a plane.<br>Clearly, we will not be able to recover the sign of $E$. Since with each $E$, there are two possible assignments of rotation $R$ and translation $T$, we therefore end up with four possible solutions for rotation and translation.</p><h2 id="5-8-Projection-onto-Essential-Space"><a href="#5-8-Projection-onto-Essential-Space" class="headerlink" title="5.8 Projection onto Essential Space"></a>5.8 Projection onto Essential Space</h2><p>The numerically estimated coefficients $E^s$ will in general not correspond to an essential matrix. One can resolve this problem by projecting it back to the essential space:</p><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/504311d8792556835b660121b51b9990.png"></p><p><code>Theorem (Projection onto essential space)</code>: Let $F \in \mathbb{R}^{3\times 3}$ be an arbitrary matrix with SVD<br>$F &#x3D; Udiag{\lambda_1, \lambda_2, \lambda_3}V^T, \lambda_1\geq \lambda_2\geq \lambda_3$. Then the essential matrix $E$ which minimizes the Frobenius norm $||F-E||^2_t$ is given by:</p><p>$$<br>E &#x3D; Udiag{\sigma ,\sigma, 0 }V^T, \quad with \ \sigma&#x3D;\frac{\lambda_1 + \lambda_2}{2}.<br>$$</p><h2 id="5-9-Eight-Point-Algorithm-Longuet-Higgins-1981"><a href="#5-9-Eight-Point-Algorithm-Longuet-Higgins-1981" class="headerlink" title="5.9 Eight Point Algorithm (Longuet-Higgins 1981)"></a>5.9 Eight Point Algorithm (Longuet-Higgins 1981)</h2><p>Given a set of $n&#x3D;8$ or more point pairs $\textbf{x}_1^i$, $\textbf{x}_2^i$:</p><ul><li><p><code>Compute an approximation of the essential matrix</code>.Construct the matrix $\chi &#x3D; (a^1, a^2, …, a^n)^T$, where $a^i &#x3D; \textbf{x}<em>1^i \otimes \textbf{x}<em>2^i$. Find the vector $E^s\in \mathbb{R}^9$ which minimizes $||\chi E^s||$ as the ninth column of $V</em>{\chi}$ in the SVD $\chi &#x3D;U</em>{\chi}\Sigma_{\chi}V^T_{\chi}$. Unstack $E^s$ into $3\times 3$-matrix $E$.</p></li><li><p><code>Project onto essential space</code>. Compute the SVD $E&#x3D;Udiag{\sigma_1, \sigma_2, \sigma_3}V^T$. Since in the reconstruction, $E$ is only defined up to a scalar, we project $E$ onto the <code>normlized essential space</code> by replacing the singular values $\sigma_1, \sigma_2, \sigma_3$ with 1, 1, 0.</p></li><li><p><code>Recover the displacement from the essential matrix</code>. The four possible solutions for rotation and translation are:</p><p>  $$<br>  R &#x3D; UR_Z^T(\pm \frac{\pi}{2})V^T, \quad \hat{T} &#x3D; UR_Z(\pm \frac{\pi}{2})\Sigma U^T,<br>  $$</p><p>  with a rotation by $\pm \frac{\pi}{2}$ around $z$:</p><p>  $$<br>  R_Z^T(\pm \frac{\pi}{2}) &#x3D; \left(\begin{matrix}<br>  0 &amp;\pm 1 &amp; 0 \\<br>  \mp 1 &amp; 0 &amp; 0 \\<br>  0 &amp; 0 &amp; 1<br>  \end{matrix} \right).<br>  $$</p></li></ul><h2 id="5-10-Do-we-need-Eight-Points-用5点法也可求解"><a href="#5-10-Do-we-need-Eight-Points-用5点法也可求解" class="headerlink" title="5.10 Do we need Eight Points? (用5点法也可求解)"></a>5.10 Do we need Eight Points? (用5点法也可求解)</h2><p>The above reasoning showed that we need at least eight points in order for the matrix $\chi$ to have rank 8 and therefore guarantee a unique solution for $E$. Yet, one can take into account the special structure of $E$. <code>The space of essential matrices is actually a five dimensional space</code>, i.e. $E$ only has 5 (and not 9) degrees of freedom.</p><p>A simple way to <code>take into account the algebraic properties of</code> $E$ is to make use of the fact that $det(E)&#x3D;0$. If now we have only 7 point pairs, the null space of $\chi$ will have (at least) Dimension 2, spanned by two vectors $E_1$ and $E_2$. Then we can solve for $E$ by determining $\alpha$ such that:<br>$$<br>det(E) &#x3D; det(E_1 + \alpha E_2) &#x3D; 0.<br>$$</p><p>Along similar lines, <code>Kruppa proved in 1913 that one needs only five point pairs to recover</code> $(R, T)$. In the case of degenerate motion (for example planar or circular motion), one can resolve the problem with even fewer point pairs.</p><h2 id="5-11-Limitations-and-Further-Extensions"><a href="#5-11-Limitations-and-Further-Extensions" class="headerlink" title="5.11 Limitations and Further Extensions"></a>5.11 Limitations and Further Extensions</h2><p>Among the four possible solutions for $R$ and $T$, there is generally <code>only one meaningful one</code> (which assigns positive depth to all points).</p><p><code>The algorithm fairs if the translation is exactly 0</code>, since then $E&#x3D;0$ and nothing can be recovered. Due to noise this typically does not happen.</p><p>In the case of infiitesimal view point change, one can adapt the eight point algorithm to the <code>continuous motion case</code>, where the epipolar constraint is replaced by the <code>continuous epipolar constraint</code>. Rather than recovering $(R, T)$ one recovers the linear and angular velocity of the camera.</p><p>In the case of independently moving objects, one can generalize the epipolar constraint. For two motions for example we have:</p><p>$$<br>(x_2^TE_1x_1)(x_2^TE_2x_1) &#x3D; 0<br>$$</p><p>with two essential matrices $E_1$ and $E_2$. Given a sufficiently large number of point pairs, one can solve the respective equations for multiple essential matrices using polynomial factorization.</p><h2 id="5-12-Structure-Reconstruction"><a href="#5-12-Structure-Reconstruction" class="headerlink" title="5.12 Structure Reconstruction"></a>5.12 Structure Reconstruction</h2><p>The linear eight-point algorithm allowed us to estimate the camera transformation parameters $R$ and $T$ from a set of corresponding point pairs. Yet, the essential matrix $E$ and hence the translation $T$ are <code>only defined up to an arbitrary scale </code>$\gamma \in \mathbb{R}^{+}$, with $||E||&#x3D;||T||&#x3D;1$. After recovering $R$ and $T$, we therefore have for point $X^j$:</p><p>$$<br>\lambda_2^j \textbf{x}_2^j &#x3D; \lambda_1^j R\textbf{x}_1^j + \gamma T, \quad j&#x3D;1, …, n,<br>$$</p><p>with unknown scale parameters $\lambda_i^j$. We can eliminate one of these scales by applying $\widehat{\textbf{x}_2^j}$:</p><p>$$<br>\lambda_1^j \widehat{\textbf{x}_2^j}R\textbf{x}_1^j + \gamma\widehat{\textbf{x}_2^j}T &#x3D; 0, \quad j&#x3D;1,…,n.<br>$$</p><p>This corresponds to $n$ <code>linear systems</code> of the form:</p><p>$$<br>(\widehat{\textbf{x}_2^j}R\textbf{x}_1^j, \widehat{\textbf{x}_2^j}T)\left(\begin{matrix}<br>    \lambda_1^j \\<br>    \gamma<br>\end{matrix}\right) &#x3D; 0, \quad j&#x3D;1,…,n.<br>$$</p><p>Combining the parameters $\vec{\lambda}&#x3D;(\lambda_1^1, \lambda_1^2,…,\lambda_1^n, \gamma)^T\in \mathbb{R}^{n+1}$, we get the linear equation system</p><p>$$<br>M\vec{\lambda} &#x3D; 0<br>$$</p><p>with</p><p>$$<br>M \equiv \left(\begin{matrix}<br>    \widehat{\textbf{x}_2^1}R\textbf{x}_1^1 &amp;0 &amp; 0&amp; 0 &amp; 0 &amp; \widehat{\textbf{x}_2^1}T\\<br>    0 &amp;\widehat{\textbf{x}_2^2}R\textbf{x}_1^2 &amp; 0&amp; 0 &amp; 0 &amp; \widehat{\textbf{x}_2^2}T\\<br>    0 &amp; 0 &amp;\ddots &amp;0 &amp; 0 &amp;\vdots \\<br>    0 &amp;0 &amp; 0&amp; \widehat{\textbf{x}_2^{n-1}}R\textbf{x}_1^{n-1} &amp; 0 &amp; \widehat{\textbf{x}_2^{n-1}}T\\<br>    0 &amp; 0 &amp; 0&amp; 0 &amp; \widehat{\textbf{x}_2^n}R\textbf{x}_1^n &amp; \widehat{\textbf{x}_2^n}T\\<br>\end{matrix}\right)<br>$$<br>(求解的优化问题是:</p><p>$$<br>\min_{\vec{\lambda}} ||M\vec{\lambda}||^2 &#x3D; \min_{\vec{\lambda}}\lambda^TM^TM\lambda, \quad ||\lambda||&#x3D;1.<br>$$</p><p>Note: 进一步，我们让优化目标函数对$\vec{\lambda}$求导数，导数最小处就是$\vec{\lambda}$的解。令导数为0有: $2M^TM\vec{\lambda}&#x3D;0$，进一步，由于矩阵可以通过其特征值进行等价代换，即著名的$Ax&#x3D;\lambda x$中，一个矩阵$A$乘以他的一个特征向量$x$等于该特征向量对应的特征值$\lambda$与该特征向量$x$的乘积,因此可以将上面的导数$2M^TM\vec{\lambda}&#x3D;0$进行变换，假设矩阵$M^TM$的一个最小的一个接近于0的特征值是$\eta$，且该特征值对应的特征向量为$p$,即$M^TM p &#x3D; \eta p$,那么上面的导数就可以被估计成$2M^TMp &#x3D; \eta p \approx 0$,也就是说，原来优化问题的解$\vec{\lambda}$近似是矩阵$M^TM$的最小特征值对应的特征向量，求解完毕。 )</p><p>The linear least squares estimate for $\vec{\lambda}$ is given by the eigenvector corresponding to the smallest eigenvalue of $M^TM$. It is <code>only defined up to a global scale</code>. It reflects the <code>ambiguity</code> that the camera has moved twice the distance, the scane is twice larger and twice as far away.</p><h2 id="5-13-Example"><a href="#5-13-Example" class="headerlink" title="5.13 Example"></a>5.13 Example</h2><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/cc3c6012ccdec7463443384d1b416adc.png"></p><h2 id="5-14-Optimality-in-Noisy-Real-World-Conditions"><a href="#5-14-Optimality-in-Noisy-Real-World-Conditions" class="headerlink" title="5.14 Optimality in Noisy Real World Conditions"></a>5.14 Optimality in Noisy Real World Conditions</h2><p>The eight-point algorithm discussed before has several nice properties. In particular, we found <code>closed-form solutions</code> to estimate the camera parameters and the 3D structure, based on <code>singular value decomposition</code>. However, if we have noisy data ${\tilde{\textbf{x}_1}}$, ${\tilde{\textbf{x}}_2}$ (correspondences not exact or even incorrect), then we have:</p><ul><li>no quarantee that $R$ and $T$ are as close as possible to the true solution.</li><li>no guarantee that we will get a consistent reconstruction.</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/7b5fd6359a9d025a7abfc91546279e99.png"></p><h2 id="5-15-Nonlinear-Optimization-Methods"><a href="#5-15-Nonlinear-Optimization-Methods" class="headerlink" title="5.15 Nonlinear Optimization Methods"></a>5.15 Nonlinear Optimization Methods</h2><p>In order to take noise and statistical fluctutation into account, one can revert to a <code>Bayesian formulation</code> and determine the most likely camera transformlation $R, T$ and ‘true’ 2D coordinates $\textbf{x}$ given the measured coordinates $\tilde{\textbf{x}}$, by performing a <code>maximum a posteriori estimate</code>:</p><p>$$<br>\\argmax_{\textbf{x}, R, T}\mathcal{P}(\textbf{x}, R, T|{\tilde{\textbf{x}}})&#x3D;\\argmax_{\textbf{x}, R, T}\mathcal{P}(\tilde{\textbf{x}}|\textbf{x}, R, T)\mathcal{P}(\textbf{x}, R, T)<br>$$</p><p>This approach will however involve modeling probability densities $\mathcal{P}$ on the fairly complicated space $SO(3)\times \mathbb{S}^2$ of rotation and translation parameters, as $R\in SO(3)$ and $T\in \mathbb{S}^2$ (3D translation with unit length).</p><p>Alternatively, one can perform a <code>constrained optimization</code> by minimizing a cost function (similarity to measurements):</p><p>$$<br>\phi(\textbf{x}, R, T) &#x3D; \sum_{j&#x3D;1}^n \sum_{i&#x3D;1}^2 ||\tilde{\textbf{x}_i^j}-\textbf{x}_i^j||^2<br>$$</p><p>suject to (consistent geometry):</p><p>$$<br>\textbf{x}_2^{jT}\hat{T}R\textbf{x}_1^j &#x3D; 0, \quad \textbf{x}_1^{jT}e_3&#x3D;1, \quad \textbf{x}_2^{jT}e_3 &#x3D; 1, j&#x3D;1,2,…,n. (where \ e_3&#x3D;(0,0,1))<br>$$</p><h2 id="5-16-Bundle-Adjustment"><a href="#5-16-Bundle-Adjustment" class="headerlink" title="5.16 Bundle Adjustment"></a>5.16 Bundle Adjustment</h2><p>Interestingly, the unknown depth parameters $\lambda_i$ <code>do not actually appear in the above cost functions</code>.<br>The depth parameters appear directly in the <code>unconstrained optimization problem</code>:</p><p>$$<br>\sum_{j&#x3D;1}^n ||{\textbf{x}_1^j}-\pi_1(\textbf{X}{^j})||^2 + ||\tilde{\textbf{x}_2^j}-\pi_2(\textbf{X}{^j})||^2,<br>$$</p><p>where $\pi_i$denotes the projections onto the two images. Expressed in coordinates of the first camera frame, this is equal to the cost function:</p><p>$$<br>\phi(\textbf{x}<em>1, R, T, \lambda) &#x3D; \sum</em>{j&#x3D;1}^n ||\tilde{\textbf{x}_1^j}-\textbf{x}_1^j||^2 + ||\tilde{\textbf{x}_2^j} - \pi(R\lambda_1^j\textbf{x}_1^j+T)||^2.<br>$$</p><p>This optimization procedure is known as <code>bundle adjustment</code>. The constrained optimization and the unconstrained bundle adjustment can be seen as <code>different parameterizations of the same optimization objective</code>.</p><h2 id="5-17-Degenerate-Configurations"><a href="#5-17-Degenerate-Configurations" class="headerlink" title="5.17 Degenerate Configurations"></a>5.17 Degenerate Configurations</h2><p>The eight-point algorithm only provides unique solutions (up to a scalar factor) if all 3D points are in a “general position”. This is no longer the case for certain <code>degenerate configurations</code>, for which all points lie on certain 2D surfaces which are called <code>critical surfaces</code>.</p><p>While most critical configurations do not actually arise in practice, a specific degenerate configuration which does arise often is the case that <code>all points lie on 2 2D plane</code> (such as floors, table, walls,…).</p><p>For the structure-from-motion problem in the context of points on a plane, one can exploit additional constraints which leads to the so-called <code>four-point algorithm</code>.</p><h2 id="5-18-Planar-Homographies"><a href="#5-18-Planar-Homographies" class="headerlink" title="5.18 Planar Homographies"></a>5.18 Planar Homographies</h2><p>Let us assume that all points lie on a plane. If $\textbf{X}_1\in\mathbb{R}^3$ denotes the point coordinates in the first frame, and these lie on a plane with normal vector $N\in \mathbb{S}^2$, then we have:</p><p>$$<br>N^T\textbf{X}_1 &#x3D; d \Leftrightarrow\frac{1}{d}N^T\textbf{X}_1 &#x3D; 1.<br>$$</p><p>In frame two, we therefore have the coordinates:</p><p>$$<br>\textbf{X}_2&#x3D;R\textbf{X}_1 +T &#x3D; R\textbf{X}_1 + T\frac{1}{d}N^T\textbf{X}_1 &#x3D; (R+\frac{1}{d}TN^T)\textbf{X}_1 \equiv H\textbf{X}_1,<br>$$</p><p>where</p><p>$$<br>H &#x3D; R + \frac{1}{d}TN^T \in \mathbb{R}^{3\times 3}<br>$$</p><p>is called a <code>homography matrix</code>. Inserting the 2D coordinates, we get:<br>$$</p><p>\lambda_2 \textbf{x}_2 &#x3D; H\lambda_1\textbf{x}_1 \Leftrightarrow\textbf{x}_2\sim H\textbf{x}_1.<br>$$</p><p>where $\sim$ means equality up to scaling. This expression is called a <code>planar homography</code>. $H$ depends on camera and plane parameters.</p><h2 id="5-19-From-Point-Pairs-to-Homography"><a href="#5-19-From-Point-Pairs-to-Homography" class="headerlink" title="5.19 From Point Pairs to Homography"></a>5.19 From Point Pairs to Homography</h2><p>For a pair of corresponding 2D points we therefore have:</p><p>$$<br>\lambda_2 \textbf{x}_2 &#x3D; H\lambda_1 \textbf{x}_1.<br>$$</p><p>By multiplying with $\widehat{\textbf{x}_2}$ we can eliminate $\lambda_2$ and obtain:</p><p>$$<br>\widehat{\textbf{x}_2}H\textbf{x}_1 &#x3D; 0<br>$$</p><p>This equation is called the <code>planar epipolar constraint</code> or <code>planar homography constraint</code>.</p><p>Again, we can cast this equation into the form:</p><p>$$<br>\textbf{a}^TH^s &#x3D; \vec{0} \in \mathbb{R}^{3\times 1}<br>$$</p><p>where we have stacked the elements of $H$ into a vector:</p><p>$$<br>H^s &#x3D; (H_{11}, H_{21}, …, H_{33})\in \mathbb{R}^9,<br>$$</p><p>and introduced the matrix:</p><p>$$<br>\textbf{a} \equiv \textbf{x}_1 \otimes \widehat{\textbf{x}_2} \in \mathbb{R}^{9\times 3}<br>$$</p><p>(原因在于$\widehat{\textbf{x}_2}$是一个$3\times 3$的矩阵，所以得到的克罗内克乘积是一个$9\times 3$的矩阵，而不是$9\times 1$的矩阵。所以$\textbf{a}^TH^s &#x3D; \vec{0} \in \mathbb{R}^{3\times 1}$会提供给我们3个方程，但不幸的是其中只有两个方程独立，即rank&#x3D;2,因为反对称矩阵$\widehat{\textbf{x}_2}$的性质不好，其秩为2.这样下来，一对点就可以提供两个方程，此前为了估计本质矩阵，general的情况下我们需要8对点，那么现在估计单应矩阵时，general情况下我们就只需要4对点了。)</p><h2 id="5-20-The-Four-Point-Algorithm"><a href="#5-20-The-Four-Point-Algorithm" class="headerlink" title="5.20 The Four Point Algorithm"></a>5.20 The Four Point Algorithm</h2><p>Let us now assume we have $n\geq 4$ pairs of corresponding 2D points ${\textbf{x}_1^j, \textbf{x}_2^j}, j&#x3D;1,…,n$ in the two images. Each point pair induces a matrix $\textbf{a}^j$, we integrate these into a larger matrix:</p><p>$$<br>\chi \equiv (\textbf{a}^1, …, \textbf{a}^n)^T \in \mathbb{R}^{3n\times 9},<br>$$</p><p>and obtain the system:</p><p>$$<br>\chi H^s &#x3D; \vec{0} \in \mathbb{R}^{3n \times 1}<br>$$</p><p>As in the case of the essential matrix, <code>the homography matrix can be estimated up to a scale factor</code>.<br>This gives rise to the <code>four point algorithm</code>:</p><ul><li>For the point pairs, compute the matrix $\chi$.</li><li>Compute a solution $H^s$ for the above equation by singular value decomposition of $\chi$.</li><li>Extract the motion parameters from the homography matrix $H &#x3D; R + \frac{1}{d}TN^T$.(这一步中从单应矩阵$H$中恢复相机的运动$R,T$比较麻烦，但是有方法的。)</li></ul><h2 id="5-21-General-Comment"><a href="#5-21-General-Comment" class="headerlink" title="5.21 General Comment"></a>5.21 General Comment</h2><p>Clearly, the derivation of the <code>four-point algorithm</code> is in close analogy to that of the <code>eight-point algorithm</code>.</p><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/948709-20200401140923626-1417471115.png"><br>Rather than estimating the <code>essential matrix</code> $E$ one estimates the <code>homography matrix </code> $H$ to derive $R$ and $T$. In the four-point algorithm, the <code>homography matrix is decomposed into </code> $R, N$ and $T&#x2F;d$. In other words, one can reconstruct the normal of the plane, but the translation is only obtained in units of the offset $d$ of the plane and the origin.</p><p>The <code>3D structure of the points</code> can then be computed in the same manner as before.</p><p>Since one uses the strong constraint that all points lie in a plane, the <code>four-point algorithm only requires four correspondences</code>.</p><p>There exist <code>numerous relations</code> between the essential matrix $E&#x3D;\hat{T}R$ and the corresponding homography matrix $H&#x3D;R+Tu^T$ with some $u\in \mathbb{R}^3$, in particular:</p><p>$$<br>E &#x3D; \hat{T}H, \quad H^TE + E^TH &#x3D; 0.<br>$$</p><h2 id="5-22-The-Case-of-an-Uncalibrated-Camera"><a href="#5-22-The-Case-of-an-Uncalibrated-Camera" class="headerlink" title="5.22 The Case of an Uncalibrated Camera"></a>5.22 The Case of an Uncalibrated Camera</h2><p>The reconstruction algorithms introduced above all assume that the camera is calibrated (K&#x3D;1). The general transformation from a 3D point to the image is given by:</p><p>$$<br>\lambda\textbf{x}’ &#x3D; K\Pi_0 g\textbf{X} &#x3D; (KR, KT)\textbf{X},<br>$$</p><p>with the <code>intrinsic parameter matrix</code> or <code>calibration matrix</code>:</p><p>$$<br>K &#x3D; \left(\begin{matrix}<br>    fs_x &amp;s_\theta &amp; o_x\\<br>    0    &amp;fs_y     &amp; o_y\\<br>    0    &amp;0        &amp;1<br>\end{matrix}\right) \in \mathbb{R}^{3\times 3}.<br>$$</p><p>The calibration matrix maps metric coordinates into image (pixel) coordinates, using the focal length $f$, the optical center $o_x, o_y$, the pixel size $s_x, s_y$ and a skew factor $s_\theta$. If these parameters are known then one can simply <code>transform the pixel coordinates</code> $\textbf{x}’$ <code>to normalized coordinates</code> $\textbf{x} &#x3D; K^{-1}\textbf{x}’$ to obtain the representation used in the previous sections. This amounts to centering the coordinates with respect to the optical center etc. </p><h2 id="5-23-The-Fundamental-Matrix"><a href="#5-23-The-Fundamental-Matrix" class="headerlink" title="5.23 The Fundamental Matrix"></a>5.23 The Fundamental Matrix</h2><p>If the camera parameter $K$ cannot be estimated in a calibration procedure beforehand, then one has to deal with <code>reconstruction from uncalibrated views</code>.</p><p>By transforming all image coordinates $\textbf{x}’$ with the inverse calibration matrix $K^{-1}$ into metric coordinates $\textbf{x}$, we obtain the <code>epipolar constraint for uncalibrated cameras</code>:</p><p>$$<br>\textbf{x}_2^T\hat{T}R\textbf{x}_1 &#x3D; 0\Leftrightarrow \textbf{x}_2^{‘T}K^{-T}\hat{T}RK^{-1}\textbf{x}_1^{‘} &#x3D; 0<br>$$</p><p>(推广也比较直观，坐标$x_1, x_2$是在归一化坐标平面中，而坐标$x’_1, x’_2$是在真实的焦平面处，实际中将世界坐标系中的点转换为图像平面的过程中时，归一化坐标平面是在真实的焦平面之前的，故有$x‘_1 &#x3D; Kx_1, x’_2 &#x3D; Kx_2$,即$x_1 &#x3D; K^{-1}x’_1, x_2&#x3D;K^{-1}x’_2$, 代入到归一化平面中的对极约束中我们就得到了基础矩阵版本的对极约束。)<br>which can be written as </p><p>$$<br>\textbf{x}_2^{‘T}F\textbf{x’}_1 &#x3D; 0,<br>$$</p><p>with the <code>fundamental matrix</code> defined as:<br>$$<br>F \equiv K^{-T}\hat{T}RK^{-1} &#x3D; K^{-T}EK^{-1}.<br>$$</p><p>Since the invertible matrix $K$ does not affect the rank of this matrix, we know that $F$ has an SVD $F&#x3D;U\Sigma V^T$ with $\Sigma &#x3D; diag(\sigma_1, \sigma_2, 0)$. In fact, <code>any matrix of rank 2 can be a fundamental matrix</code>.</p><h2 id="5-24-Limitations-完全从基础矩阵中恢复出相机运动R-T较难实施"><a href="#5-24-Limitations-完全从基础矩阵中恢复出相机运动R-T较难实施" class="headerlink" title="5.24 Limitations (完全从基础矩阵中恢复出相机运动R,T较难实施)"></a>5.24 Limitations (完全从基础矩阵中恢复出相机运动R,T较难实施)</h2><p>While it is straight-forward to extend the eight-point algorithm, such that one can extract a <code>fundamental matrix</code> from a set of corresponding image points, it is less straight forward how to proceed from there.</p><p>Firstly, one cannot impose a strong constraint on the specific structure of the fundamental matrix (apart from the fact that the last singular value is zero).</p><p>Secondly, for a given fundamental matrix $F$, there does not exist a finite number of decompositions into extrinsic parameters $R, T$ and intrinsic parameters $K$ (even apart from the global scale factor).</p><p>As a consequence, one can only determine so-called <code>projective reconstructions</code>, i.e. reconstructions of geometry and camera position which are defined up to a so-called projective transformation.</p><p>As a solution, one typically choses a <code>canonical reconstruction</code> from the family of possible reconstructions.</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
          <category> SFM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> SFM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多视图几何（四）</title>
      <link href="/2022/02/28/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
      <url>/2022/02/28/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter-4-Estimating-Point-Correspondence"><a href="#Chapter-4-Estimating-Point-Correspondence" class="headerlink" title="Chapter 4 Estimating Point Correspondence"></a>Chapter 4 Estimating Point Correspondence</h1><h2 id="4-1-From-Photometry-to-Geometry"><a href="#4-1-From-Photometry-to-Geometry" class="headerlink" title="4.1 From Photometry to Geometry"></a>4.1 From Photometry to Geometry</h2><p>In the last sections, we discussed how points and lines are transformed from 3D world coordinates to 2D image and pixel coordinates.</p><p>In practice, <code>we do not actually observe points or lines, but rather brightness or color values</code> at the individual pixels. In order to transfer from this photometric representation to a geometric representation of the scene, one can identify points with <code>characteristic image features</code> and try to associate these points with corresponding points in the other frames.</p><p>The matching of corresponding points will allow us to infer 3D structure. Nevertheless, one should keep in mind that this approach is <code>suboptimal</code>:<code>By selecting a small number of feature points</code> from each image, we <code>throw away a large amount of potentially useful information</code> contained in each image. Yet, retaining all image information is computationally challenging. The selection and matching of a small number of feature points, on the other hand, was shown to permit tracking of 3D objects from a moving camera in real time.</p><h2 id="4-2-Example-of-tracking"><a href="#4-2-Example-of-tracking" class="headerlink" title="4.2 Example of tracking"></a>4.2 Example of tracking</h2><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/06e0e660b240fe9932572112a1b19be0.png"></p><h2 id="4-3-Identifying-Corresponding-Points"><a href="#4-3-Identifying-Corresponding-Points" class="headerlink" title="4.3 Identifying Corresponding Points"></a>4.3 Identifying Corresponding Points</h2><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/2173ef034aef7823c285db3f92ced945.png"></p><p>To identify corresponding points in two or more images is one of the biggest challenges in computer vision. Which of the points identified in the left image corresponds to which point in the right one?</p><h2 id="4-4-Non-rigid-Deformation"><a href="#4-4-Non-rigid-Deformation" class="headerlink" title="4.4 Non-rigid Deformation"></a>4.4 Non-rigid Deformation</h2><p>In what follows we will assume that objects move rigidly.<br>However, in general, objects may also deform <code>non-rigid</code>. Moreover, there may be <code>partial occlusions</code>:<br><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/ffa5ad0e2093698793330158d8f18c0e.png"></p><h2 id="4-5-Small-Deformation-versus-Wide-Baseline"><a href="#4-5-Small-Deformation-versus-Wide-Baseline" class="headerlink" title="4.5 Small Deformation versus Wide Baseline"></a>4.5 Small Deformation versus Wide Baseline</h2><p>In point matching one distinguishes two cases:</p><ul><li><code>Small deformation</code>: The deformation from one frame to the other is assumed to be (infinitesimally) small. In this case the displacement from one frame to the other can be estimated by classical <code>optical flow estimation</code>, for example using the methods of <code>Lucas/Kanade</code> or <code>Horn/Schunck</code>. In particular, these methods allow to model dense deformation fields (giving a displacement for every pixel in the image). But one can also track the displacement of a few feature points which is typically faster.</li><li><code>Wide baseline stereo</code>: In this case the displacement is assumed to be large. A dense matching of all points to all is in general computationally infeasible. Therefore, one typically selects a <code>small number of feature points</code> in each of the images and develops efficient methods to <code>find an appropriate pairing of points</code>.</li></ul><h2 id="4-6-Small-Deformation"><a href="#4-6-Small-Deformation" class="headerlink" title="4.6 Small Deformation"></a>4.6 Small Deformation</h2><p>The transformation of all points of a rigidly moving obnect is given by:<br><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/30d3d0d56c56e95291738783f22fa0c.jpg"><br>$$<br>x_2 &#x3D; h(x_1) &#x3D; \frac{1}{\lambda _2(\textbf{X})}(R\lambda_1(\textbf{X})x_1 + T).<br>$$</p><p>Locally this motion can be <code>approximated</code> in several ways.</p><ul><li><p>Translational model:<br>$$<br>h(x) &#x3D; x + b.<br>$$</p></li><li><p>Affine model:</p><p>  $$<br>  h(x) &#x3D; Ax + b.<br>  $$<br>  The 2D affine model can also be written as:<br>  $$h(x) &#x3D; x + u(x)\quad (u就是像素从当前帧到下一帧中的运动) $$<br>  with<br>  $$<br>  u(x) &#x3D; S(x)p &#x3D; \left(\begin{matrix}<br>  x &amp;y&amp;1&amp;0&amp;0&amp;0\\<br>  0&amp;0&amp;0&amp;x&amp;y&amp;1<br>  \end{matrix}\right)(p_1, p_2, p_3,p_4,p_5,p_6)^T &#x3D;<br>  \left(\begin{matrix}<br>  p_1x+p_2y+p_3\\<br>  p_4x+p_5y+p_6<br>  \end{matrix}\right).<br>  $$</p></li></ul><h2 id="4-7-Optic-Flow-Estimation"><a href="#4-7-Optic-Flow-Estimation" class="headerlink" title="4.7 Optic Flow Estimation"></a>4.7 Optic Flow Estimation</h2><p>The <code>optic flow</code> refers to the apparent 2D motion field observable between consecutive images of a video. It is different from the motion of objects in the scene, in the extreme case of motion along the camera axis, for example, there is no optic flow, while on the other hand camera rotation generates an optic flow field even for entirely static scenes.</p><p>In 1981, two seminal works on optic flow estimation were published, namely the works of <code>Lucas &amp; Kanade</code>, and of <code>Horn &amp; Schunck</code>. Both methods have become very influential with thousands of citations. They are complementary in the sense that the Lucas-Kanade method generates sparse flow vectors under the assumption of constant motion in a local neighborhood, whereas the Horn-Schunck method generates a dense flow field under the assumption of spatially smooth flow fields. Despite more than 30 years of research, the estimation of optic flow fields is still a highly active research direction.</p><h2 id="4-8-The-Lucas-Kanade-Method"><a href="#4-8-The-Lucas-Kanade-Method" class="headerlink" title="4.8 The Lucas-Kanade Method"></a>4.8 The Lucas-Kanade Method</h2><ul><li><p><code>Brightness Constancy Assumption</code>: Let $x(t)$ denote a moving point at time $t$, and $I(x,t)$ a video sequence, then:<br>  $$<br>  I(x(t), t) &#x3D; const. \quad \forall t,<br>  $$<br>  i.e. the brightness of point $x(t)$ is constant. Then the total time derivative must be zero:<br>  $$<br>  \frac{d}{dt}I(x(t), t) &#x3D; \nabla I^T(\frac{dx}{dt}) + \frac{\partial I}{\partial t} &#x3D; 0.<br>  $$<br>  This constraint is often called the (different) <code>optical flow constraint</code>. The desired local flow vector (velocity) is given by $v &#x3D; \frac{dx}{dt}$.</p></li><li><p><code>Constant motion in a neighborhood</code>: Since the above equation cannot be solved for $v$, one assumes that $v$ is constant over a neighborhood $W(x)$ of the point $x$:<br>  $$<br>  \nabla I(x’, t)^T v + \frac{\partial I}{\partial t}(x’, t) &#x3D; 0 \quad \forall x’ \in W(x).<br>  $$</p></li></ul><p>The brightness is typically not exactly constantly and the velocity is typically not exactly the same for the local neighborhood.</p><p><code>Lucas and Kanade(1981)</code> therefore compute the best velocity vector $v$ for the point $x$ by minimizing the <code>least squares error</code>:<br>$$<br>E(v) &#x3D; \int_{W(x)}|\nabla I(x’, t)^Tv + I_t(x’, t)|^2dx’.<br>$$</p><p>Expanding the terms and setting the derivative to zero one obtains:<br>$$<br>\frac{dE}{dv} &#x3D; 2Mv + 2q &#x3D; 0,<br>$$<br>with<br>$$<br>M &#x3D; \int_{W(x)}\nabla I\nabla I^T dx’, \quad and \quad q &#x3D; \int_{W(x)}I_t\nabla Idx’.<br>$$<br><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/be844ec34f53801541b876e5c1cd1cd.jpg"><br>If $M$ is invertible, i.e. $det(M)\neq 0$, then the solution is:<br>$$<br>v &#x3D; -M^{-1}q.<br>$$</p><h2 id="4-9-Estimating-Local-Displacements"><a href="#4-9-Estimating-Local-Displacements" class="headerlink" title="4.9 Estimating Local Displacements"></a>4.9 Estimating Local Displacements</h2><ul><li>Translational motion: Lucas &amp; Kanade 1981:<br>  $$<br>  E(b) &#x3D; \int_{W(x)} |\nabla I^T b+ I_t|^2dx’ \rightarrow min.\\<br>  \frac{dE}{db}&#x3D;0 \Leftrightarrow b &#x3D; …<br>  $$</li><li>Affine motion: (为window内的像素细致的估计各自的运动)<br>  $$<br>  E(p) &#x3D; \int_{W(x)} |\nabla I^T S(x’)p+ I_t|^2dx’ \rightarrow min.\\<br>  \frac{dE}{dp}&#x3D;0 \Leftrightarrow p &#x3D; …<br>  $$<br>  where:<br>  $$<br>  u(x) &#x3D; S(x)p &#x3D; \left(\begin{matrix}<br>  x &amp;y&amp;1&amp;0&amp;0&amp;0\\<br>  0&amp;0&amp;0&amp;x&amp;y&amp;1<br>  \end{matrix}\right)(p_1, p_2, p_3,p_4,p_5,p_6)^T &#x3D;<br>  \left(\begin{matrix}<br>  p_1x+p_2y+p_3\\<br>  p_4x+p_5y+p_6<br>  \end{matrix}\right).<br>  $$</li></ul><h2 id="4-10-When-can-Small-Motion-be-Estimated"><a href="#4-10-When-can-Small-Motion-be-Estimated" class="headerlink" title="4.10 When can Small Motion be Estimated?"></a>4.10 When can Small Motion be Estimated?</h2><p>In the formalism of Lucas and Kanade, one cannot always estimate a translational motion. This problem is often referred to as the <code>aperture problem</code>. It arises for example, if the region in the window $W(x)$ around the point $x$ has entirely constant intensity (for example a white wall), because then $\nabla I(x)&#x3D;0$ and $I_t(x)&#x3D;0$ for all points in the window.</p><p>In order for the solution of $b$ to be unique the <code>structure tensor</code>:<br>$$<br>M(x) &#x3D; \int_{W(x)} \left(\begin{matrix}<br>    I_x^2 &amp;I_xI_y\\<br>    I_xI_y &amp;I_y^2<br>\end{matrix} \right)dx’.<br>$$<br>needs to be invertible. That means that we must have $det(M)\neq 0$.</p><p>If the structure tensor is not invertible but not zero, then one can estimate the <code>normal motion</code>, i.e. the motion in direction of the image gradient.</p><p>For those points with $det(M(x))\neq 0$, we can compute a motion vector $b(x)$. This leads to the following simple feature tracker.</p><h2 id="4-11-A-Simple-Feature-Tracking-Algorithm"><a href="#4-11-A-Simple-Feature-Tracking-Algorithm" class="headerlink" title="4.11 A Simple Feature Tracking Algorithm"></a>4.11 A Simple Feature Tracking Algorithm</h2><p>Feature tracking over a sequence of images can now be done as follows:</p><ul><li>For a given time instance $t$, compute at each point $x\in \Omega$ the structure tensor”<br>  $$<br>  M(x) &#x3D; \int_{W(x)} \left(\begin{matrix}<br>  I_x^2 &amp;I_xI_y\\<br>  I_xI_y &amp;I_y^2<br>  \end{matrix} \right)dx’.<br>  $$</li><li>Mark all points $x\in \Omega$ for which the determinant of $M$ is larger than a threshold $\theta &gt;0$:<br>  $$det(M(x))\geq 0<br>  $$</li><li>For all these points the local velocity is given by:<br>  $$<br>  b(x, t) &#x3D; -M(x)^{-1}\left(\begin{matrix}<br>  \int I_xI_t dx’\\<br>  \int I_yI_t dx’<br>  \end{matrix} \right).<br>  $$</li><li>Repeat the above steps for the points $x+b$ at time $t+1$.</li></ul><h2 id="4-12-Robust-Feature-Point-Extraction"><a href="#4-12-Robust-Feature-Point-Extraction" class="headerlink" title="4.12 Robust Feature Point Extraction"></a>4.12 Robust Feature Point Extraction</h2><p>Even $det(M(x))\neq 0$ does not guarantee robust estimates of velocity—the inverse of $M(x)$ may not be very stable if, for example, the determinant of $M$ is very small. Thus locations with $det(M)\neq 0$ are not always reliable features for tracking.</p><p>One of the classical feature detectors was proposed independently by <code>Forstner 1984</code> and <code>Harris &amp; Stephens 1988</code>.<br>It is based on the <code>structure tensor</code>:<br>$$<br>M(x) &#x3D; G_{\sigma}*\nabla I \nabla I^T &#x3D; \int G_\sigma (x-x’)\left(\begin{matrix}<br>    I_x^2 &amp;I_xI_y\\<br>    I_xI_y &amp;I_y^2<br>    \end{matrix} \right)(x’)dx’,<br>$$<br>where rather than simple summing over the window $W(x)$ we perform a summation weighted by a Gaussian $G$ of width $\sigma$.<br>The Forstner&#x2F;Harris detector is given by:<br>$$<br>C(x) &#x3D; det(M) + \kappa trace^2(M).<br>$$</p><p>One selects points for which $C(x)&gt;\theta$ with a threshold $\theta &gt; 0$.</p><h2 id="4-13-Wide-Baseline-Matching"><a href="#4-13-Wide-Baseline-Matching" class="headerlink" title="4.13 Wide Baseline Matching"></a>4.13 Wide Baseline Matching</h2><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/930b4a946c9d21d6280785661df7b0ee.png"></p><p>Corresponding points and regions may look very different in different views. Determining correspondence is a challenge.</p><p>In the case of <code>wide baseline matching</code>, large parts of the image plane will not matching at all because they are not visible in the other image. In other words, while a given point may have many potential matches, quite possibly it does not have a correpsonding point in the other image.</p><h2 id="4-14-Extensions-to-Larger-Baseline"><a href="#4-14-Extensions-to-Larger-Baseline" class="headerlink" title="4.14 Extensions to Larger Baseline"></a>4.14 Extensions to Larger Baseline</h2><p>One of the limitations of tracking features frame by frame is that <code>small errors in the motion accumulate over time</code> and the window gradually moves away from the point that was originally tracked. This is known as <code>drift</code>.<br> A remedy is to match a given point back to the first frame. This generally implies larger displacements between frames.</p><p> Two aspects matter when extending the above simple feature tracking method to somewhat larger displacements:</p><ul><li>Since the motion of window between frames is (in general) no longer translational, one needs to <code>generalize the motion model</code> for the window $W(x)$, for example by using an <code>affine motion model</code>.</li><li>Since the illumination will change over time (especially when comparing more distant frames), one can replace the sum-of-squared-differences by the <code>normlized cross correlation</code> which is more <code>robost to illumination changes</code>.</li></ul><h2 id="4-15-Normlized-Cross-Correlation-（就是用均值和方差进行归一化处理）"><a href="#4-15-Normlized-Cross-Correlation-（就是用均值和方差进行归一化处理）" class="headerlink" title="4.15 Normlized  Cross Correlation （就是用均值和方差进行归一化处理）"></a>4.15 Normlized  Cross Correlation （就是用均值和方差进行归一化处理）</h2><p>The <code>normalized cross correlation</code> is defined as:<br>$$<br>NCC(h) &#x3D; \frac{\int_{W(x)}(I_1(x’)-\bar{I_1})(I_2(h(x’)) - \bar{I_2})dx’}{\sqrt{\int_{W(x)}(I_1(x’)-\bar{I_1})^2dx’ \int_{W(x)}(I_2(h(x’)) - \bar{I_2})^2dx’}}<br>$$<br>where $\bar{I_1}$ and $\bar{I_2}$ are the average intensity over the window $W(x)$. By subtracting this average intensity, the measure becomes <code>invariant to additive intensity changes</code> $I\rightarrow I+\gamma$. (对加法不敏感，即均值处理)<br>Dividing by the intensity variances of each window makes the measure <code>invariant to multiplicative changes</code> $I\rightarrow \gamma I$.(对乘法不敏感，即单位化)<br>If we stack the normlized intensity values of respective windows into one vector, $v_i\equiv vec(I_i - \bar{I_i})$, then the normlized cross correction is the <code>cosine of the angle between them</code>:<br>$$<br>NCC(h) &#x3D; \cos \angle(v_1, v_2).<br>$$</p><h2 id="4-16-Special-Case-Optimal-Affine-Transformation"><a href="#4-16-Special-Case-Optimal-Affine-Transformation" class="headerlink" title="4.16 Special Case: Optimal Affine Transformation"></a>4.16 Special Case: Optimal Affine Transformation</h2><p>The normalized cross correlation can be used to determine the optimal affine transformation between two given patches. Since the affine transformation is given by:<br>$$<br>h(x) &#x3D; Ax + d,<br>$$<br>we need to <code>maximize the cross correlation with respect to the 2x2-matrix </code> $A$ <code>and the displacement</code> $d$:<br>$$<br>\hat{A}, \hat{d} &#x3D; \\argmax_{A, d}NCC(A, d).<br>$$<br>where<br>$$<br>NCC(A, d) &#x3D; \frac{\int_{W(x)}(I_1(x’)-\bar{I_1})(I_2(Ax’+d) - \bar{I_2})dx’}{\sqrt{\int_{W(x)}(I_1(x’)-\bar{I_1})^2dx’ \int_{W(x)}(I_2(Ax’+d) - \bar{I_2})^2dx’}}<br>$$<br>Efficiently finding appropriate optima, however, is a challenge.</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
          <category> SFM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> SFM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多视图几何（三）</title>
      <link href="/2022/02/23/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/2022/02/23/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter-3-Perspective-Projection"><a href="#Chapter-3-Perspective-Projection" class="headerlink" title="Chapter 3 Perspective Projection"></a>Chapter 3 Perspective Projection</h1><h2 id="3-1-Mathematics-of-Perspective-Projection"><a href="#3-1-Mathematics-of-Perspective-Projection" class="headerlink" title="3.1 Mathematics of Perspective Projection"></a>3.1 Mathematics of Perspective Projection</h2><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/946429bf8edc558203e31c5e05ee8318.png"></p><p>The perspective transformation $\pi$ from a point with coordinates $\textbf{X}&#x3D;(X,Y,Z)\in \mathbb{R}^3$ relative to the reference frame centered at the optical center and with z-axis being the optical axis (of the lens) is obtained by comparing similar triangles $\textbf{A}$ and $\textbf{B}$:<br>$$<br>\frac{Y}{Z} &#x3D; -\frac{y}{f} \Leftrightarrow y &#x3D; -f\frac{Y}{Z}.<br>$$</p><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/0ae78f3f4b6d8d778addc3250a2fdded.png"><br>To simplify equations, one flips the signs of x- and y-axes, which amounts to considering the image plane to be in front of the center of projection (rather than behind it). The perspective transformation $\pi$ is therefore given by:<br>$$<br>\pi : \mathbb{R}^3 \rightarrow \mathbb{R}^2; \quad \textbf{X}\mapsto x&#x3D;\pi(\textbf{X})&#x3D;\left(\begin{matrix} f\frac{X}{Z} \\ f\frac{Y}{Z}\end{matrix}\right).<br>$$</p><h2 id="3-2-An-Ideal-Perspective-Projection"><a href="#3-2-An-Ideal-Perspective-Projection" class="headerlink" title="3.2 An Ideal Perspective Projection"></a>3.2 An Ideal Perspective Projection</h2><p><code>In homogeneous coordinates</code>, the perspective transformation is given by:<br>$$<br>Z\textbf{x} &#x3D; Z\left(\begin{matrix}<br>    x\\y\\1<br>\end{matrix}\right)&#x3D;<br>\left(\begin{matrix}<br>    f &amp; 0&amp;0 &amp;0\\<br>    0 &amp;f &amp;0 &amp;0\\<br>    0 &amp;0&amp;1&amp; 0<br>\end{matrix}\right)\left(\begin{matrix}<br>    X\\Y\\ Z \\1<br>\end{matrix}\right) &#x3D; K_f\Pi_0 \textbf{X},<br>$$<br>where we have introduced the two matrices:<br>$$<br>K_f \equiv \left(\begin{matrix}<br>    f &amp; 0&amp; 0\\<br>    0 &amp; f &amp;0 \\<br>    0 &amp; 0 &amp; 1<br>\end{matrix}\right) and\quad \Pi_0 \equiv<br>\left(\begin{matrix}<br>    1 &amp;0 &amp;0 &amp;0 \\<br>    0 &amp;1 &amp;0 &amp; 0 \\<br>    0 &amp;0 &amp;1 &amp;0<br>\end{matrix}\right).<br>$$</p><p>The matrix $\Pi_0$ is referred to as the <code>standard projection matrix</code>. Assuming $Z$ to be a constant $\lambda &gt;0$, we obtain:<br>$$<br>\lambda x &#x3D; K_f\Pi_0 \textbf{X}.<br>$$</p><p>From the previous lectures, we know that due to the <code>rigid motion of the camera</code>, the point $\textbf{X}$ <code>in camera coordinates</code> is given as function of the point in <code>world co ordinates</code> $\textbf{X}_0$ by:<br>$$<br>\textbf{X} &#x3D; R\textbf{X}_0 + T,<br>$$<br>or in homogeneous coordinates $\textbf{X}&#x3D;(X, Y, Z, 1)^T$:<br>$$<br>\textbf{X} &#x3D; g\textbf{X}_0 &#x3D; \left(\begin{matrix}<br>    R &amp;T\\<br>    0 &amp;1<br>\end{matrix}\right)\textbf{X}_0.<br>$$</p><p>In total, the transformaiton from world coordinates to image coordinates is therefore given by:<br>$$<br>\lambda \textbf{x}&#x3D;K_f\Pi _0 g\textbf{X}_0.<br>$$</p><p>If the focal length $f$ is known, it can be normlized to 1 (by changing the units of the image cooedinates),such that:<br>$$<br>\lambda \textbf{x} &#x3D; \Pi_0 \textbf{X} &#x3D; \Pi_0 g \textbf{X}_0.<br>$$</p><h2 id="3-3-Intrinsic-Camera-Parameters"><a href="#3-3-Intrinsic-Camera-Parameters" class="headerlink" title="3.3 Intrinsic Camera Parameters"></a>3.3 Intrinsic Camera Parameters</h2><h2 id="从世界坐标系—-gt-camera坐标系—-gt-image坐标系—-gt-pixel坐标系"><a href="#从世界坐标系—-gt-camera坐标系—-gt-image坐标系—-gt-pixel坐标系" class="headerlink" title="(从世界坐标系—&gt;camera坐标系—&gt;image坐标系—&gt;pixel坐标系)"></a>(从世界坐标系—&gt;camera坐标系—&gt;image坐标系—&gt;pixel坐标系)</h2><p>If the camera is not centered at the optical center, we have an additional translation $o_x, o_y$ and if pixel coordinates do not have unit scale, we need to introduce an additional scaling in x- and y-direction by $s_x$ and $s_y$. If the pixels are not rectangular, we have a <code>skew factor</code> $s_\theta$.<br>The pixel coordinates $(x’, y’, 1)$ as a function of homogeneous camera coordinates $\textbf{X}$ are then given by:</p><p>$$<br>\lambda \left(\begin{matrix}<br>    x’\\y’\\1<br>\end{matrix}\right)&#x3D;\underbrace{\left(\begin{matrix}<br>    s_x &amp;s_y &amp;o_x\\<br>    0 &amp;s_y  &amp;o_y \\<br>    0 &amp; 0&amp; 1<br>\end{matrix}\right)}<em>{\equiv K_s}<br>\underbrace{\left(\begin{matrix}<br>    f &amp;0&amp;0\\<br>    0 &amp;f&amp;0\\<br>    0 &amp; 0&amp; 1<br>\end{matrix}\right)}</em>{\equiv K_f}<br>\underbrace{\left(\begin{matrix}<br>    1&amp;0&amp;0&amp;0\\<br>    0&amp;1&amp;0&amp;0\\<br>    0&amp;0&amp;1&amp;0<br>\end{matrix}\right)}_{\equiv \Pi_0}<br>\left(\begin{matrix}<br>    X\\Y\\Z\\1<br>\end{matrix}\right)<br>$$</p><p>After the perspective projection $\Pi_0$ (with focal length 1), we have an additional transformation which depends on the (intrinsic) camera parameters. This can be expressed by the <code>intrisic parameter matrix</code> $K&#x3D;K_sK_f$.</p><h2 id="3-4-The-Intrinsic-Parameter-Matrix"><a href="#3-4-The-Intrinsic-Parameter-Matrix" class="headerlink" title="3.4 The Intrinsic Parameter Matrix"></a>3.4 The Intrinsic Parameter Matrix</h2><p>All intrinsic camera parameter therefore enetr the <code>intrinsic parameter matrix</code>:<br>$$<br>K \equiv K_sK_f &#x3D; \left(\begin{matrix}<br>    fs_x&amp;fs_\theta&amp;o_x\\<br>    0&amp;fs_y&amp;o_y\\<br>    0&amp;0&amp;1<br>\end{matrix}\right).<br>$$</p><p>As a function of the world coordinates $\textbf{X}_0$, we therefore have:<br>$$<br>\lambda \textbf{x}’ &#x3D; K\Pi_0 \textbf{X} &#x3D; K\Pi_0 g\textbf{X}_0 \equiv \Pi \textbf{X}_0.<br>$$</p><p>The $3\times 4$ matrix $\Pi\equiv K\Pi_0 g&#x3D;(KR, KT)$ is called a <code>general projection matrix</code>.</p><p>Although the above equation looks like a linear one, we still have the scale parameter $\lambda$. Dividing by $\lambda$ gives:<br>$$<br>x’ &#x3D; \frac{\pi_1^T\textbf{X}_0}{\pi_3^T\textbf{X}_0}, \quad y’&#x3D;\frac{\pi_2^T\textbf{X}_0}{\pi_3^T\textbf{X}_0}, \quad z’&#x3D;1,<br>$$<br>where $\pi_1^T,\pi_2^T,\pi_3^T \in \mathbb{R}^4$ are the three rows of the projection matrix $\Pi$.<br>The entries of the intrinsic parameter matrix:<br>$$<br>\left(\begin{matrix}<br>    fs_x&amp;fs_\theta&amp;o_x\\<br>    0&amp;fs_y&amp;o_y\\<br>    0&amp;0&amp;1<br>\end{matrix}\right),<br>$$<br>can be interpreted as follows:<br>$o_x:$ x-coordinate of principal point in pixels,<br>$o_y$: y-coordinate of principal point in pixels,<br>$fs_x&#x3D;\alpha_x$:size of unit length in horizontal pixels,<br>$fs_y&#x3D;\alpha_y$:size of unit length in vertical pixels,<br>$\alpha_x &#x2F; \alpha_y$: aspect ratio $\sigma$,<br>$fs_\theta$: skew of pixel, often close to zero.</p><h2 id="3-5-Spherical-Perspective-Projection"><a href="#3-5-Spherical-Perspective-Projection" class="headerlink" title="3.5 Spherical Perspective Projection"></a>3.5 Spherical Perspective Projection</h2><p>The perspective pinhole camera introduced above considers a planar imaging surface. Instead, one can consider a spherical projection surface given by the unit sphere $\mathbb{S}^2\equiv {\textbf{x}\in \mathbb{R}^3 | |\textbf{x}|&#x3D;1}$. The <code>spherical projection</code> $\pi_s$ of a 3D point $\textbf{X}$ is given by:</p><p>$$<br>\pi_s :\mathbb{R}^3 \rightarrow \mathbb{S}; \quad \textbf{X} \mapsto \textbf{x}&#x3D;\frac{\textbf{X}}{\textbf{|X|}}.<br>$$</p><p>The pixel coordinates $\textbf{x}’$ as a function of the world coordinates $\textbf{X}_0$ are:</p><p>$$<br>\lambda \textbf{x}’ &#x3D; K\Pi_0 g\textbf{X}_0,<br>$$</p><p>except that the scalar factor is now $\lambda&#x3D;|\textbf{X}|&#x3D;\sqrt{X^2+Y^2+Z^2}$. One often writes $\textbf{x}~\textbf{y}$ for homogeneous vectors $\textbf{x}$ and $\textbf{y}$ if they are equal up to a scalar factor. Then we can write:</p><p>$$<br>\textbf{x}’~\Pi \textbf{X}_0 &#x3D; K\Pi_0 g\textbf{X}_0.<br>$$</p><p>This property holds for any imaging surface, as long as the ray between $\textbf{X}$ and the origin intersects the imaging surface.</p><h2 id="3-6-Radial-Distortion"><a href="#3-6-Radial-Distortion" class="headerlink" title="3.6 Radial Distortion"></a>3.6 Radial Distortion</h2><p>The intrinsic parameters in the matrix $K$ model linear distortions in the transformations to pixel coordinates. In practice, however, one can also encounter significant <code>distortions along the radial axis</code>, in particular if a wide field of view is used or if one uses cheaper cameras such as webcams. A simple effective model for such distortions is:</p><p>$$<br>x &#x3D; x_d (1+a_1r^2 + a_2r^4), \quad y &#x3D; y_d(1+a_1r^2+a_2r^4),<br>$$</p><p>where $\textbf{x}_d\equiv (x_d, y_d)$ is the diatorted point, $r^2&#x3D;x_d^2+y_d^2$. If a calibration rig is available, the distortion parameter $a_1$ and $a_2$ can be estimated.</p><p>Alternatively, one can estimate a distortion model directly from the images. A more general model is :<br>$$<br>\textbf{x}&#x3D;c+f(r)(\textbf{x}_d - c), \ with \ f(r)&#x3D;1+ a_1r + a_2r^2 + a_3r^3 + a_4r^4,<br>$$</p><p>Here, $r &#x3D; |\textbf{x} -c|$ is the distance to an arbitrary center of distortion $c$ and the <code>distortion correction factor </code> $f(r)$ is an arbitrary 4-th order expression. Parameter are computed <code>from disrtotions of straight lines</code> or <code>simultaneously with the 3D reconstruction</code>.</p><h2 id="3-7-Preimage-of-Points-and-Lines"><a href="#3-7-Preimage-of-Points-and-Lines" class="headerlink" title="3.7 Preimage of Points and Lines"></a>3.7 Preimage of Points and Lines</h2><p>The perspective transformation introduced above allows to define images for arbitrary geometric entities by simply transformaing all points of the entity. However, due to the unknown scale factor, each point is mapped not to a single point $\textbf{x}$, but to an <code>qquivalence class of points </code> $\textbf{y}~\textbf{x}$. It is therefore useful to <code>study how lines are transformed</code>. A line $L$ in 3-D is characterized by a base point $\textbf{X}_0&#x3D; (X_0, Y_0, Z_0, 1)^T \in \mathbb{R}^4$ and a vector $\textbf{V}&#x3D;(V_1, V_2, V_3, 0)^T\in \mathbb{R}^4$:</p><p>$$<br>\textbf{X}&#x3D;\textbf{X}_0 + \mu\textbf{V}, \quad \mu\in\mathbb{R}.<br>$$</p><p>The image of the line $L$ is given by:</p><p>$$<br>\textbf{x}~\Pi_0 \textbf{X} &#x3D; \Pi_0 (\textbf{X}_0 + \mu \textbf{V}) &#x3D; \Pi_0 \textbf{X}_0 + \mu \Pi_0 \textbf{V}.<br>$$</p><p>All points $\textbf{x}$ treated as vectors from the origin $o$ span a 2-D subspace $P$. The intersection of this plane $P$ with the image plane gives the image of the line. $P$ is called the preimage of the line.</p><p>A <code>preimage of a point or a line</code> in the image plane is the largest set of 3D points that give rise to an image equal to the given point or line.</p><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/20210715102405529.png"></p><p>Preimages can be defined for curves or other more complicated geometric structures. In the case of points and lines, however, the preimage is  a subspace of $\mathbb{R}^3$. This subspace can also be represented by its orthogonal complement, i.e. the normal vector in the case of a plane. This complement is called the coimage. The <code>coimage of a point or a line</code> is the subspace in $\mathbb{R}^3$ that is the (unique) orthogonal complement of its preimage. Image, preimage and coimage are <code>equivalent</code> because they uniquely determine one another:<br>$$<br>image &#x3D; preimage \cap image plane, \quad preimage &#x3D; span(image), \\<br>$$</p><p>$$<br>preimage &#x3D; coimage^{\perp}, \quad coimage &#x3D; preimage^{\perp}.<br>$$</p><h2 id="3-8-Preimage-and-Colimage-of-Points-and-Lines"><a href="#3-8-Preimage-and-Colimage-of-Points-and-Lines" class="headerlink" title="3.8 Preimage and Colimage of Points and Lines"></a>3.8 Preimage and Colimage of Points and Lines</h2><p>In the case of the line $L$, the preimage is a 2D subspace, characterized by the 1D colimage given by the span of its normal vector $\ell \in \mathbb{R}^3$. All points of the preimage, and hence all points $\textbf{x}$ of the image of $L$ are orthogonal to $\ell$:<br>$$<br>\ell^T\textbf{x} &#x3D; 0.<br>$$</p><p>The space of all vectors orthogonal to $\ell$ is spanned by the row vectors of $\hat{\ell}$, thus we have:<br>$$<br>P &#x3D; span(\hat{\ell}).<br>$$</p><p>In the case that $\textbf{x}$ is the image of a point $p$, the preimage is a line and the colimage is the plane orthogonal to $\textbf{x}$, i.e. it is spanned by the rows of the matrix $\hat{x}$. </p><p>In summary we have the following table:</p><table><thead><tr><th align="center"></th><th align="center">Image</th><th align="center">Preimage</th><th align="center">Colimage</th></tr></thead><tbody><tr><td align="center">Point</td><td align="center">span($\textbf{x}$)$\cap$ im.plane</td><td align="center">span($\textbf{x}$)$\subset\mathbb{R}^3$</td><td align="center">span($\textbf{x}$)$\subset\mathbb{R}^3$</td></tr><tr><td align="center">Line</td><td align="center">span($\hat{\ell}$)$\cap$ im.plane</td><td align="center">span($\hat{\ell}$)$\subset\mathbb{R}^3$</td><td align="center">span($\hat{\ell}$)$\subset\mathbb{R}^3$</td></tr></tbody></table><h2 id="3-9-Summary"><a href="#3-9-Summary" class="headerlink" title="3.9 Summary"></a>3.9 Summary</h2><p>In this part of the lecture, we studied the <code>perspective projection</code> which takes us from the 3D (4D) camera coordinates to 2D camera image coordinates and pixel coordinates. In homogeneous coordinates, we have the transformations:<br>$$<br>4D\ Wolrd\ coordinates \stackrel{g\in SE(3)}\longrightarrow 4D \ {Camera \ coordinates} \stackrel{K_f\Pi_0}{\longrightarrow} 3D \ {image \ coordinates}\stackrel{K_s}{\longrightarrow} 3D \ {pixel \ coordinates} (齐次坐标形式).<br>$$</p><p>In particular, we can summarize the <code>(intrinsic) camera parameters</code> in the matrix:<br>$$<br>K &#x3D; K_sK_f.<br>$$</p><p>The full transformation from world coordinates $\textbf{X}_0$ to pixel coordinates $\textbf{x}’$ is given by :<br>$$<br>\lambda \textbf{x}’ &#x3D; K\Pi_0 g\textbf{X}_0.<br>$$</p><p>Moreover, for the images of points and lines we introduced the notions of <code>preimage</code> (maximal point set which is consistent with a given image) and <code>colimage</code> (its orthogonal complement). Both can be used equivlently to the image.</p><h2 id="3-10-Projective-Geometry"><a href="#3-10-Projective-Geometry" class="headerlink" title="3.10 Projective Geometry"></a>3.10 Projective Geometry</h2><p>In order to formally write transformation by linear operations, we made extensive use of <code>homogenoeus coordinates</code> to represent a 3D point as a 4D-vector $(X,Y,Z,1)$ with the last coordinate fixed to 1. This normlization is not always necessary: One can represent 3D points by a general 4D vector:<br>$$<br>\textbf{X} &#x3D; (XW, YW, ZW, W)\in \mathbb{R}^4,<br>$$<br>remembering that merely the direction of this vector is of importance. <code>We therefore identify the point in homogeneous coordinates with the line connecting it with the origin</code>. This leads to the definition of projective coordinates.</p><p><code>An n-dimensional projective space</code> $\mathbb{P}^n$ is the set of all one-dimensional subspaces (i.e. lines through the origin) of the vector space $\mathbb{R}^{n+1}$. A point $p\in \mathbb{P}^n$ can then be assigned homogeneous coordinates $\textbf{X}&#x3D;(x_1, …, x_{n+1})^T$, among which at least one $x$ is nonzero. For any nonzero $\lambda \in \mathbb{R}$, the coordinates $\textbf{Y}&#x3D;(\lambda x_1, …, \lambda x_{n+1})^T$ represent the same point $p$.</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
          <category> SFM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> SFM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多视图几何（二）</title>
      <link href="/2022/02/23/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2022/02/23/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter-2-Representing-a-Moving-Scene"><a href="#Chapter-2-Representing-a-Moving-Scene" class="headerlink" title="Chapter 2 Representing a Moving Scene"></a>Chapter 2 Representing a Moving Scene</h1><h2 id="2-0-Overview"><a href="#2-0-Overview" class="headerlink" title="2.0 Overview"></a>2.0 Overview</h2><h3 id="2-0-1-The-Origins-of-3D-Reconstruction"><a href="#2-0-1-The-Origins-of-3D-Reconstruction" class="headerlink" title="2.0.1 The Origins of 3D Reconstruction"></a>2.0.1 The Origins of 3D Reconstruction</h3><p>The goal to reconstrcut the 3D structure of the world from a set of 2-D views has a long history in computer vision. It is a classical <code>ill-posed problem</code>, because the reconstruction consistent with a given set of observations&#x2F;images is typically not unique. Therefore, one will need to impose additional assumations. Mathematically, the study of geometric relations between a 3D scene and the observed 2D projections is based on two types of transformations, namely:</p><ul><li><code>Euclidean motion</code> or <code>rigid-body motion</code> representing the motion of the camera from one frame to the next.</li><li><code>Perspective projection</code> to account for the image formation process (see pinhole camera, etc).</li></ul><p>The notion of perspective projection has its roots among the ancient Greeks and the Renaissance period. The study of perspective projection lead to the field of <code>projective geometry</code>. </p><p>The joint estimation of camera mation and 3D location is called <code>structure and motion</code> or <code>visual SLAM</code>.</p><h3 id="2-0-2-Three-Dimension-Euclidean-Space"><a href="#2-0-2-Three-Dimension-Euclidean-Space" class="headerlink" title="2.0.2 Three-Dimension Euclidean Space"></a>2.0.2 Three-Dimension Euclidean Space</h3><p>The three-dimension Euclidedan space $\mathbb{E}^3$ consists of all points $p\in \mathbb{E}^3$ characterized by coordinates<br>$$<br>\textbf{X} \equiv (X_1, X_2, X_3)^T \in \mathbb{R}^3,<br>$$<br>such that $\mathbb{E}^3$ can be identified with $\mathbb{R}^3$. That means we talk about points ($\mathbb{E}^3$) and coordinates ($\mathbb{R}^3$) as if they were the same thing. Given two points $\textbf{X}$ and $\textbf{Y}$, ont can define a <code>bound vector</code> as<br>$$<br>v &#x3D; \textbf{Y}-\textbf{X} \in \mathbb{R}^3.<br>$$</p><p>Considering this vector independent of its base point $\textbf{Y}$ makes it a <code>free vector</code>. The set of free vectors $v\in \mathbb{R}^3$ forms a linear vector space. By identifying $\mathbb{E}^3$ and $\mathbb{R}^3$, one can endow $\mathbb{E}^3$ with a scalar product, a norm and a metric. This allows to compute <code>distances, curve length</code><br>$$<br>I(\gamma) \equiv \int_0^1 |\dot{\gamma}(s)|ds \quad for a curve \gamma : [0,1] \rightarrow \mathbb{R}^3,<br>$$<br><code>areas</code> or <code>volumes</code>.</p><h3 id="2-0-3-Cross-Product-Skew-symmetrc-Matrices"><a href="#2-0-3-Cross-Product-Skew-symmetrc-Matrices" class="headerlink" title="2.0.3 Cross Product $ Skew-symmetrc Matrices"></a>2.0.3 Cross Product $ Skew-symmetrc Matrices</h3><p>On $\mathbb{R}^3$ one can define a cross product<br>$$<br>\times : \mathbb{R}^3 \times \mathbb{R}^3 \rightarrow \mathbb{R}^3; \quad u\times v &#x3D; \left(<br>    \begin{matrix}<br>        u_2v_3-u_3v_2\\<br>        u_3v_1 - u_1v_3\\<br>        u_1v_2-u_2v_1<br>    \end{matrix}<br>    \right) \in \mathbb{R}^3,<br>$$<br>which is a vector <code>orthogonal to </code> $u$ and $v$. Since $u\times v&#x3D;-v\times u$, the cross product introduces an <code>orientation</code>. Fixing $u$ induces a linear mapping $v\mapsto u\times v$ which can be represented by the <code>skew-symmetric matrix</code><br>$$<br>\hat{u}&#x3D;\left(\\<br>    \begin{matrix}<br>        &amp;0 &amp;-u_3 &amp; u_2\\<br>        &amp;u_3 &amp;0 &amp;-u_1 \\<br>        &amp;-u_2 &amp;u_1 &amp;0<br>    \end{matrix}<br>    \right) \in \mathbb{R}^{3\times 3}.<br>$$</p><p>In turn, every skew symmeric matrix $M&#x3D;-M^T\in \mathbb{R}^{3\times 3}$ can be identified with a vector $u\in \mathbb{R}^{3}$. The operator $\hat{ }$ defines  an <code>isomorphism</code> between $\mathbb{R}^3$ and the space <code>so(3)</code> of all $3\times 3$ skew-symmetric matrices. Its inverse is denoted by $\vee: so(3) \rightarrow \mathbb{R}^3$.</p><h3 id="2-0-4-Rigid-Body-Motion"><a href="#2-0-4-Rigid-Body-Motion" class="headerlink" title="2.0.4 Rigid-Body Motion"></a>2.0.4 Rigid-Body Motion</h3><p>a <code>rigid-body motion</code> (or rigid-body transformation) is a familly of maps:<br>$$<br>g_t: \mathbb{R}^3 \rightarrow \mathbb{R}^3; \quad \textbf{X}\mapsto g_t(\textbf{X}), \quad t \in [0, T]<br>$$<br>which preserve the norm and cross product of any two vectors:</p><ul><li>$|g_t(v)|&#x3D;|v|, \forall v \in \mathbb{R}^3$,</li><li>$g_t(u)\times g_t(v)&#x3D; g_t(u\times v), \forall u, v\in \mathbb{R}^3$.</li></ul><p>Since norm and scalar product are related by the <code>polarization identity</code><br>$$<br>&lt;u, v&gt; &#x3D; \frac{1}{4}(|u+v|^2-|u-v|^2),<br>$$<br>one can also state that a rigid-body motion is a map which preserves inner product and cross product. As a consequence, rigid-body motions also preserve the <code>triplet product</code><br>$$<br>&lt;g_t(u), g_t(v)\times g_t(w)&gt; &#x3D; &lt;u, v\times w&gt;, \forall u, v, w \in \mathbb{R}^3,<br>$$<br>which means that they are volume-preserving.</p><h3 id="2-0-5-Exponetial-Coordinates-of-Rotation"><a href="#2-0-5-Exponetial-Coordinates-of-Rotation" class="headerlink" title="2.0.5 Exponetial Coordinates of Rotation"></a>2.0.5 Exponetial Coordinates of Rotation</h3><p>We will now devive a representation of an  <code>infinitesimal rotation</code>. To this end, consider a family of rotation matrices $R(t)$ which continuously transform a point from its original location $(R(0)&#x3D;I)$ to a different one.<br>$$<br>\textbf{X}<em>{trans}(t) &#x3D; R(t)\textbf{X}</em>{orig}, \quad with R(t)\in SO(3).<br>$$</p><p>Since $R(t)R(t)^T&#x3D;I, \forall t, $ we have<br>$$<br>\frac{d}{dt}(RR^T)&#x3D;\dot{R}R^T + R\dot{R}^T&#x3D;0 \Rightarrow \dot{R}R^T&#x3D;-(\dot{R}R^T)^T.<br>$$</p><p>Thus, $\dot{R}R^T$ is a <code>skew-symmetric matrix</code>. As shown in the section about the $\hat{}$ operator, this implies that there exists a vector $w(t)\in\mathbb{R}^3$ such that:<br>$$<br>\dot{R}(t)R^T(t)&#x3D;\hat{w}(t) \Leftrightarrow \dot{R}(t)&#x3D;\hat{w}R(t).<br>$$</p><p>Since $R(0)&#x3D;I$, it follows that $\dot{R}(0)&#x3D;\hat{w}(0)$. Therefore the <code>skew-symmetric matrix</code> $\hat{w}(0)\in so(3)$ <code>gives the first order approximation of a rotation:</code><br>$$<br>R(dt)&#x3D;R(0) + dR &#x3D; I + \hat{w}(0)dt.<br>$$</p><h3 id="2-0-6-Lie-Group-and-Lie-Algebra"><a href="#2-0-6-Lie-Group-and-Lie-Algebra" class="headerlink" title="2.0.6 Lie Group and Lie Algebra"></a>2.0.6 Lie Group and Lie Algebra</h3><p>The above calculations showned that the effect of any infinitesimal rotation $R\in SO(3)$ can be approximated by an element from the space of skew-symmetric matrices<br>$$<br>so(3)&#x3D;{\hat{w}| w \in \mathbb{R}^3}<br>$$</p><p>The rotation group $SO(3)$ is called <code>Lie Group</code>. The space $so(3)$ is called its <code>Lie algebra</code>.</p><p>Def.: A <code>Lie group</code> (or infinitesimal group) is a smooth manifold that is also a group, such that the group operations muliplication and inversion are smooth maps.</p><p>As shown above: <code>The Lie algebra so(3) is the tangent space at the identity of the rotation group SO(3)</code>.</p><p>An <code>algebra over a field</code> $K$ is a vector space $V$ over $K$ with multiplication on the space $V$. Elements $\hat{w}$ and $\hat{v}$ of the Lie algebra generally do not commute.</p><p>One can define the <code>Lie bracket</code>:<br>$$<br>[\cdot, \cdot]: so(3)\times so(3)\rightarrow so(3); \quad [\hat{w}, \hat{v}]\equiv \hat{w}\hat{v}-\hat{v}\hat{w}.<br>$$</p><h3 id="2-0-7-The-Exponential-Map-a-map-from-Lie-algebra-to-Lie-Group"><a href="#2-0-7-The-Exponential-Map-a-map-from-Lie-algebra-to-Lie-Group" class="headerlink" title="2.0.7 The Exponential Map (a map from Lie algebra to Lie Group)"></a>2.0.7 The Exponential Map (a map from Lie algebra to Lie Group)</h3><p>Given the infinitesimal formulation or rotation in terms of the skew-symmetric matrix $\hat{w}$, is it possible to determine a useful representation of the rotation $R(t)$? Let us assume that $\hat{w}$ is constant in time.</p><p>The differential equation system</p><p>$$<br>\begin{cases}<br>\dot{R}(t)&#x3D;\hat{w}R(t), \\\\<br>R(0)&#x3D;I.<br>\end{cases}<br>$$</p><p>has the solution:</p><p>$$<br>R(t)&#x3D;e^{\hat{w}t} &#x3D; \sum_{n&#x3D;0}^{\infty}\frac{ {(\hat{w}t)}^n}{n!} &#x3D; I + \hat{w}t + \frac{(\hat{w}t)^2}{2!} + …,<br>$$</p><p>which is a rotation around the axis $w\in \mathbb{R}^3$ by an angle of $t$ if $||w||&#x3D;1$. Alternatively, one can absorb the scalar $t\in \R$ into the skew symmetric matrix $\hat{w}$ to obtain $R(t)&#x3D;e^{\hat{\vee}}$ with $\hat{\vee}&#x3D;\hat{w}t$.<br>This <code>matrix expomemtial</code> therefore defines a map from the Lie algebra to the Lie Group:</p><p>$$<br>\exp: so(3)\rightarrow SO(3); \quad \hat{w}\mapsto e^{\hat{w}}.<br>$$ </p><h3 id="2-0-8-The-Logarithm-of-SO-3"><a href="#2-0-8-The-Logarithm-of-SO-3" class="headerlink" title="2.0.8 The Logarithm of SO(3)"></a>2.0.8 The Logarithm of SO(3)</h3><p>As in the case of real analysis one can define an inverse function to the exponential map by the logarithm. In the context of Lie Groups, this will lead to a mapping from the Lie group to the Lie algebra. For any rotation matrix $R\in SO(3)$, there exists a $w\in \mathbb{R}^3$ such that $R&#x3D;\exp(\hat{w})$. Such an element is denoted by $\hat{w}&#x3D;log(R)$. </p><p>If $R&#x3D;(r_{ij})\neq I$, then an appropriate $w$ is given by:<br>$$<br>|w|&#x3D;cos^{-1}(\frac{trace(R)-1}{2}), \frac{w}{|w|}&#x3D;\frac{1}{2sin(|w|)}\left(<br>    \begin{matrix}<br>        r_{32}-r_{23}\\<br>        r_{13}-r_{31}\\<br>        r_{21}-r_{12}<br>    \end{matrix}\right).<br>$$</p><p>For $R&#x3D;I$, we have $|w|&#x3D;0$, i.e. a rotation by an angle 0. The above statement says: <code>Any orthogonal transformation </code> $R\in SO(3)$ <code>can be realized by rotating by an angle</code> $|w|$ <code>around an axis </code> $\frac{w}{|w|}$ <code>as defined above</code>. We will not prove this statement.</p><p>Obviously the above representation is not unique since increasing the angle by multiples of $2\pi$ will give the same rotation $R$.</p><h3 id="2-0-9-Rodrigues-Formula"><a href="#2-0-9-Rodrigues-Formula" class="headerlink" title="2.0.9 Rodrigues Formula"></a>2.0.9 Rodrigues Formula</h3><p>We have seen that any rotation can be realized by computing $R&#x3D;e^{\hat{w}}$. In analogy to the well-known Euler equation<br>$$<br>e^{i\phi} &#x3D; cos(\phi) + isin(\phi), \quad \forall \phi \in \mathbb{R},<br>$$<br>we have an expression for skew-symmetric matrices $\hat{w}\in so(3)$:<br>$$<br>e^{\hat{w}} &#x3D; I + \frac{\hat{w}}{w}sin(|w|) + \frac{\hat{w}^2}{|w|^2}(1-cos(|w|)).<br>$$</p><p>This is known as <code>Rodrigues formula</code>.</p><p>Proof: Let $t&#x3D;|w|$ and $v&#x3D;w&#x2F;|w|$. Then<br>$$<br>\hat{v}^2 &#x3D; vv^T-I, \quad \hat{v}^3 &#x3D; -\hat{v}, …<br>$$<br>and<br>$$<br>e^{\hat{w}}&#x3D;e^{\hat{v}t}&#x3D;I + (t-\frac{t^3}{3!} + \frac{t^5}{5!}-…)\hat{v} + (\frac{t^2}{2!} - \frac{t^4}{4!} + \frac{t^6}{6!}-…)\hat{v}^2.<br>$$</p><h3 id="2-0-10-Representation-of-Rigid-body-Motions-SE-3"><a href="#2-0-10-Representation-of-Rigid-body-Motions-SE-3" class="headerlink" title="2.0.10 Representation of Rigid-body Motions SE(3)"></a>2.0.10 Representation of Rigid-body Motions SE(3)</h3><p>We have seen that motion of a rigid-body is uniquely determined by specifying the translation $T$ of any given point and a rotation matrix $R$<br>defining the transformation of an oriented Cartesian coordinate frame at the given point. Thus thw space of rigid-body motions given by the group<br>of special Euclidean transformations<br>$$<br>SE(3) \equiv {g&#x3D;(R, T) | R\in SO(3), T\in \mathbb{R}^3},<br>$$</p><p>In homogeneous coordinates, we have:<br>$$<br>SE(3)\equiv {g&#x3D;\left(<br>    \begin{matrix}<br>        R &amp;T\\<br>        0 &amp;1<br>    \end{matrix}\right) |R\in SO(3), T\in \mathbb{R}^3} \subset \mathbb{R}^{4\times 4},<br>$$</p><p>In the context of rigid motions, one can see the difference between points in $\mathbb{E}^3$ (which can be rotated and translated) and vectors in $\mathbb{R}^3$ (which can only be rotated).</p><h3 id="2-0-11-The-Lie-Algebra-of-Twists"><a href="#2-0-11-The-Lie-Algebra-of-Twists" class="headerlink" title="2.0.11 The Lie Algebra of Twists"></a>2.0.11 The Lie Algebra of Twists</h3><p>Given a continuous family of rigid-body transformations<br>$$<br>g: \mathbb{R} \rightarrow SE(3); \quad g(t) &#x3D; \left(<br>    \begin{matrix}<br>        R(t) &amp;T(t)\\<br>        0 &amp;1<br>    \end{matrix}\right) \in \mathbb{R}^{4\times 4},<br>$$<br>we consider<br>$$<br>\dot{g}(t)g^{-1}(t)&#x3D; \left(<br>    \begin{matrix}<br>        \dot{R}R^T &amp;\dot{T}-\dot{R}R^TT \\<br>        0 &amp; 0<br>    \end{matrix}\right) \in \mathbb{R}^{4\times 4}.<br>$$</p><p>As in the case of SO(3), the $\dot{R}R^T$ corresponds to some skew-symmetric matrix $\hat{w}\in so(3)$. Defining a vector $v(t)&#x3D;\dot{T}(t)-\hat{w}(t)T(t)$, we have:<br>$$<br>\dot{g}(t)g^{-1}(t) &#x3D;  \left(<br>    \begin{matrix}<br>        \hat{w}(t) &amp;v(t) \\<br>        0 &amp; 0<br>    \end{matrix}\right) \equiv \hat{\xi}(t) \in \mathbb{R}^{4\times 4}.<br>$$</p><p>Multiplying with $g(t)$ from the right, we obtain:<br>$$<br>\dot{g}&#x3D;\dot{g}g^{-1}g &#x3D; \hat{\xi}g.<br>$$</p><p>The $4\times 4$&#x3D;matrix $\hat{\xi}$ can be viewed as a tangent vector along the curve $g(t)$. $\hat{\xi}$ is called a <code>twist</code>. As in the case of $so(3)$, the set of all twists forms a the tangent space which is the <code>Lie Algebra</code><br>$$</p><p>$$</p><h3 id="2-0-12-The-Lie-Algebra-of-Twists"><a href="#2-0-12-The-Lie-Algebra-of-Twists" class="headerlink" title="2.0.12 The Lie Algebra of Twists"></a>2.0.12 The Lie Algebra of Twists</h3><p>Multiplying with $g(t)$ from the right, we obtain:<br>$$<br>\dot{g} &#x3D; \dot{g}g^{-1}g &#x3D; \hat{\xi}g.<br>$$</p><p>The $4\times 4$-matrix $\hat{\xi}$ can be viewed as a tangented vector along the curve $g(t)$. $\hat{\xi}$ is called a <code>twist</code>. As in the case of $so(3)$, the set of all twists forms a the tangent space which is the <code>Lie algebra</code><br>$$<br>se(3) &#x3D; {<br>    \hat{\xi}&#x3D;<br>    \left(<br>        \begin{matrix}<br>            \hat{w} &amp;v \\<br>            0 &amp;0<br>        \end{matrix}   \right) | \hat{w}\in so(3), v\in \mathbb{R}^3} \subset \mathbb{R}^{4\times 4}.<br>$$<br>to the <code>Lie group SE(3)</code>.</p><p>As before, we can define operators $\vee$ and $\wedge$ to convert between a twist $\hat{\xi} \in se(3)$ and its twist coordinates $\xi \in \mathbb{R}^6$:<br>$$<br>\hat{\xi} \equiv \left(<br>    \begin{matrix}<br>        v \\<br>        w<br>    \end{matrix}<br>    \right)^{\wedge} \equiv<br>    \left(<br>    \begin{matrix}<br>        \hat{w} &amp; v\\<br>        0 &amp;0<br>    \end{matrix}<br>    \right) \in \mathbb{R}^{4\times 4},<br>    \left(<br>    \begin{matrix}<br>        \hat{w} &amp;v\\<br>        0 &amp;0<br>    \end{matrix}<br>    \right)^{\vee} &#x3D;<br>    \left(<br>    \begin{matrix}<br>        v \\<br>        w<br>    \end{matrix}<br>    \right) \in \mathbb{R}^6,<br>$$</p><h3 id="2-0-13-Exponential-Coordinates-for-SE-3"><a href="#2-0-13-Exponential-Coordinates-for-SE-3" class="headerlink" title="2.0.13 Exponential Coordinates for SE(3)"></a>2.0.13 Exponential Coordinates for SE(3)</h3><p>The twist coordinates $\xi &#x3D; \left(<br>    \begin{matrix}<br>    v \\<br>    w<br>\end{matrix}\right)$ are formed by stacking the <code>Linear velocity</code> $v\in \mathbb{R}^3$ (related to translation) and the <code>angular velocity</code> $w\in \mathbb{R}^3$ （related to rotation).<br>The differential equation system<br>$$<br>\begin{cases}<br>    \dot{g}(t) &#x3D; \hat{\xi}g(t), \quad \hat{\xi} &#x3D;const, \\<br>    g(0)&#x3D;I,<br>\end{cases}<br>$$<br>has the solution<br>$$<br>g(t) &#x3D; e^{\hat{\xi}t}&#x3D;\sum_{n&#x3D;0}^{\infty}\frac{(\hat{\xi}t)^n}{n!}.<br>$$</p><p>For $w&#x3D;0$, we have $e^{\hat{\xi}}&#x3D;\left(<br>    \begin{matrix}<br>        I &amp;v\\<br>        0 &amp;1<br>    \end{matrix}<br>    \right),$ while for $w\neq0$ one can show:<br>$$<br>e^{\hat{\xi}}&#x3D;\left(<br>    \begin{matrix}<br>        e^{\hat{w}} &amp;\frac{(I-e^{\hat{w}})\hat{w}v + ww^Tv}{|w|} \\<br>        0 &amp;1<br>    \end{matrix}<br>    \right)<br>$$</p><p>The above shows that the exponential map defines a transformation from the Lie algebra $se(3)$ too the Lie Group $SE(3)$:<br>$$<br>\exp :se(3)\rightarrow SE(3) ; \hat{\xi} \mapsto e^{\hat{\xi}}.<br>$$</p><p>The elements $\hat{\xi} \in se(3)$ are called the <code>exponential coordinates</code> for $SE(3)$. </p><p>Conversely: <code>For erery</code> $g\in SE(3)$, <code>there exist twist coordinates </code> $\xi &#x3D; (v, w)\in \mathbb{R}^6$ <code>such that</code> $g&#x3D;\exp(\hat{\xi})$.</p><p>Proof: Given $g&#x3D;(R, T)$, we know that there exists $w\in \mathbb{R}^3$ with $e^{\hat{w}}&#x3D;R$. If $|w|\neq 0$, the exponential form of $g$ introduced above shows that we merely need to solve the equation<br>$$<br>\frac{(I-e^{\hat{w}})\hat{w}v + ww^Tv}{|w|}&#x3D;T<br>$$<br>for the velocity vector $v\in \mathbb{R}^3$. Just as in the case of $SO(3)$, this representation is generally not unique, i.e. there exist many twists $\hat{\xi}\in se(3)$ which represent the same rigid-body motion $g\in SE(3)$.</p><h3 id="2-0-14-Representing-the-Motion-of-the-Camera"><a href="#2-0-14-Representing-the-Motion-of-the-Camera" class="headerlink" title="2.0.14 Representing the Motion of the Camera"></a>2.0.14 Representing the Motion of the Camera</h3><p>When observing a scene from a moving camera, the coordintes and velocity of points in camera coordinates will change. We will use a rigid-body transformation<br>$$<br>g(t) &#x3D; \left(<br>    \begin{matrix}<br>        R(t) &amp;T(t)\\<br>        0 &amp;1<br>    \end{matrix}\right) \in SE(3)<br>$$<br>to represent the motion from a fixed world frame to the camera frames at time $t$. In particular we assume that at time $t&#x3D;0$ the camera frames coincides with the world frame, i.e. $g(0)&#x3D;I$. For any point $\textbf{X}_0$ in world coordinates, its coordinates in the camera frame at time $t$ are:<br>$$<br>\textbf{X}(t) &#x3D; R(t)\textbf{X}_0 +  T(t).<br>$$<br>or in the homogeneous representation(齐次表示—&gt;[x,y,z,1])<br>$$<br>\textbf{X}(t) &#x3D; g(t)\textbf{X}_0.<br>$$</p><h3 id="2-0-15-Concatenation-of-Motions-over-Frames"><a href="#2-0-15-Concatenation-of-Motions-over-Frames" class="headerlink" title="2.0.15 Concatenation of Motions over Frames"></a>2.0.15 Concatenation of Motions over Frames</h3><p>Given two different times $t_1$ and $t_2$, we denote the transformation from the points in frame $t_1$ to the points in frame $t_2$ by $g(t_2, t_1)$:<br>$$<br>\textbf{X}(t_2) &#x3D; g(t_2, t_1)\textbf{X}(t_1).<br>$$</p><p>Obviously we have:<br>$$<br>\textbf{X}(t_3) &#x3D; g(t_3, t_2)\textbf{X}_2&#x3D;g(t_3, t_2)g(t_2, t_1)\textbf{X}(t_1)&#x3D;g(t_3, t_1)\textbf{X}(t_1),<br>$$<br>and thus:<br>$$<br>g(t_3, t_1) &#x3D; g(t_3, t_2)g(t_2, t_1).<br>$$</p><p>By transferring the coordinates of frame $t_1$ to coordinates in frame $t_2$ and back, we see that:<br>$$<br>\textbf{X}(t_1) &#x3D; g(t_1, t_2)\textbf{X}(t_2) &#x3D; g(t_1, t_2)g(t_2, t_1)\textbf{X}(t_1),<br>$$<br>which must hold for any point coordinates $\textbf{X}(t_1)$, thus:<br>$$<br>g(t_1, t_2)g(t_2, t_1)&#x3D;I \Leftrightarrow g^{-1}(t_2, t_1)&#x3D;g(t_1, t_2).<br>$$</p><h3 id="2-0-16-Rules-of-Velocity-Transformation"><a href="#2-0-16-Rules-of-Velocity-Transformation" class="headerlink" title="2.0.16 Rules of Velocity Transformation"></a>2.0.16 Rules of Velocity Transformation</h3><p>The coordinates of point $\textbf{X}_0$ in frame $t$ are given by $\textbf{X}(t)&#x3D;g(t)\textbf{X}_0$. Therefore the velocity is given by<br>$$<br>\dot{\textbf{X}}(t) &#x3D; \dot{g}(t)\textbf{X}_0 &#x3D; \dot{g}(t)g^{-1}(t)\textbf{X}(t)<br>$$</p><p>By introducing the <code>twist coordinates</code><br>$$<br>\hat{V}(t)\equiv \dot{g}(t)g^{-1}(t) &#x3D; \left(<br>    \begin{matrix}<br>        \hat{w}(t) &amp; v(t) \\<br>        0 &amp; 0<br>    \end{matrix}<br>    \right) \in so(3),<br>$$<br>we get the expression:<br>$$<br>\dot{\textbf{X}(t)}&#x3D;\hat{V}(t)\textbf{X}(t).<br>$$</p><p>In simple 3D-coordinates this gives:<br>$$<br>\dot{\textbf{X}}(t) &#x3D; \hat{w}(t)\textbf{X}(t) + v(t0). (齐次形式)<br>$$</p><p>The symbol $\hat{V(t)}$ therefore represents the relative velocity of the world frame as viewed from the camera frame.</p><h3 id="2-0-17-Transfer-Between-Frames-The-Adjoint-Map-se-3-之间的映射"><a href="#2-0-17-Transfer-Between-Frames-The-Adjoint-Map-se-3-之间的映射" class="headerlink" title="2.0.17 Transfer Between Frames: The Adjoint Map (se(3)之间的映射)"></a>2.0.17 Transfer Between Frames: The Adjoint Map (se(3)之间的映射)</h3><p>Suppose that a viewer in another frame $A$ is displaced relative to the current frame by a transformation $g_{xy}$ :$\textbf{Y} &#x3D; g_{xy}\textbf{X}(t)$. Then the velocity in this view frame is given by:<br>$$<br>\dot{\textbf{Y}}(t) &#x3D; g_{xy}\dot{\textbf{X}}(t) &#x3D; g_{xy}\hat{V}(t)\textbf{X}(t) &#x3D; g_{xy}\hat{V}g^{-1}_{xy}\textbf{Y}(t).<br>$$</p><p>This shows that the relative velocity of points observed from camera frame $A$ is represented by the twist:<br>$$<br>\hat{V}<em>y<br> &#x3D; g</em>{xy}\hat V g_{xy}^{-1} \equiv ad_{g_{xy}}(\hat<br> V).  (就是上式中直接把 \dot{Y}(t)根据定义写成Y(t)本身和在Y(t)处的李代数相乘) $$</p><p>where we have introduced the <code>adjoint map on </code> $se(3)$:<br>$$<br>ad_g : se(3) \rightarrow se(3); \quad \hat{\xi} \mapsto g \hat{\xi}g^{-1}. ( \hat{\xi}就是 twist.)<br>$$</p><h3 id="2-0-18-Summary"><a href="#2-0-18-Summary" class="headerlink" title="2.0.18 Summary"></a>2.0.18 Summary</h3><table><thead><tr><th align="center"></th><th align="center">Rotation $SO(3)$</th><th align="center">Rigid-body $SE(3)$</th></tr></thead><tbody><tr><td align="center">Matrix representation</td><td align="center">$R\in GL(3);$ <br> $R^TR&#x3D;I,$ <br> $det(R)&#x3D;1$</td><td align="center">$g&#x3D;\left(\begin{matrix} R &amp;T\\ 0 &amp;1\end{matrix}\right)$</td></tr><tr><td align="center">3-D coordinates</td><td align="center">$\textbf{X}&#x3D;R\textbf{X}_0$</td><td align="center">$\textbf{X}&#x3D;R\textbf{X}_0 + T$</td></tr><tr><td align="center">Inverse</td><td align="center">$R^{-1} &#x3D; R^T$</td><td align="center">$g^{-1} &#x3D; \left(\begin{matrix} R^T &amp; -R^TT  \\ 0 &amp;1\end{matrix}\right)$</td></tr><tr><td align="center">Exponential representation</td><td align="center">$R&#x3D;\exp({\hat{w}})$</td><td align="center">$g&#x3D;\exp({\hat{\xi}})$</td></tr><tr><td align="center">Velocity</td><td align="center">$\dot{\textbf{X}}&#x3D;\hat{w}\textbf{X}$</td><td align="center">$\dot{\textbf{X}}&#x3D;\hat{w}\textbf{X} + v$</td></tr><tr><td align="center">Adjoint map</td><td align="center">$\hat{w}\mapsto R\hat{w}R^{T}$</td><td align="center">$\hat{\xi} \mapsto g\hat{\xi}g^{-1}$</td></tr></tbody></table><h3 id="2-0-19-Alternative-Representations-Euler-Angles"><a href="#2-0-19-Alternative-Representations-Euler-Angles" class="headerlink" title="2.0.19 Alternative Representations: Euler Angles"></a>2.0.19 Alternative Representations: Euler Angles</h3><p>In addition to the exponential parameterization, there exist alternative mathematical representations to parameterize rotation matrices $R\in SO(3)$,given by the <code>Euler angles</code>. These are <code>local</code> coordinates, i.e. the parameterization is only corret for a protion of $SO(3)$.</p><p>Given a basis $(\hat{w}_1, \hat{w}_2, \hat{w}_3)$ of the Lie algebra $so(3)$, we can define a mapping from $\mathbb{R}^3$ to the Lie froup $SO(3)$ by:<br>$$<br>\alpha : (\alpha_1, \alpha_2, \alpha_3) \mapsto \exp (\alpha_1 \hat{w}_1 + \alpha_2 \hat{w}_2 + \alpha_3 \hat{w}_3).<br>$$</p><p>The coordinates $(\alpha_1, \alpha_2, \alpha_3)$ are called <code>Lie-Cartan coordinates of the first kind</code> relative to the above basis.</p><p>The <code>Lie-Cartan coordinates of the second kind</code> are defined as:<br>$$<br>\beta : (\beta_1, \beta_2, \beta_3) \mapsto \exp (\beta_1 \hat{w}_1)\exp( \beta_2 \hat{w}_2 )\exp(\beta_3 \hat{w}_3).<br>$$</p><p>For the basis representing rotation around the $z-, y-,x-$axis<br>$$<br>w_1 &#x3D; (0,0,1)^T, w_2&#x3D;(0,1,0)^T, w_3&#x3D;(1,0,0)^T,$$<br>the coordinates $\beta_1, \beta_2, \beta_3$ are called <code>Euler Angles</code>.</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
          <category> SFM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> SFM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MVG-多视图几何（一）</title>
      <link href="/2022/02/21/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2022/02/21/%E5%A4%9A%E8%A7%86%E5%9B%BE%E5%87%A0%E4%BD%95%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>多视图几何(Multi view Geometry)中广泛使用到矩阵运算，因此在开始MVG之旅前，先将MVG中重点使用的线代表示介绍一下。这一章中主要涉及到向量空间的表示以及基本的矩阵运算，包括像内积与叉积。接着有适用于刚体运动的李群和李代数的讲解，最后涉及到特征值，特征向量，以及通过SVD分解求解方程的知识。</p><h1 id="Chapter-1-Mathematical-Background-Linear-Algebra"><a href="#Chapter-1-Mathematical-Background-Linear-Algebra" class="headerlink" title="Chapter 1 Mathematical Background Linear Algebra"></a>Chapter 1 Mathematical Background Linear Algebra</h1><p>##1.1 Vector Spaces</p><p>A set $V$ is called a <code>linear space</code> or  a <code>vector space over the field</code> $\mathbb{R}$ if it is closed under vector summation<br>$$+:V\times V \rightarrow V$$<br>and  under scalar multiplication<br>$$\dot : \mathbb{R} \times V \rightarrow V, $$</p><p>i.e. $\alpha v_1 + \beta v_2 \in V \forall v_1, v_2 \in V, \forall \alpha, \beta \in \mathbb{R}$. With respect to addition $(+)$ it forms a commutative group (existence of neutral element 0, inverse element $-v$). Scalar multiplication and addition respect the distributive law:<br>$$(\alpha + \beta)v &#x3D; \alpha v + \beta v \quad \alpha (v+u)&#x3D;\alpha v + \alpha u.$$<br>Example: $V&#x3D;\mathbb{R}^n, v &#x3D; (x_1, x_2, …, x_n)^T$.</p><p>A subset $W\subset V$ of a vector space $V$ is called <code>subspace</code> if $0 \in W$ and $W $ is closed under $+$ and $\cdot$ (for all $\alpha \in \mathbb{R}$)</p><p>##1.2 Linear Independence and Basis<br>The spanned subspace of a set of vectors $S&#x3D;{v_1, v_2, …, v_k} \subset V$ is the subspace formed by all linear combination s of these vectors:</p><p>$$span(S) &#x3D; {v\in V | v &#x3D; \sum_{i&#x3D;1}^k \alpha_i v_i}$$<br>The set $S$ is called <code>linear independent</code> if:<br>$$\sum_{i&#x3D;1}^k \alpha_i v_i&#x3D;0  \Rightarrow \alpha_i&#x3D;0 \forall i.$$</p><p>$$<br>Assume: \<br>\sum_{i&#x3D;1}^k \alpha_i v_i &#x3D;0 \quad \land \alpha_i \neq 0 \<br>\Rightarrow v_j &#x3D; -\frac{1}{\alpha_j}\sum_{i&#x3D;1, i\neq j}^k \alpha_i v_i<br>$$</p><p>in other word, if none of the vectors can be expressed as a linear combination of the remaining vectors. Otherwise the set is called <code>linear dependent</code>.</p><p>A set of vectors $B&#x3D;{v_1, v_2, …,v_n}$ is called a <code>basis of</code> $V$ if it is linearly independent and if it spans the vector space $V$. A basis is a maximal set of linearly independent vectors.</p><p>##1.3 Properties of a Basis<br>Let $B$ and $B’$ be two bases of a linear space $V$.</p><ul><li>$B$ and $B’$ contain the same number of vectors. This number $n$ is called <code>dimension of the space</code> $V$.</li><li>Any vector $v\in V$ can be uniquely expressed as a linear combination of the basis vectors in $B&#x3D;{b_1, b_2, …, b_n}:$<br>$$<br>v &#x3D; \sum_{i&#x3D;1}^n \alpha_i b_i<br>$$</li><li>In particular, all vectors of $B$ can be expressed as linear combinations of vectors of another basis $b_j’\in B’$:<br>$$<br>b_i’ &#x3D; \sum_{j&#x3D;1}^n \alpha_{ji}b_j<br>$$</li></ul><p>The coefficients $\alpha_{ji}$ for this <code>basis transform</code> can be combined in a matrix $A$. Setting $B&#x3D;(b_1, b_2, …, b_n)$ and $B’&#x3D;(b_1’, b_2’, .., b_n’)$ as the matrices of basis vectors, we can write: $B’&#x3D;BA \Leftrightarrow B&#x3D; B’A^{-1}$.</p><p>##1.4 Inner Product<br>On a vector space one can define an <code>inner product (dot product)</code>, dt: Skalarproduct $\neq$ skalare Multiplication:<br>$$<br>(\cdot, \cdot): V \times V \rightarrow \mathbb{R}<br>$$<br>which is defined by three properties:</p><ul><li>$(u, \alpha v + \beta w) &#x3D; \alpha (u,v) + \beta (u, w) \quad (linear)$</li><li>$(u, v) &#x3D; (v, u) \quad (symmetric)$</li><li>$(v, v) \geq 0$ and  $(v, v)&#x3D;0 \Leftrightarrow v&#x3D;0\quad (positive \ definite)$</li></ul><p>The scalar product induces a <code>norm</code>:<br>$$<br>|\cdot|: V \rightarrow \mathbb{R}, \quad |v| &#x3D; \sqrt{(v, v)}<br>$$<br>and a <code>metric</code>:<br>$$<br>d: V\times V \rightarrow \mathbb{R}, \quad d(v, w) &#x3D; |v-w| &#x3D; \sqrt{(v-w, v-w)}<br>$$<br>for measuring lengths and distances, making $V$ a <code>metrin space</code>. Since the metric is induced by a scalar produt $V$ is called a <code>Hilbert space</code>.</p><h2 id="1-5-Canocial-and-Induces-Inner-Product"><a href="#1-5-Canocial-and-Induces-Inner-Product" class="headerlink" title="1.5 Canocial and Induces Inner Product"></a>1.5 Canocial and Induces Inner Product</h2><p>On $V&#x3D;\mathbb{R}^n$, one can define the canoncial inner product for the canoncial basis $B&#x3D;I_n$ as:<br>$$<br>(x, y) &#x3D; x^T y &#x3D; \sum_{i&#x3D;1}^n x_iy_i,<br>$$<br>which induces the standard $L_2$ norm or Euclidean norm:<br>$$<br>|x|_2 &#x3D; \sqrt{x^Tx} &#x3D; \sqrt{x_1^2+ … + x_n^2}.<br>$$</p><p>with a basis transform $A$ to the new basis $B’$ given by $I &#x3D; B’A^{-1}$ the canoncial product in the new coordinates $x’, y’$ is given by :<br>$$<br>(x ,y) &#x3D; x^Ty &#x3D; (Ax’)^T(Ay’) &#x3D; x’^TA^TAy’\equiv (x’, y’)_{A^TA}<br>$$</p><p>The latter product is called the <code>induced inner product</code> fron the matrix $A$.</p><p>Two vectors $v$ and $w$ are <code>orthogonal</code> iff $(v, w)&#x3D;0$.</p><h2 id="1-6-Kronecker-Product-and-Stack-of-a-Matrix"><a href="#1-6-Kronecker-Product-and-Stack-of-a-Matrix" class="headerlink" title="1.6 Kronecker Product and Stack of a Matrix"></a>1.6 Kronecker Product and Stack of a Matrix</h2><p>Given two matrices $A\in \mathbb{R}^{m \times n}$ and $B\in \mathbb{R}^{k\times l}$, one can define their <code>Kronecker product</code> $A\otimes B$ by:<br>$$<br>A\otimes B \equiv \left(<br>\begin{matrix}<br> a_{11}B        &amp; \cdots &amp; a_{1n}B      \\<br> \vdots  &amp; \ddots &amp; \vdots \\<br>a_{m1}B        &amp; \cdots &amp; a_{mn}B      \\<br>\end{matrix}<br>\right) \in \mathbb{R}^{mk\times nl}.<br>$$</p><p>In matlab ths can be implemented by <code>C=kron(A, B)</code>.</p><p>Given a matrix $A\in \mathbb{R}^{m\times n}$, its <code>stack</code> $A^s$ is obtained by staking its $n$ column vectors $a_1, …, a_n \in \mathbb{R}^m$:<br>$$<br>A^s \equiv \left(<br>    \begin{matrix}<br>    a_1\\<br>    \vdots \\<br>    a_n<br>    \end{matrix}<br>    \right) \in \mathbb{R}^{mn}.<br>$$</p><p>These notations allow to rewrite algebraic xpressions, for example:<br>$$<br>u^TAv &#x3D; (v\otimes u)^T A^s<br>$$</p><h2 id="1-7-Linear-Transformation-and-Matrices"><a href="#1-7-Linear-Transformation-and-Matrices" class="headerlink" title="1.7 Linear Transformation and Matrices"></a>1.7 Linear Transformation and Matrices</h2><p>Linear algebra studies the properties of linear transformations between linear spaces. Since these can be represented by matrices, linear algebra studies the properties of matrices. A <code>linear transformation</code> $L$ between two linear spaces $V$ and $W$ is a map $L: V\rightarrow W$ such that:</p><ul><li>$L(x+y) &#x3D; L(x) + L(y) \quad \forall x, y \in V$</li><li>$L(\alpha x) &#x3D; \alpha L(x) \quad \forall x \in V, \alpha \in \mathbb{R}$.</li></ul><p>Due to the linearity, the action of $L$ on the space $V$ is uniquely defined its action on the basis vectors of $V$. In the canoncial basis ${e_1, …,e_n}$ we have:<br>$$<br>L(x) &#x3D; Ax \quad \forall x\in V,<br>$$<br>where<br>$$<br>A &#x3D; (L(e_1), …, L(e_n)) \quad \in \mathbb{R}^{m\times n}.<br>$$</p><p>The set of all real $m\times n$ matrices is denoted by $M(m, n)$. In the case that $m&#x3D;n$, the set $M(m, n)\equiv M(n)$ forms a <code>ring</code> over the field $\mathbb{R}$, i.e. it is closed under matrix multiplication and summation.</p><h2 id="1-8-The-linear-Groups-GL-n-and-SL-n"><a href="#1-8-The-linear-Groups-GL-n-and-SL-n" class="headerlink" title="1.8 The linear Groups $GL(n)$ and $SL(n)$"></a>1.8 The linear Groups $GL(n)$ and $SL(n)$</h2><p>There exist certain sets of linear transformstions which form a group.</p><p>A <code>group</code> is a set $G$ with an operation $\circ: G \times G \rightarrow G$ such that :</p><ul><li>$g_1 \circ g_2 \in G \quad \forall g_1, g_2 \in G $ (closed).</li><li>$(g_1 \circ g_2) \circ g_3 &#x3D; g_1 \circ(g_2 \circ g_3) \forall g_1 ,g_2 ,g_3 \in G$ (assoc.)</li><li>$\exists e\in G: e\circ g&#x3D;g\circ e &#x3D; g \quad \forall g\in G$ (neutral),</li><li>$\exists g^{-1} \in G: g\circ g^{-1} &#x3D; g^{-1}\circ g &#x3D; e \quad \forall g \in G$ (inverse).</li></ul><p>Example: All invertible (non-singular) real $n\times n$-matrices form a group with respect to matrix multiplication. This group is called the <code>general linear froup</code> $GL(n)$. It consists of all $A\in M(n)$ for which<br>$$<br>det(A) \neq 0<br>$$</p><p>All matrices $A\in GL(n)$ for which $det(A)&#x3D;1$ form a group called the $special linear group SL(n)$. The inverse of $A$ is also in the group, as $det(A^{-1})&#x3D;det(A)^{-1}$.</p><h2 id="1-9-Matrix-Representation-of-Groups"><a href="#1-9-Matrix-Representation-of-Groups" class="headerlink" title="1.9 Matrix Representation of Groups"></a>1.9 Matrix Representation of Groups</h2><p>A group $G$ has a <code>matrix representation</code> (dt: Darstellung) or can be realized as a matrix froup if there exists an injective transformation:<br>$$<br>R: G\rightarrow GL(n)<br>$$<br>which <code>preserves the group structure</code> of $G$ that is inverse and composition are preserved by the map:<br>$$<br>R(e)&#x3D;I_{m\times n}, R(g\circ h) &#x3D; R(g)R(h) \quad \forall g, h\in G<br>$$</p><p>Such a map $R$ is called a <code>grop homomorphism (dt: Homomorphismus)</code>.</p><p>The idea of matrix representations of a group is that they allow to analyze more abstract groups by looking at the properties of the respective matrix group. Example: The rotations of an object form a group, as there exists a neutral element (no rotation) and an inverse (the inverse rotation) and any concatenation of rotations is again a rotation (around a different axis). Studing the properties of the rotation group is easier if rotations are represented by respective matrices. </p><h2 id="1-10-The-Affine-Group-A-n"><a href="#1-10-The-Affine-Group-A-n" class="headerlink" title="1.10 The Affine Group $A(n)$"></a>1.10 The Affine Group $A(n)$</h2><p>An afine transformation $L: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is defined by a matrix $A\in GL(n)$ and a vector $b \in \mathbb{R}^n$ such that:<br>$$<br>L(x)&#x3D;Ax+b<br>$$</p><p>The set of all such affine transformations is called the <code>affine group of dimension n</code>, denoted by $A(n)$. </p><p>$L$ defined above is not a linear map unless $b&#x3D;0$. By introducing <code>homogeneous coordinates</code> to represent $x\in \mathbb{R}^n$ by $\left(\begin{matrix}x \ 1\end{matrix}\right) \in \mathbb{R}^{n+1}$, $L$ becomes a linear mapping from<br>$$<br>L:\mathbb{R}^{n+1}\rightarrow \mathbb{R}^{n+1}; \quad \left(<br>    \begin{matrix}<br>    x\\   1<br>    \end{matrix}<br>    \right) \rightarrow \left(<br>    \begin{matrix}<br>    A \ &amp;b  \\<br>    0\ &amp;1<br>    \end{matrix}<br>    \right)\left(<br>    \begin{matrix}<br>    x \\  1<br>    \end{matrix}<br>    \right).<br>$$</p><p>A matrix $\left(<br>    \begin{matrix}<br>    A \ &amp;b\\<br>    0\ &amp;1<br>    \end{matrix}<br>    \right)$ with $A\in GL(n)$ and $b\in \mathbb{R}^n$ is called an <code>affine matrix</code>. It is an element of $GL(n+1)$. The affine matrices from a subgroup of $GL(n+1)$.</p><h2 id="1-11-The-Orthogonal-Group-O-n"><a href="#1-11-The-Orthogonal-Group-O-n" class="headerlink" title="1.11 The Orthogonal Group $O(n)$"></a>1.11 The Orthogonal Group $O(n)$</h2><p>A matrix $A\in M(n)$ is called <code>orthogonal</code> if it preserves the inner product, i.e.:<br>$$<br>&lt;Ax, Ay&gt; &#x3D; &lt;x, y&gt;, \quad \forall x, y \in \mathbb{R}^n<br>$$</p><p>The set of all orthogonal matrices forms the <code>orthogonal group</code> $O(n)$, which is a subgroup og $GL(n)$. For an orthogonal matrix $R$ we have:<br>$$<br>&lt;Rx, Ry&gt; &#x3D; x^TR^TRy&#x3D;x^Ty, \quad \forall x,y\in \mathbb{R}^n<br>$$</p><p>Therefore we must have $R^TR&#x3D;RR^T&#x3D;I$, in other words:<br>$$<br>O(n) &#x3D; {R\in GL(n) | R^TR&#x3D;I}<br>$$</p><p>The above identity shows that for any orthogonal matsix $R$, we have $det(R^TR)&#x3D;(det(R))^2&#x3D;det(I)&#x3D;1$, such that $det(R)\in {\pm 1}$. </p><p>The subgroup of $O(n)$ with $det(R)&#x3D;+1 $ is called the <code>special orthogonal group</code> $SO(n)$. $SO(n)&#x3D;O(n)\cap SL(n)$. In particular, $SO(3)$ is the group of all 3-dimensional rotation matrices.</p><h2 id="1-12-The-Euclidean-Group-E-n"><a href="#1-12-The-Euclidean-Group-E-n" class="headerlink" title="1.12 The Euclidean Group $E(n)$"></a>1.12 The Euclidean Group $E(n)$</h2><p>A Euclidean transformation $L$ from $\mathbb{R}^n$ to $\mathbb{R}^n$ is defined by an orthogonal matrix $R\in O(n)$ and a vector $T\in \mathbb{R}^n$:<br>$$<br>L: \mathbb{R}^n \rightarrow \mathbb{R}^n; \quad x \rightarrow Rx + T<br>$$</p><p>The set of all such transformations is called the <code>Euclidean group</code> $E(n)$. It is a subgroup of the affine group $A(n)$. Embedded by homogeneous coordinates, we get:<br>$$<br>E(n)&#x3D;{<br>    \left(<br>        \begin{matrix}<br>            R \quad &amp;T\\<br>            0\quad &amp;1<br>        \end{matrix}<br>        \right) |R\in O(n), T\in \mathbb{R}^n<br>    }.<br>$$</p><p>If $R\in SO(n) (i.e. det(R)&#x3D;1)$, then we have the <code>special Euclidean group</code> $SE(n)$. In particular, $SE(3)$ represents the <code>rigid-body motions</code> (dt: Starrkorpertransformationen) in $\mathbb{R}^3$.</p><p>In summary:<br>$$<br>SO(n) \subset O(n) \subset GL(n), \quad \<br>SE(n) \subset E(n) \subset A(n) \subset GL(n+1).<br>$$</p><h2 id="1-13-Range-Span-Null-Space-and-Kernel"><a href="#1-13-Range-Span-Null-Space-and-Kernel" class="headerlink" title="1.13 Range, Span, Null Space and Kernel"></a>1.13 Range, Span, Null Space and Kernel</h2><p>Let $A\in \mathbb{R}^{m\times n}$ be a matrix defining a linear map from $\mathbb{R}^n$ to $\mathbb{R}^m$. The <code>range</code> or <code>span</code> of $A$ (dt: Bild) is defined as the subspace of $\mathbb{R}^m$ which can be reached by $A$:<br>$$<br>range(A)&#x3D;{y\in \mathbb{R}^m | \exists  x\in \mathbb{R}^n: Ax&#x3D;y}.<br>$$</p><p>The range of a matrix $A$ is given by the span of its column vectors.</p><p>The <code>null space</code> or <code>kernel</code> of a matrix $A$ (dt: Kern) is given by the subset of vectors $x\in \mathbb{R}^n$ which are mapped to zero:<br>$$<br>null(A) &#x3D; ker(A) &#x3D; {x\in \mathbb{R}^n | Ax&#x3D;0}.<br>$$</p><p>The null space of a matrix $A$ is given by the vectors orthogonal to its row vectors. <code>MatLab: z=null(A)</code>.</p><p>The concepts of range and null space are useful when studying the <code>solution of linear equations</code>. The system $Ax&#x3D;b$ will have a solution $x\in \mathbb{R}^n$ if and only if $b\in range(A)$. Moreover, this solution will be unique only if $ker(A)&#x3D;0$. Indeed, if $x_s$ is a solution of $Ax&#x3D;b$ and $x_o \in ker(A)$, then $x_s+x_o$ is also a solution: $A(x_s + x_o)&#x3D;Ax_s + Ax_o &#x3D; b$.</p><h2 id="1-14-Rank-of-a-Matrix"><a href="#1-14-Rank-of-a-Matrix" class="headerlink" title="1.14 Rank of a Matrix"></a>1.14 Rank of a Matrix</h2><p>The <code>rank</code> of a matrix (dt. Rang) is the dimension of its range:<br>$$<br>rank(A) &#x3D; dim(range(A))<br>$$</p><p>The rank of a matrix $A\in \mathbb{R}^{m\times n}$ has the following properties:</p><ul><li>$rank(A) &#x3D; n-dim(ker(A))$</li><li>$0\leq rank(A) \leq min(m, n)$</li><li>$rank(a)$ is equal to the maximum number of linearly independent row (or column) vectors of $A.$</li><li>$rank(A)$ is the highest order of a nonzero minor of $A$, where a <code>minor of order k</code> is the determinant of a $k\times k$ submatrix of $A$.</li><li>Sylvester’s inequality: Let $B\in \mathbb{R}^{n\times k}$, then $AB\in \mathbb{R}^{m\times k}$ and $rank(A) + rank(B) - n \leq rank(AB) \leq min(rank(A), rank(B))$.</li><li>For any nonsingular matrices $C\in \mathbb{R}^{m\times m}$ and $D\in \mathbb{R}^{n\times n}$, we have : $rank(A)&#x3D;rank(CAD)$.</li></ul><h2 id="1-15-Eigenvalues-and-Eigenvectors"><a href="#1-15-Eigenvalues-and-Eigenvectors" class="headerlink" title="1.15 Eigenvalues and Eigenvectors"></a>1.15 Eigenvalues and Eigenvectors</h2><p>Let $A\in \mathbb{C}^{n\times n}$ be a complex matrix. A non-zero vector $v\in \mathbb{C}^n$ is called a <code>(right) eigenvector of A</code> if:<br>$$<br>Av &#x3D; \lambda v, \quad with \lambda \in \mathbb{C}.<br>$$<br>$\lambda$ is called an <code>eigenvalue of </code> $A$. Similarly $v$ is called a <code>left eigenvalue</code> of $A$, if $v^TA&#x3D;\lambda v^T$ for some $\lambda \in \mathbb{C}$. The <code>spectrum </code> $\sigma(A)$ of a matrix $A$ is the set of all its eigenvalues. <code>Matlab: [V, D] = eig(A);</code><br>where $D$ is a diagonal matrix containing the eigenvalues and $V$ is a matrix whose columns are the corresponding eigenvectors, such that $AV&#x3D;VD$.</p><h2 id="1-16-Properties-of-Eigenvalues-and-Eigenvectors"><a href="#1-16-Properties-of-Eigenvalues-and-Eigenvectors" class="headerlink" title="1.16 Properties of Eigenvalues and Eigenvectors"></a>1.16 Properties of Eigenvalues and Eigenvectors</h2><p>Let $A\in \mathbb{R}^{n\times n}$ be a square matrix. Then:</p><ul><li>If $Av&#x3D;\lambda v$ for some $\lambda \in \mathbb{R}$, then there also exists a left-eigenvector $\eta \in \mathbb{R}^n$: $\eta^TA &#x3D; \lambda \eta ^T$. Hence $\sigma (A)&#x3D;\sigma (A^T)$.</li><li>The eigenvectors of a matrix $A$ associated with different eigenvalues are linearly independent.</li><li>All eigenvalues $\sigma(A)$ are the roots of the characteristic polynomial equation $det(\lambda I-A)&#x3D;0$. Therefore $det(A)$ is equal to the product of all eigenvalues (some of which may appear multiple times).</li><li>If $B&#x3D;PAP^{-1}$ for some nonsingular matrix $P$, then $\sigma (B)&#x3D;\sigma(A)$.</li><li>If $\lambda \in \mathbb{C}$ is an eigenvalues, then its conjugate $\bar{\lambda}$ is also an eigenvalue. Thus $\sigma (A)&#x3D;\sigma(\bar{A})$ for real matrices $A$.</li></ul><h2 id="1-17-Symmetric-Matrices"><a href="#1-17-Symmetric-Matrices" class="headerlink" title="1.17 Symmetric Matrices"></a>1.17 Symmetric Matrices</h2><p>A matrix $S\in \mathbb{R}^{n\times n}$ is called <code>symmetric</code> if $S^T&#x3D;S$. A symmetric matrix $S$ is called <code>positive semi-definite (denoted by )</code> $S\geq 0$ or $S\succeq 0$. $S$ is called <code>positive definite (denoted by )</code> $S&gt;0$ or $S\succ 0$ if $x^TSx &gt;0 \ \forall x\neq 0$.</p><p>Let $S\in \mathbb{R}^{n\times n}$ be a real symmetric matrix. Then:</p><ul><li>All eigenvalues of $S$ are real, i.e. $\sigma (S) \subset \mathbb{R}$.</li><li>Eigenvectors $v_i$ and $v_j$ of $S$ corresponding to distinct eigenvalues $\lambda_i \neq \lambda_j$ are orthogonal.</li><li>There always exist $n$ orthogonal eogenvectors of $S$ which form a basis of $\mathbb{R}^n$. Let $V&#x3D;(v_1, v_2,…,v_n) \in O(n)$ be the orthogonal matrix of these diagonal matrix of eigenvalues. Then we have $S&#x3D;V\land V^T$.</li><li>$S$ is positive (semi-) definite, if all eigenvalues are positive (nonnegative).</li><li>Let $S$ be positive semi-definite and $\lambda_1, \lambda_n$ are the largest and smallest eigenvalues. Then $\lambda_1&#x3D;max_{|x|&#x3D;1}&lt;x, Sx&gt;$ and $\lambda_n &#x3D; min_{|x|&#x3D;1}&lt;x, Sx&gt;$.</li></ul><h2 id="1-18-Norms-of-Matrices"><a href="#1-18-Norms-of-Matrices" class="headerlink" title="1.18 Norms of Matrices"></a>1.18 Norms of Matrices</h2><p>There are many ways to define norms on the space of matrices $A \in \mathbb{R}^{n \times n}$. They can be defined based on norms on the domain or codomain spaces on which $A$ operates. In particular, the <code>induced 2-norm of a matrix</code> $A$ is defined as<br>$$<br>||A||<em>2 \equiv \max</em>{|x|_2&#x3D;1}|Ax|<em>2&#x3D;\max</em>{|x|_2&#x3D;1}\sqrt{&lt;x, A^TAx&gt;}.<br>$$</p><p>Alternatively, one can define the <code>Frobenius norm of</code> $A$ as:<br>$$<br>||A||<em>t \equiv \sqrt{\sum</em>{i, j}a_{ij}^2}&#x3D;\sqrt{trace(A^TA)}.<br>$$</p><p>Note that these norms are in general not the same. Since the matrix $A^TA$ is symmetric and pos, semi-definite, we can diagonalize it as: $A^TA&#x3D;Vdiag{\sigma_1^2, …,\sigma_n^2}V^T$ with $\sigma_1^2\geq \sigma_i^2 \geq 0$. This leads to:<br>$$<br>||A||_2&#x3D;\sigma_1, \quad and \ ||A||_i &#x3D; \sqrt{trace(A^TA)}&#x3D;\sqrt{\sigma_1^2 + …+\sigma_n^2}.<br>$$</p><h2 id="1-19-Skew-symmetric-Matrices"><a href="#1-19-Skew-symmetric-Matrices" class="headerlink" title="1.19 Skew-symmetric Matrices"></a>1.19 Skew-symmetric Matrices</h2><p>A matrix $A\in \mathbb{R}^{n\times n}$ is called <code>skew-symmetric</code> or <code>anti-symmetric</code> (dt. schiefsymmetrisch) if $A^T&#x3D;-A$.</p><p>If $A$ is a real skew-symmetric matrix, then:</p><ul><li>All eigenvalues of $A$ are either zero or purely imaginary, i.e. of the form $i\omega$ with $i^2&#x3D;-1, \omega \in \mathbb{R}$.</li><li>There exists an orthogonal matrix $V$ such that<br>  $$A &#x3D; V\Lambda V^T,$$<br>  where $\Lambda$ is a block-diagonal matrix $\Lambda&#x3D;diag{A_1, …, A_m,0, …,0}$, with real skew-symmetric matrices $A_i$ of the form:<br>  $$A_i&#x3D;\left(\begin{matrix} 0 \ &amp;a_i \\ -a_i\ &amp;0\end{matrix}\right) \in \mathbb{R}^{2\times 2}, \ i&#x3D;1,…,m.$$</li></ul><p>In particular, the rank of any skew-symmetric matrix is even.</p><h2 id="1-20-Example-of-Skew-symmetric-Matrices"><a href="#1-20-Example-of-Skew-symmetric-Matrices" class="headerlink" title="1.20 Example of Skew-symmetric Matrices"></a>1.20 Example of Skew-symmetric Matrices</h2><p>In Computer Vision, a common skew-symmetric matrix is given by <code>hat operator</code> of a vector $u\in \mathbb{R}^3$ is :<br>$$<br>\hat{u} &#x3D; \left(<br>    \begin{matrix}<br>    0 \ &amp;-u_3 \ &amp;u_2 \\<br>    u_3 \ &amp;0 \ &amp;-u_1\\<br>    -u_2 \ &amp;u_1 \ &amp;0<br>    \end{matrix}\right) \in \mathbb{R}^{3\times 3}.<br>$$</p><p>This is a linear operator from the space of vectors $\mathbb{R}^3$ to the space of skew symmetric matrices in $\mathbb{R}^{3\times 3}$. In particular, the matrix $\hat{u}$ has the property that<br>$$\hat{u}v&#x3D;u\times v,$$<br>where $\times$ denotes the standard vector cross product in $\mathbb{R}^3$. For $u\neq 0$, we have $rank(\hat{u})&#x3D;2$ and the null space of $\hat{u}$ is spanned by $u$, because $\hat{u}u&#x3D;u^T\hat{u}&#x3D;0$.</p><h2 id="1-21-The-Singular-Value-Decomposition-SVD"><a href="#1-21-The-Singular-Value-Decomposition-SVD" class="headerlink" title="1.21 The Singular Value Decomposition (SVD)"></a>1.21 The Singular Value Decomposition (SVD)</h2><p>In the above knowledge, we have studied many properties of matrices, such as rank, range, null space, and included norms of matrices. Many of these properties can be captured by the so-called <code>singular value decomposition (SVD)</code>.</p><p>SVD can be seen as a generalization of eigenvalues and eignvectors to non-square matrices. The computation of SVD is numerically well-conditioned. It is very useful for solving linear-algebraic problems such as matrix inversion, rank computation, linear least-squares estimation, projections, and fixed-rank approximations.</p><p>In practice, both singular value decomposition and eigenvalue decomposition are used quite extensively.</p><h2 id="1-22-Algebraic-Derivation-of-SVD"><a href="#1-22-Algebraic-Derivation-of-SVD" class="headerlink" title="1.22 Algebraic Derivation of SVD"></a>1.22 Algebraic Derivation of SVD</h2><p>Let $A\in \mathbb{R}^{m\times n}$ with $m\geq n$ be a matrix of $rank(A)&#x3D;p$. Then there exist</p><ul><li>$U\in \mathbb{R}^{m\times p}$ whose columns are orthonormal</li><li>$V\in \mathbb{R}^{n\times p}$ whose column are orthonormal, and </li><li>$\Sigma \in \mathbb{R}^{p\times p}$, $\Sigma &#x3D;diag{\sigma_1,…\sigma_p}$, with $\sigma_1 \geq … \geq \sigma_p$,<br>such that<br>$$<br>A &#x3D; U\Sigma V^T<br>$$</li></ul><p>Note that this <code>generalizes the eigenvalue decomposition</code>. While the latter decomposition symmetric square matrix $A$ with an orthogonal transformation $V$ as:<br>$$<br>A &#x3D;V\Lambda V^T, \ with V\in O(n), \ \Lambda&#x3D;diag{\lambda_1, …, \lambda_n},<br>$$</p><p>SVD allows to decompose an arbitrary (non-square) matrix $A$ of rank $p$ with two transformations $U$ and $V$ with orthonormal columns as shown above. Nevertheless, we will see that SVD is based on the eigenvalue decomposiiion of symmetric square matrices.</p><h2 id="1-23-Proof-of-SVD-Decomposition-1"><a href="#1-23-Proof-of-SVD-Decomposition-1" class="headerlink" title="1.23 Proof of SVD Decomposition 1"></a>1.23 Proof of SVD Decomposition 1</h2><p>Given a matrix $A\in \mathbb{R}^{m\times n}$ with $m\geq n$ and $rank(A)&#x3D;p$, the matrix<br>$$A^TA\in \mathbb{R}^{n\times n}$$<br>is symmetric and positive semi-definite. Therefore it can be decomposed with non-negative eigenvalues $\sigma_1^2\geq …\geq \sigma_n^2\geq 0$ with orthonormal eigenvalues $v_1,…,v_n$. The $\sigma_i$ are called <code>singular values</code>. Since<br>$$<br>ker(A^TA)&#x3D;ker(A) \ and \ range(A^TA) &#x3D; range(A^T),<br>$$<br>we have $span{v_1,…,v_p}&#x3D;range(A^T) and \ span{v_{p+1},…,v_n}&#x3D;ker(A)$. Let<br>$$<br>u_i&#x3D;\frac{1}{\sigma_i}Av_i \Leftrightarrow Av_i &#x3D; \sigma_i u_i, \ i&#x3D;1,…,p<br>$$<br>then the $u_i\in \mathbb{R}^m$ are orthonormal:<br>$$<br>&lt;u_i, u_j&gt;&#x3D;\frac{1}{\sigma_i \sigma_j}&lt;Av_i, Av_j&gt;&#x3D;\frac{1}{\sigma_i \sigma_j}&lt;v_i, A^TAv_j&gt;&#x3D;\delta_{ij},<br>$$<br>where<br>$$<br>\delta_{ij} &#x3D; \begin{cases}<br>             1, &amp;  i&#x3D;j\\<br>             0, &amp;i\neq j<br>             \end{cases}<br>$$</p><h2 id="1-23-Proof-of-SVD-Decomposition-2"><a href="#1-23-Proof-of-SVD-Decomposition-2" class="headerlink" title="1.23 Proof of SVD Decomposition 2"></a>1.23 Proof of SVD Decomposition 2</h2><p>Complete ${u_i}<em>{i&#x3D;1}^p$ to a basis ${u_i}</em>{i&#x3D;1}^m$. Since $Av_i&#x3D;\sigma_i u_i$, we have<br>$$<br>A(v_1, …, v_n)&#x3D;(u_1, …, u_m)\left(\begin{matrix}<br>    &amp;\sigma_1  &amp;0 &amp;0  &amp;\cdots  &amp;0 \\<br>    &amp;0 \ &amp;\ddots &amp;0 &amp;\vdots &amp;0 \\<br>    &amp;0 &amp;\cdots &amp;\sigma_p &amp;\vdots &amp;0 \\<br>    &amp;\vdots &amp;\cdots &amp;\cdots &amp;\vdots &amp;0 \\<br>    &amp;0 &amp;\cdots &amp;\cdots &amp;0 &amp;0<br>\end{matrix}\right),<br>$$<br>which is of the form $A\tilde{V}&#x3D;\tilde{U}\tilde{\Sigma}$, thus<br>$$<br>A &#x3D; \tilde{U}\tilde{\Sigma}\tilde{V}^T.<br>$$</p><p>Now simply delete all columns of $\tilde{U}$ and the rows of $\tilde{V}^T$ which are multiplied by zero singular values and we obtain the form $A&#x3D;U\Sigma V^T$, with $U\in \mathbb{R}^{m\times p}$ and $V\in \mathbb{R}^{n\times p}$.<br>In Matlab, <code>[U, S, V]=svd(A)</code>.</p><h2 id="1-24-A-Geomtric-Interpretation-of-SVD"><a href="#1-24-A-Geomtric-Interpretation-of-SVD" class="headerlink" title="1.24 A Geomtric Interpretation of SVD"></a>1.24 A Geomtric Interpretation of SVD</h2><p>For $A\in \mathbb{R}^{n\times n}$, the singular value decomposition $A&#x3D;U\Sigma V^T$ is sunch that the columns $U&#x3D;(u_1, …, u_n)$, and $V &#x3D; (v_1,…,v_n)$ form orthogonal bases of $\hat{n}$. If a point $x\in \mathbb{R}^n$ is mapped to a point $y\in \mathbb{R}^n$ by the transformation $A$, then the coordinates of $y$ in basis $U$ are related to the coordinates of $x$ in basis $V$ by the diagonal matrix $\Sigma:$ each coordinate is merely scaled by the corresponding singular value:<br>$$<br>y&#x3D;Ax &#x3D; U\Sigma V^T x \Leftrightarrow U^T y &#x3D; \Sigma V^T x.<br>$$</p><p><code>The matrix </code>$A$ <code>maps</code> the unit sphere into an ellipsoid with semi-axes $\sigma <em>i u_i$.<br>To see this, we call $\alpha \equiv V^T x$ the coefficients of the point $x$ in the basis $V$ and those of $y$ in basis $U$ shall be called $\beta \equiv U^Ty$. All points of the circle fulfill $|x|^2_2\sum</em>{i}\alpha ^2_i &#x3D; 1$. The above statement says that $\beta _i &#x3D; \sigma_i \alpha_i$. Thus for the points on the sphere we have:<br>$$<br>\sum_i \alpha_i^2 &#x3D; \sum_i \beta^2_i &#x2F; \sigma_i ^2 &#x3D; 1,<br>$$<br>which states that the transformed points lie on an ellipsoid oriented along the axes of the basis $U$.</p><h2 id="1-25-The-Generalized-Moore-Penrose-Inverse"><a href="#1-25-The-Generalized-Moore-Penrose-Inverse" class="headerlink" title="1.25 The Generalized (Moore Penrose) Inverse"></a>1.25 The Generalized (Moore Penrose) Inverse</h2><p>For certain quadratic matrices one can  define an inverse matrix, if $det(A)\neq 0 $. The set of all invertible matrices forms the group $GL(n)$. One can also define a (generalized) inverse (also called pseudo inverse) for an arbirary (non-quadratic) matrix $A\in \mathbb{R}^{m\times n}$. If its SVD is $A&#x3D;U\Sigma V^T$, the pseudo inverse is deined as:<br>$$<br>A^{\dagger} &#x3D; V\Sigma ^{\dagger} U^{T}, \ where \ \Sigma{\dagger}&#x3D;\left(<br>    \begin{aligned} &amp;\Sigma_1^{-1} &amp;0 \\<br>                    &amp;0 &amp;0\end{aligned}\right)_{n\times m}<br>$$<br>where $\Sigma_1$ is the diagonal matrix of non-zero singular values. In Matlab: <code>X=pinv(A)</code>. In particular, the pseudo inverse can be employed in a singular fashion as the inverse of quadratic invertible matrices:<br>$$<br>AA^{\dagger}A&#x3D;A, \quad A^{\dagger}AA^{\dagger}&#x3D;A{\dagger}.<br>$$</p><p>The linear system $Ax&#x3D;b$ with $A\in \mathbb{R}^{m\times n}$ of rank $r\leq \min(m,n)$ can have multiple por no solutions. $x_{min}&#x3D;A^{\dagger}b$ is among all minimizers of $|Ax-b|^2$ the one with the smallest norm $|x|$.</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
          <category> SFM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> SFM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer的前世今生</title>
      <link href="/2021/11/21/Transformer%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"/>
      <url>/2021/11/21/Transformer%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/</url>
      
        <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>Transformer，用过都说好。Transformer统治了NLP的半壁江山后，已经开始大规模蚕食CV的诸多任务了。这篇文章总结了Transformer的诞生和里面的具体细节，作为自己的总结文章bia :raising_hand:。</p><h2 id="1-机器翻译的发展"><a href="#1-机器翻译的发展" class="headerlink" title="1. 机器翻译的发展"></a>1. 机器翻译的发展</h2><p>研究员们为了进行机器翻译，得到你现在使用的Google翻译，百度翻译等等翻译引擎，从1950年就开始了努力。他们尝试过各种各样的方法，比如一大推略显朴素和僵硬的既定规则，或者是设定一些翻译的范例和prototype，又或者使用数据驱动的方式，借助数以万计的翻译数据来设计深度学习模型来进行自动化的翻译。从发展历程来看，我们有了以下这样几种翻译方案：</p><ul><li>「基于规则的机器翻译RBMT」</li><li>「基于范例的机器翻译EBMT」</li><li>「统计机器翻译SMT」</li><li>「神经机器翻译NMT」</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121154536969.png" alt="image-20211121154536969"></p><p>RBMT，EBMT距离我们太远，就不再细细讲述，我们简短回顾一下SMT方案和NMT方案。</p><h3 id="1-1-基于统计的机器翻译"><a href="#1-1-基于统计的机器翻译" class="headerlink" title="1.1 基于统计的机器翻译"></a>1.1 基于统计的机器翻译</h3><p>典型的SMT模型中有基于短语（phase）的PBMT。基于短语的统计翻译，将基本的翻译单元从原来的整句调整到了短语级别，被切分的短语不一定具有任何语法意义，在歧义消除，局部排序，解码效率上有一定的优势，降低了机器翻译系统的所要面对的复杂度，表现出了较好的模型健壮性，常常作为统计机器翻译系统研究的baseline。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121155758648.png" alt="image-20211121155758648" style="zoom:50%;" /><h3 id="1-2-基于神经网络的机器翻译"><a href="#1-2-基于神经网络的机器翻译" class="headerlink" title="1.2 基于神经网络的机器翻译"></a>1.2 基于神经网络的机器翻译</h3><p>神经机器翻译，顾名思义，就是借助神经网络进行翻译模型的学习。翻译模型的输入是一个自然语言的句子，输入网络前这条句子被我们使用分词+word Embedding表达成一个带有时序信息的向量序列，然后经过网路的翻译之后，输出target方语言中word Embedding的一个概率分布，进而得到翻译好的句子。由此可见一般的神经机器翻译的套路。当我们借助大量数据来驱动这样一个模型训练时，输入的源语言和期望的目标语言这两个独立的向量序列构成了一个pair，然后使用损失函数就能够驱动模型进行学习的优化，指导模型收敛，能够将源语言下的句子翻译成比较好的目标语言。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/rnn-animate.gif" alt="rnn-animate" style="zoom:67%;" /><h2 id="2-Seq2Seq模型的发展"><a href="#2-Seq2Seq模型的发展" class="headerlink" title="2. Seq2Seq模型的发展"></a>2. Seq2Seq模型的发展</h2><p>在诸多可以用来学习数据特性的深度学习模型中，非常适合处理拥有时序特性的数据的模型有RNN，LSTM，GRU…，他们在序列数据建模中大放异彩。就神经翻译框架来看，外形是差不多的，只是用来编码和解码的基础构件不同。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/d0icm-0nvdd.gif" alt="d0icm-0nvdd" style="zoom:50%;" /><h3 id="2-1-基础的RNN"><a href="#2-1-基础的RNN" class="headerlink" title="2.1 基础的RNN"></a>2.1 基础的RNN</h3><p>R(Recurent)NN其实有好几种villain版本。</p><ul><li>One-to-one</li></ul><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121163857635.png" alt="image-20211121163857635" style="zoom:50%;" /><p>也就是最基本的单层网络，输入是x，经过Wx+b和激活函数f得到输出y。</p><ul><li>one-to-n</li></ul><p>输入不是一个序列，但是输出是序列的情况。输入的注入方式可以只在序列开始时注入，也可以是每个时间步都将输入注入模型。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121164211589.png" alt="image-20211121164211589" style="zoom:50%;" /><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121164233669.png" alt="image-20211121164233669" style="zoom:40%;" /><p>这种one-to-n的结构可以处理的问题有：</p><ol><li>:honeybee:image caption「从图像生成文字」，此时输入的X就是图像中抽取的特征，而输出的y序列就是一段句子。</li><li>:first_quarter_moon:从类别生成语音或者音乐。</li></ol><ul><li>n-to-n</li></ul><p>最为经典的RNN结构，输入和输出都是等长的序列数据。假设输入X&#x3D;(x1, x2, x3, x4)，每个x是一个单词的词向量。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121165119281.png" alt="image-20211121165119281" style="zoom:30%;" /><p>为了对序列数据进行建模，RNN引入了隐状态h(hidden state)的概念，h可以对序列形的数据提取特征，接着再转换为输出。先从h1的计算开始看：</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121165301935.png" alt="image-20211121165301935" style="zoom:40%;" /><p>h2的计算和h1类似。Note：在计算时，每一个时间步使用的参数U，W，b都是重复使用的，这也是Recurent的由来，也就是说没个时间步中的参数都是共享的，这是RNN的重要特性。按照相同方式一次计算剩下来的隐状态（使用相同的U，W，b）：</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121165606769.png" alt="image-20211121165606769" style="zoom:37%;" /><p>为了方便描述。在这里只画出序列长度为4的情况，实际上，You can play it all day。得到输出值的方法就是直接通过h进行计算：</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121165740947.png" alt="image-20211121165740947" style="zoom:40%;" /><p><strong>一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1。</strong>剩下的输出类似的进行（使用相同和y1相同的参数V和c）。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121165851729.png" alt="image-20211121165851729" style="zoom:40%;" /><p>这就是最经典的RNN结构，它的输入是x1, x2, …..xn，输出为y1, y2, …yn，也就是说，<strong>输入和输出序列必须要是等长的</strong>。由于这个限制的存在，经典RNN的适用范围比较小，但也有一些问题适合用经典的RNN结构建模，如：</p><ol><li>:eagle:计算视频中每一帧的分类标签，这种情况下输入输出序列的时间长度相同。</li><li>:rabbit2:输入为字符，输出为下一个字符的概率。这就是著名的CharRNN（The Unreasonable Effectiveness of Recurrent Neural <a href="https://link.zhihu.com/?target=http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Network</a>），可以用来生成文章，诗歌，甚至是代码。</li></ol><ul><li>n-to-one</li></ul><p>要处理的问题输入是一个序列，输出是一个单独的值而不是序列，应该怎样建模呢？实际上，我们只在最后一个h上进行输出变换就可以了：</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121170239796.png" alt="image-20211121170239796" style="zoom:45%;" /><p>这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。</p><ul><li>n-to-m</li></ul><p>最后一种<strong>n-to-m</strong>的范式，就是久负盛名的<strong>Encoder-Decoder</strong>，它的输入和输出的序列长度不相等。Encoder-Decoder也叫做<strong>Seq2Seq</strong>。在机器翻译中，源语言和目标语言的句子长度往往不相同，为此，<strong>Encoder-Decoder结构先将输入数据编码成一个上下文语义向量c：</strong></p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121170710429.png" alt="image-20211121170710429" style="zoom:50%;" /><p><strong>语义向量c</strong>可以有多种表达方式，最朴素的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后一个隐状态做些后续的变换得到c，也可以对所有的隐状态做变换。<strong>拿到c之后，就用另一个RNN网络对其进行解码</strong>，这部分RNN网络被称为Decoder。Decoder的RNN可以与Encoder的一样，也可以不一样。具体做法就是将c当做之前的初始状态h0输入到Decoder中：</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121170926202.png" alt="image-20211121170926202" style="zoom:50%;" /><p>还有一种做法是将c当作每一步的输入：</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121170956481.png" alt="image-20211121170956481" style="zoom:50%;" /><h3 id="2-2-Encoder-Decoder的应用"><a href="#2-2-Encoder-Decoder的应用" class="headerlink" title="2.2 Encoder-Decoder的应用"></a>2.2 Encoder-Decoder的应用</h3><p>由于这种Encoder-Decoder的结构不限制输入和输出的序列长度，因此应用范围非常广泛，比如：</p><ol><li>:walking:机器翻译：Encoder-Decoder的最经典应用，事实上，Seq2Seq结构就是机器翻译领域最先提出的。</li><li>:earth_asia:文本摘要：输入是一段文本序列，输出是这段文本序列的摘要序列。</li><li>:taco:阅读理解：将输入的文章和问题分别编码，再对其进行解码得到问题的答案。</li><li>:yum:语音识别：输入是语音信号序列，输出是文字序列。</li></ol><h3 id="2-3-Encoder-Decoder框架"><a href="#2-3-Encoder-Decoder框架" class="headerlink" title="2.3 Encoder-Decoder框架"></a>2.3 Encoder-Decoder框架</h3><p>Encoder-Decoder不是一个具体的模型，而是一种框架。</p><ul><li>Encoder：将输入的序列向量转换为固定长度的Embedding</li><li>Decoder：将固定长度的Embedding转换为Output序列</li><li>Encoder与Decoder可以彼此独立使用，实际上经常一起使用。</li></ul><p>因为Seq2Seq最早出现在机器翻译领域，最早被广泛使用的基模型是RNN，其实基础模型可以是RNN,CNN,BiRNN,LSTM,GRU…</p><h3 id="2-4-Encoder-Decoder缺点"><a href="#2-4-Encoder-Decoder缺点" class="headerlink" title="2.4 Encoder-Decoder缺点"></a>2.4 Encoder-Decoder缺点</h3><ul><li>最大的局限性：Encoder和Decoder之间的唯一联系是固定长度的语义Embedding<strong>c</strong>。</li><li>编码器要将整个序列的信息压缩进一个固定长度的语义向量<strong>c</strong>。</li><li>语义向量<strong>c</strong>无法完全表达整个序列的信息。</li><li>先输入的内容携带的信息会被后来的信息稀释掉，或者覆盖掉，因为输入的序列长度太长时，编码器无法将所有输入的信息保留。</li><li>输入序列越长，这样的现象越严重，这样使得在Decoder解码时一开始就没有办法获得足够的输入序列，解码效果就会大打折扣。</li></ul><p>因此为了弥补基础的Encoder- Decoder的缺陷，提出了Attention机制，来在很长的输入序列中建立Attention，建立近、中远程的token依赖关系，捕捉重点信息。</p><h3 id="2-4-加入attention后"><a href="#2-4-加入attention后" class="headerlink" title="2.4 加入attention后"></a>2.4 加入attention后</h3><p>注意力机制是对基础Encoder-Decoder的改良。Attention机制通过在每个时间输入不同的<strong>c</strong>来解决问题，下图是带有Attention机制的Decoder：</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211121221045984.png" alt="image-20211121221045984" style="zoom:50%;" /><p>上图中的<strong>c</strong>不再简单的是Encoder最后输出的一个隐状态了，而是通过一个加权矩阵，将Encoder中所有时间步的隐状态$h_i$进行了加权整合，这个加权矩阵由Encoder中的n个隐状态$h_i$和Decoder中的m个隐状态$h_j^‘$计算相似度得出，是一个$n\times m$​的矩阵。<strong>具体来说，我们用该矩阵中的元素aij衡量Encoder中第j阶段的hj和解码时第i阶段的相关性，最终Decoder中第i阶段的输入的上下文信息 ci就来自于所有 hj 对 aij 的加权和</strong>。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211122161627809.png" alt="image-20211122161627809" style="zoom:40%;" /><p>举个栗子，在上图中翻英的过程中，输入的中文序列的长度假设为4，输出的英文序列的长度假设为3，那么在Encoder和Decoder中分别会得到4个和3个隐状态，我们将其进行相似度的度量后，拿到了attention矩阵，然后将Encoder部分的隐状态通过这个attention矩阵进行加权，得到真正的往Decoder中送入的输入**$c_1$​,$ c_2$​, $c_3$​**。整个过程就是这样的啦，可以看到加了Attention矩阵后，就算是再长的序列，我们也能将其中的信息通过attention矩阵很好的考虑进来。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211122162913371.png" alt="image-20211122162913371" style="zoom:40%;" /><h3 id="2-5-attention的优点"><a href="#2-5-attention的优点" class="headerlink" title="2.5 attention的优点"></a>2.5 attention的优点</h3><ol><li>在机器翻译时，可以让生词不只是关注全局的语义向量c，由于增加了注意力机制，可以让接下来的输出重点关注输入序列中的一部分，根据注意力区域来获得此时的输出。</li><li>不再要求Encoder将所有信息都编码到一个固定的global的向量中啦。</li><li>将输入编码成向量序列，Decoder时，每一步选择性的从序列中挑选一个子集进行处理。</li><li>在输出序列的每一个时间步上，借助attention从Encoder的诸多隐状态中整理出最有助于当前输出节点的语义向量$c_i$。也就是说，学习到的注意力是会随着上下文进行变化的。</li></ol><h3 id="2-6-attention的缺点"><a href="#2-6-attention的缺点" class="headerlink" title="2.6 attention的缺点"></a>2.6 attention的缺点</h3><ol><li>需要为每个输入输出组合分别计算attention。50个单词的输出输出序列需要计算2500个attention。</li><li>attention在决定专注于某个方面之前需要遍历一遍记忆再决定下一个输出是以什么。</li></ol><h2 id="3-Encoder-Decoder-Attention和Transformer的关系"><a href="#3-Encoder-Decoder-Attention和Transformer的关系" class="headerlink" title="3. Encoder+Decoder+Attention和Transformer的关系"></a>3. Encoder+Decoder+Attention和Transformer的关系</h2><h3 id="3-1-基础款RNN做Encoder-Decoder的缺陷"><a href="#3-1-基础款RNN做Encoder-Decoder的缺陷" class="headerlink" title="3.1 基础款RNN做Encoder- Decoder的缺陷"></a>3.1 基础款RNN做Encoder- Decoder的缺陷</h3><p>看起来Encoder- Decoder+Attention已经无敌了，可Transformer黄雀在后，进行了改进就表明基础编码器RNN是有改进空间的。</p><p>细细想一想RNN处理数据的模式，输入一个序列向量，要反复重用同一个参数组进行前传，就意味着，RNN处理序列数据时，现阶段只能是串行处理，即上一个时间点的输入被编码好之后，才能去处理当前时间点以及后续时间点的输入，这样的处理方式，让RNN在面对序列数据时丧失了高效的处理速度。或许，只有在编码时不去依靠先前时间步的输入能够让串行数据得到并行处理，而Transformer就是这样做的。Transformer在编码阶段将序列向量分离编码，然后在编码结束后，统一计算self- attention进行注意力的计算，然后再统一将计算得到的注意力作用到先前的分离编码上。这样的手段让串行数据向量能够在编码的同时，还能兼顾到上下文这个概念。相比起RNN反复recurrent的处理数据的手段，参数量是变大了，但确实能够并行处理了序列数据了。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211122164729221.png" alt="image-20211122164729221" style="zoom:50%;" /><h3 id="3-2-Transformer做Encoder-Decoder的优势"><a href="#3-2-Transformer做Encoder-Decoder的优势" class="headerlink" title="3.2 Transformer做Encoder- Decoder的优势"></a>3.2 Transformer做Encoder- Decoder的优势</h3><p>说到这，Transformer的优势就不言而喻了。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/Transformer_decoder.png" alt="Transformer_decoder" style="zoom:67%;" /><table><thead><tr><th>RNN+Attention+ Seq2Seq</th><th>Transformer</th></tr></thead><tbody><tr><td>Encoder中的隐状态承接上文信息，通过串行处理为上下文建模</td><td>Encoder中通过self-attention为输入序列中建立attention，以此为上下文建模</td></tr><tr><td>DEcoder中的隐状态承接上文信息，通过串行处理为上下文建模</td><td>Decoder中通过self-attention为输出序列中建立attention，以此为上下文建模</td></tr><tr><td>Encoder- Decoder通过计算attention来为输入序列的编码和当前输出建立注意力</td><td>Encoder- Decoder通过encoder-decoder-attention为输入序列编码和当前输出建立编解码注意力</td></tr></tbody></table><p>以Transformer为Encoder和Decoder的NMT系统，基本上可以分为6个步骤：</p><ol><li>Encoder为输入序列里的每个token产生原始的embedding，在上图中用空圈表示。</li><li>利用self-attention将输入序列中所有token的信息进行汇总，先计算得到attention，然后以此为权重，重新矫正加权过的每个token的embedding，上图中用实圈表示。</li><li>Encoder重复N次self- attention，让每个token持续学习到完整的上下文语义。</li><li>Decoder在生成输出文字时也运用了self-attention，关注自己之前时刻点已经生成的元素，将其上文信息纳入生成后续元素的过程中。</li><li>在Encoder和Decoder各自捕捉到上下文信息后Decoder接着利用attention关注Encoder的所有输出，并计算Encoder编码与Decoder之间的attention，并把这种连接Encoder和Decoder的注意力用来帮助生成当前时刻点的输出。「其实这个注意力才是上文RNN- based的Seq2Seq模型中加入的attention。」</li><li>Decoder重复步骤4，5，来让当前元素包含到更完整的整体语义。</li></ol><h2 id="4-Transformer的内部运行机制"><a href="#4-Transformer的内部运行机制" class="headerlink" title="4. Transformer的内部运行机制"></a>4. Transformer的内部运行机制</h2><h3 id="4-1-Transformer的Input和position-Encoder"><a href="#4-1-Transformer的Input和position-Encoder" class="headerlink" title="4.1 Transformer的Input和position Encoder"></a>4.1 Transformer的Input和position Encoder</h3><ol><li><p>Input的表示方法</p><p>​    当我们进行机器翻译时，我们的输入输出是自然语言，所以第一步是要将语言表述成数学形式。</p><ul><li>1-of-N embedding。这种方式是在一个封闭的词库里面，将出现的每个单词进行one-hot表述。</li><li>word class。将相同类的单词放在一个class。（表示方式依然比较粗糙）</li><li>word Embedding。把不同的词用一个vector表示，vector的每一维隐式的表示物体的某种性质。word embedding希望相同性质的词的embedding能够聚的近一些，不同性质的词的embedding能够离得远一些，换言之，word embedding其实是一种soft的word class。</li></ul></li><li><p>位置信息编码</p><p>​    为什么要加入位置编码？我们的输入是一段带有时序信息的序列向量，但如果就这样把它放进Transformer的Encoder中进行特征的重表示，attention计算等等操作，它是会对序列信息不敏感的，换句话说，无论句子的结构怎么打乱，Transformer都会得到类似的结果。为了解决这个问题，就需要将位置信息显式的注入进输入。</p><p>​    位置编码也有绝对位置编码和相对位置编码等几种形式，在这里仅介绍绝对位置编码，通常这个位置编码的长度是和输入的序列向量同维的，这样方便位置编码和词向量逐位相加。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211122201305924.png" alt="image-20211122201305924" style="zoom: 67%;" /><p>具体公式如下：<br>$$<br>PE(pos, 2i)&amp;&#x3D;sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\<br>PE(pos, 2i+1)&amp;&#x3D; cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})<br>$$<br>在公式中，pos表示单词的位置，i表示单词的维度，关于位置编码的实现可以参看Google的实现<a href="https://link.zhihu.com/?target=https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py">get_timing_masignal_1d()</a>函数找到对应代码。作者使用正余弦编码位置，是根据公式$sin(\alpha + \beta)&#x3D;sin\alpha cos\beta + cos\alpha sin\beta$​以及$cos(\alpha+\beta)&#x3D;cos\alpha cos\beta -sin\alpha sin\beta$​，这表明位置 k+p的位置向量可以表示为位置k的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。</p></li></ol><h3 id="4-2-Transformer的Encoder"><a href="#4-2-Transformer的Encoder" class="headerlink" title="4.2 Transformer的Encoder"></a>4.2 Transformer的Encoder</h3><ol><li><p>输入再编码</p><p>我们的输入数据X的维度是[batch_size, sequence_length]，经过上面的词嵌入（word2Vec）后，我们得到的输入的size是[batch_size, sequence_length, embedding_dimension]，embedding_dimension取决于Word2Vec算法，Transformer中采用的是512长度的字向量。比如我们得到了一些输入，他的shape是[3, 10, 512]，Transformer的Encoder分别使用三个shape为[embedding_dimension, embedding_dimension]参数可学习权重矩阵Weight来分别乘以输入，将其再次通过LinearLayer对输入进行编码，这样我们就得到了所谓的Q（query），K（key），V（value）向量。</p></li><li><p>self-attention</p><p>通过第一步，我们得到了序列中的每个token的Q、K、V向量。然后Q向量和K向量先两两做内积，计算相似度，然后被$\sqrt {d_k}$​​进行归一化（$d_k$​是Q向量的维度长度），这样就得到了一个weight matrix，里面存了score，然后将这个score进行softMax处理，得到[0, 1]的一个分布，这时候的矩阵就是Attention了。然后将attention乘以V向量，就得到了包含了上下文信息的输出value。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211123185651042.png" alt="image-20211123185651042" style="zoom:50%;" />$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt {d_k}})V$$</li><li><p>归一化（layer Normalization）+ skipConnection</p><p>由self-attention整合完上下文的信息后，得到输出后，要进行layer Normalization。在处理序列数据时，通常使用的是layer Normalization，而非Batch Normalization，因为序列数据通常长短不一，不同batch数据之间的gap通常存在于序列的长短中，而使用layer Normalization就能够越过长度不同这个问题。skipConnection是帮助网络收敛的惯用伎俩，即把输入短接到输出位置。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211123170513273.png" alt="image-20211123170513273" style="zoom:30%;" /></li><li><p>MLP（Feed Forward Network）</p><p>上图中Transformer- Encoder的最后一脚是feed Forward层，就是MLP，就是linear layer。他的作用是把向量的维度再次进行变化，尤其是我们下面将要说到的Multi-head Attention中会用到它来compress特征维数。</p></li><li><p>Multi Head Attention</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/multihead.jpeg" alt="multihead" style="zoom:20%;" /><p>Multi- head- attention就是用h套参数组去做上面的事情（包括input再编码，self-attention计算及使用，Layer Normalization），然后将得到的h套结果concatenate到一起之后，使用Feed Forward Layer进行维度压缩，就是这样啦。</p></li></ol><h3 id="4-3-Transformer的Decoder"><a href="#4-3-Transformer的Decoder" class="headerlink" title="4.3 Transformer的Decoder"></a>4.3 Transformer的Decoder</h3><ol><li><p>输入再编码</p><p>目标的输出序列也要进行再编码，假如输入的数据是[batch_size,  seq_length]–&gt;[3, 7]–&gt;[batch_size, seq_length, embedding_dimension]–&gt;[3, 7, 512]，而期望的输出数据是[batch_size,  seq_length]–&gt;[3, 13]–&gt;[batch_size, seq_length, embedding_dimension]–&gt;[3, 13, 512]，这时先要使用三个权重矩阵进行再编码。</p></li><li><p>输入mask处理attention</p><p>经过第一步处理后，得到序列向量的编码后，计算self-attention时，由于下文要输出的信息还未生成，只有上文的信息能够参与到attention中，所以要使用mask来mute掉attention矩阵中的下文关注，如下图中的Masked Multi-Head Attention所示。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211123182823830.png" alt="image-20211123182823830" style="zoom:30%;" /></li><li><p>用Encoder-Decoder-attention矫正输入编码</p><p>和经典款的Seq2Seq模型中的attention关系最接近的就是Encoder-Decoder-attention了，他是Encoder与Decoder之间的关系和桥梁。用来度量输入序列向量编码和输出序列向量编码之间的alignment程度，即把输出序列向量作为key，去查询输入序列向量，得到attention后，通过attention聚合出能够最能帮助得到准确输出的V（value）。</p></li></ol><h3 id="4-4-Transformer的Training-Process-amp-amp-Inference-Process"><a href="#4-4-Transformer的Training-Process-amp-amp-Inference-Process" class="headerlink" title="4.4 Transformer的Training Process&amp;&amp;Inference Process"></a>4.4 Transformer的Training Process&amp;&amp;Inference Process</h3><ul><li><p>当进行测试时，Input Embedding可以完整的作为Encoder的输入进行特征表示，但是Decoder的输入侧所需要的Output Embedding需要从token**<start><strong>作为开始输入，经过前向后，Decoder得到了下一个单词，然后将</strong><start><strong>和该单词连接共同作为Decoder的输入，再进行前向过程，经过一番for循环后，当Decoder得到的输出是</strong><eos/>**时，翻译就完成了。</p></li><li><p>训练Transformer时，上图中的Input Embedding和Output Embedding都是完整的句子，但在Decoder中要注意为attention加mask来模拟当下想要的输出结果只能依靠上文信息来得到。其实训练过程应当是和测试过程保持一致，但为了减少训练过程的时间，我们做了并行化处理，例如我们的完整的OutputEmbedding是[[start], i , love, maching, learning] ，我们会乘一个下三角矩阵，得到的是：<br>[[start]<br>[start] i<br>[start] i love<br>[start] i love maching<br>[start] i love maching learning]，训练阶段将这个矩阵直接作为GT，当作Output Embedding直接输入Decoder，分别可以得到5个输出$O_i, i\in[1,2,3,4,5]$​，但理想的输出应该是[i, love, maching learning, [eos&#x2F;]，然后在$O_i$​和理想输出间计算交叉熵损失，反传梯度驱动模型参数更新。</p></li></ul><h3 id="4-5-常见的Transformer大模型"><a href="#4-5-常见的Transformer大模型" class="headerlink" title="4.5 常见的Transformer大模型"></a>4.5 常见的Transformer大模型</h3><ul><li>「BERT」<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers。其实就是大规模语料库在预训练任务上训练Transformer的Encoder。</li><li>「GPT」<strong>G</strong>enerative <strong>P</strong>re-<strong>T</strong>raining。其实就是Transformer的Decoder。后来的GPT2可以做阅读理解，Summarization， Translation等，GPT3最近也开放了api接口的申请名单，但是没有中国昂。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
          <category> 人工智能 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vison Transformer系列文章解读（一）</title>
      <link href="/2021/11/21/Vison-Transformer%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2021/11/21/Vison-Transformer%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>x x x</p><h2 id="1-Transformer快速回顾"><a href="#1-Transformer快速回顾" class="headerlink" title="1. Transformer快速回顾"></a>1. Transformer快速回顾</h2><p>我们先来简单而快速的回顾一下Transformer的构成。和串处理数据的Seq2Seq模型不同，Transformer在处理序列数据时，能够做到并行，这也使得Transformer在Seq2Seq模型中快速脱颖而出。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/transformer.png" alt="transformer" style="zoom:60%;" /><h3 id="1-1-Encoder侧输入数据详情"><a href="#1-1-Encoder侧输入数据详情" class="headerlink" title="1.1 Encoder侧输入数据详情"></a>1.1 Encoder侧输入数据详情</h3><p>假设上图中的Encoder侧的inputs是一个[$ x^1, x^2, x^3, x^4$]的序列，该序列长度为4，而每个时间点上的输入数据$ x^i \in \R^{d_{model}}$，这样一个shape为[seq_len, $ d_{model}$]的tensor进入到input Embedding中，通过乘以三种不同的可学习权重矩阵$ W_q, W_k, W_v$，其中$W_q \in \mathbb{R}^{d_{model}\times dimension}, W_k \in \R^{d_{model}\times dimension}, W_v \in \R^{d_{model}\times dimension}$，然后就可以将原来$d_{model}$维的序列数据转换转换为$dimension $维。值得注意的是，我们在实现transformer的Encoder过程中，为了极大程度的使用矩阵并行计算的优点，我们会将$W_q,W_k, W_v$矩阵和在一起进行统一计算，得到结果后，再将tensor进行split。同样的道理，当我们会用到multi-head的attention时，（假如我们有k个头）我们也不会分离的做一次，在去继续做k-1次，而是统一把所有头的参数集中放在一起，统一经过矩阵运算得到结果后，再去把结果split成k个头。就这样，我们可以把输入数据进行初次的Linear Layer变换。但这样变换完成后，序列数据中的先后顺序和空间位置信息被丢失了，即把输入数据[$x^1, x^2, x^3, x^4$]打乱顺序也能得到一样的结果，但很明显，这样的位置不敏感是不正确的，因此，我们下面需要为embedding添加位置编码信息进去。</p><h3 id="1-2-Encoder侧位置编码"><a href="#1-2-Encoder侧位置编码" class="headerlink" title="1.2 Encoder侧位置编码"></a>1.2 Encoder侧位置编码</h3><p>对于位置的编码方式，我们可以用绝对位置编码，相对位置编码和可学习的位置编码。</p><ul><li><p>绝对位置编码<br>$$<br>PositionEmbedding(pos, 2i) &#x3D; sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \<br>PositionEmbedding(pos, 2i+1) &#x3D; cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})<br>$$<br>上式中$pos \in [0, seqlen)$​，表示token在序列中的位置；$i\in[0, d_{model})$​​，表示embedding的各个维度的索引。</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211129185724397.png" alt="image-20211129185724397" style="zoom:80%;" /></li><li><p>相对位置编码</p><p>相对位置并没有完整建模每个输入的位置信息，而是在算Attention的时候考虑当前位置与被Attention的位置的相对距离，由于自然语言一般更依赖于相对位置，所以相对位置编码通常也有着优秀的表现。相对位置编码也有好几类，分为bias偏置类和contextual上下文类等几种。</p><ul><li><p>contextual上下文型相对位置<br>$$<br>z_i &#x3D; \sum_{j&#x3D;1}^n \alpha_{ij}(x_jW^V + p_{ij}^V)\<br>\alpha_{ij} &#x3D; \frac{(x_i W^Q+p_{ij}^Q)(x_j W^K+p_{ij}^K)^T}{\sqrt{d_z}}<br>$$<br>上式中，$z_i$​表示token第$i$​个位置的输出向量，$\alpha_{ij}$​表示token第i个位置和第j个位置的attention，$x_j\in\R^{d_{model}}$​是第j个位置的token的向量值，$W^V\in \R^{d_{model}\times dimension}$​​表示将输入embedding转换为value的可学习权重矩阵。$p_{ij}^V \in \R^{dimension}$​​表示value侧的相对位置值向量，它是指第i个token和第j个token的相对位置clip(i-j, max_k)。</p></li><li><p>bias偏置型相对位置编码<br>$$<br>\alpha_{ij}&#x3D;\frac{(x_i W^Q)(x_j W^K)^T + b_{ij}}{\sqrt{d_z}}<br>$$<br>上式中，相对位置编码和输入完全无关，是单独计算出attention矩阵后，再将相对位置编码加到attention中。</p></li></ul><p>一般而言，上下文型的相对位置编码的效果要稍比偏置型的相对位置编码好一些。</p></li><li><p>可学习的位置编码</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211129201119567.png" alt="image-20211129201119567" style="zoom:40%;" /><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211129201151144.png" alt="image-20211129201151144" style="zoom:40%;" /></li></ul><h3 id="1-3-Encoder侧self-attention计算方式"><a href="#1-3-Encoder侧self-attention计算方式" class="headerlink" title="1.3 Encoder侧self-attention计算方式"></a>1.3 Encoder侧self-attention计算方式</h3><ul><li><p>self attention</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211129205336426.png" alt="image-20211129205336426" style="zoom:40%;" /></li><li><p>attention乘以value</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211129205540745.png" alt="image-20211129205540745" style="zoom:40%;" /></li><li><p>并行计算$b_1, b_2, b_3, b_4$:</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211129205727932.png" alt="image-20211129205727932" style="zoom:40%;" /><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211129205900105.png" alt="image-20211129205900105" style="zoom:40%;" /><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211129205952554.png" alt="image-20211129205952554" style="zoom:40%;" /></li><li><p>whole process of Encoder layer of Transformer</p><img src="https://cdn.jsdelivr.net/gh/Wastoon/myPicGo/img/image-20211129210103751.png" alt="image-20211129210103751" style="zoom:40%;" /></li></ul><h3 id="1-4-Decoder侧mask-self-attention计算方式"><a href="#1-4-Decoder侧mask-self-attention计算方式" class="headerlink" title="1.4 Decoder侧mask-self-attention计算方式"></a>1.4 Decoder侧mask-self-attention计算方式</h3><p>Decoder侧在训练时，我们的输出侧的序列数据的样例一般是[[start] $x_1, x_2, x_3, x_4, x_5$]，按照和输入侧的attention一样的计算方式，我们可以得到一个$6\times 6$的attention，但是考虑到下文的数据不应该对上文结果预测带来影响，因此，我们要在attention矩阵中添加一个target_mask矩阵，该矩阵是一个下三角矩阵，下三角处为0，上三角处为负的极大值如-100000，然后将target_mask矩阵和计算得到的attention相加，就可以得到真正的，不关注下文信息的attention了，接着进行正常的softmax和ratio_scale，再和value相乘，就得到了最后的输出value。整体的计算思路是先计算，再进行mask掩盖。</p><h3 id="1-5-Encoder-Decoder-attention计算方式"><a href="#1-5-Encoder-Decoder-attention计算方式" class="headerlink" title="1.5 Encoder- Decoder- attention计算方式"></a>1.5 Encoder- Decoder- attention计算方式</h3><p>在具体实现过程中，我们使用的其实还是和encoder侧相同的self-attention计算函数，只是调用的query, key, value不同了。在计算encoder-decoder-attention时，我们query来自于decoder输出的value值，key和value来自encoder的输出value值，然后进行正常计算就好咯。</p><h3 id="1-6-Decoder输出结果接分类头"><a href="#1-6-Decoder输出结果接分类头" class="headerlink" title="1.6 Decoder输出结果接分类头"></a>1.6 Decoder输出结果接分类头</h3><p>Decoder的输出value经过了transformer内部的ffn(全连接层)映射，但要进行word的分类时，还需要最后接一个全连接层Linear Layer将ffn层后feature的dimension转换为target_vocab_size，比如target的单词中加上特殊的[start]，[end]，[pad]，[unknown]等token后总共有6023个，那就是要进行6023个类别的分类，经过最后的linearLayer映射后，使用cross Entropy Loss即可以计算损失了。</p><h3 id="1-7-Decoder侧mask在训练和测试时的异同"><a href="#1-7-Decoder侧mask在训练和测试时的异同" class="headerlink" title="1.7 Decoder侧mask在训练和测试时的异同"></a>1.7 Decoder侧mask在训练和测试时的异同</h3><ul><li>在训练时，我们的Decoder侧的target_mask的size是固定的，因为整个target语言的句子的最大长度我们在训练数据集中能够得到，因此可以直接先使用pad将所有target语言的句子补齐成为一样的长度，然后生成target_mask来掩盖无效的attention。</li><li>在测试时，我们的Decoder侧的输序列是随着翻译进度而逐渐增加的，从最开始只有一个[start]的token到后面翻译的target单词越来越多，因此测试时的target_mask是实时根据Decoder侧输入序列数据的长度来变化的。</li></ul><h2 id="2-首个将Transformer引入计算机视觉任务的DETR"><a href="#2-首个将Transformer引入计算机视觉任务的DETR" class="headerlink" title="2. 首个将Transformer引入计算机视觉任务的DETR"></a>2. 首个将Transformer引入计算机视觉任务的DETR</h2><h3 id="2-1-Encoder侧输入图像序列化处理"><a href="#2-1-Encoder侧输入图像序列化处理" class="headerlink" title="2.1 Encoder侧输入图像序列化处理"></a>2.1 Encoder侧输入图像序列化处理</h3><h3 id="2-2-Decoder侧序列输入理解"><a href="#2-2-Decoder侧序列输入理解" class="headerlink" title="2.2 Decoder侧序列输入理解"></a>2.2 Decoder侧序列输入理解</h3><h3 id="2-3-Decoder侧输出的使用"><a href="#2-3-Decoder侧输出的使用" class="headerlink" title="2.3 Decoder侧输出的使用"></a>2.3 Decoder侧输出的使用</h3><h3 id="2-4-检测任务头的使用"><a href="#2-4-检测任务头的使用" class="headerlink" title="2.4 检测任务头的使用"></a>2.4 检测任务头的使用</h3><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ButterFly的标签语法糖</title>
      <link href="/2021/11/17/ButterFly%E7%9A%84%E6%A0%87%E7%AD%BE%E8%AF%AD%E6%B3%95%E7%B3%96/"/>
      <url>/2021/11/17/ButterFly%E7%9A%84%E6%A0%87%E7%AD%BE%E8%AF%AD%E6%B3%95%E7%B3%96/</url>
      
        <content type="html"><![CDATA[<p>##自定义文本块</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note no-icon %&#125;</span><br><span class="line">你是刷 Visa 还是 UnionPay</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note blue no-icon %&#125;</span><br><span class="line">2021年快到了....</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note pink no-icon %&#125;</span><br><span class="line">小心开车 安全至上</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note red no-icon %&#125;</span><br><span class="line">这是三片呢？还是四片？</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note orange no-icon %&#125;</span><br><span class="line">你是刷 Visa 还是 UnionPay</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note purple no-icon %&#125;</span><br><span class="line">剪刀石头布</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line">&#123;% note green no-icon %&#125;</span><br><span class="line">前端最讨厌的浏览器</span><br><span class="line">&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><div class="note no-icon flat"><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue no-icon flat"><p>2021年快到了….</p></div><div class="note pink no-icon flat"><p>小心开车 安全至上</p></div><div class="note red no-icon flat"><p>这是三片呢？还是四片？</p></div><div class="note orange no-icon flat"><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple no-icon flat"><p>剪刀石头布</p></div><div class="note green no-icon flat"><p>前端最讨厌的浏览器</p></div><p>##复选列表</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;% checkbox 纯文本测试 %&#125;</span><br><span class="line">&#123;% checkbox checked, 支持简单的 [<span class="string">markdown</span>](<span class="link">https://guides.github.com/features/mastering-markdown/</span>) 语法 %&#125;</span><br><span class="line">&#123;% checkbox red, 支持自定义颜色 %&#125;</span><br><span class="line">&#123;% checkbox green checked, 绿色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox yellow checked, 黄色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox cyan checked, 青色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox blue checked, 蓝色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox plus green checked, 增加 %&#125;</span><br><span class="line">&#123;% checkbox minus yellow checked, 减少 %&#125;</span><br><span class="line">&#123;% checkbox times red checked, 叉 %&#125;</span><br></pre></td></tr></table></figure><div class='checkbox'><input type="checkbox" />            <p>纯文本测试</p>            </div><div class='checkbox checked'><input type="checkbox" checked="checked"/>            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>            </div><div class='checkbox red'><input type="checkbox" />            <p>支持自定义颜色</p>            </div><div class='checkbox green checked'><input type="checkbox" checked="checked"/>            <p>绿色 + 默认选中</p>            </div><div class='checkbox yellow checked'><input type="checkbox" checked="checked"/>            <p>黄色 + 默认选中</p>            </div><div class='checkbox cyan checked'><input type="checkbox" checked="checked"/>            <p>青色 + 默认选中</p>            </div><div class='checkbox blue checked'><input type="checkbox" checked="checked"/>            <p>蓝色 + 默认选中</p>            </div><div class='checkbox plus green checked'><input type="checkbox" checked="checked"/>            <p>增加</p>            </div><div class='checkbox minus yellow checked'><input type="checkbox" checked="checked"/>            <p>减少</p>            </div><div class='checkbox times red checked'><input type="checkbox" checked="checked"/>            <p>叉</p>            </div><p>##单选列表 radio</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;% radio 纯文本测试 %&#125;</span><br><span class="line">&#123;% radio checked, 支持简单的 [<span class="string">markdown</span>](<span class="link">https://guides.github.com/features/mastering-markdown/</span>) 语法 %&#125;</span><br><span class="line">&#123;% radio red, 支持自定义颜色 %&#125;</span><br><span class="line">&#123;% radio green, 绿色 %&#125;</span><br><span class="line">&#123;% radio yellow, 黄色 %&#125;</span><br><span class="line">&#123;% radio cyan, 青色 %&#125;</span><br><span class="line">&#123;% radio blue, 蓝色 %&#125;</span><br></pre></td></tr></table></figure><div class='checkbox'><input type="radio" />            <p>纯文本测试</p>            </div><div class='checkbox checked'><input type="radio" checked="checked"/>            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>            </div><div class='checkbox red'><input type="radio" />            <p>支持自定义颜色</p>            </div><div class='checkbox green'><input type="radio" />            <p>绿色</p>            </div><div class='checkbox yellow'><input type="radio" />            <p>黄色</p>            </div><div class='checkbox cyan'><input type="radio" />            <p>青色</p>            </div><div class='checkbox blue'><input type="radio" />            <p>蓝色</p>            </div><p>##行内文本样式 text</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 带 &#123;% u 下划线 %&#125; 的文本</span><br><span class="line"><span class="bullet">2.</span> 带 &#123;% emp 着重号 %&#125; 的文本</span><br><span class="line"><span class="bullet">3.</span> 带 &#123;% wavy 波浪线 %&#125; 的文本</span><br><span class="line"><span class="bullet">4.</span> 带 &#123;% del 删除线 %&#125; 的文本</span><br><span class="line"><span class="bullet">5.</span> 键盘样式的文本 &#123;% kbd command %&#125; + &#123;% kbd D %&#125;</span><br><span class="line"><span class="bullet">6.</span> 密码样式的文本：&#123;% psw 这里没有验证码 %&#125;</span><br></pre></td></tr></table></figure><ol><li>带 <u>下划线</u> 的文本</li><li>带 <emp>着重号</emp> 的文本</li><li>带 <wavy>波浪线</wavy> 的文本</li><li>带 <del>删除线</del> 的文本</li><li>键盘样式的文本 <kbd>command</kbd> + <kbd>D</kbd></li><li>密码样式的文本：<psw>这里没有验证码</psw></li></ol><p>##行内文本 span</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 彩色文字</span><br><span class="line">  在一段话中方便插入各种颜色的标签，包括：&#123;% span red, 红色 %&#125;、&#123;% span yellow, 黄色 %&#125;、&#123;% span green, 绿色 %&#125;、&#123;% span cyan, 青色 %&#125;、&#123;% span blue, 蓝色 %&#125;、&#123;% span gray, 灰色 %&#125;。</span><br><span class="line"><span class="bullet">-</span> 超大号文字</span><br><span class="line">  文档「开始」页面中的标题部分就是超大号文字。</span><br><span class="line">  &#123;% span center logo large, Volantis %&#125;</span><br><span class="line">  &#123;% span center small, A Wonderful Theme for Hexo %&#125;</span><br></pre></td></tr></table></figure><ul><li>彩色文字<br>在一段话中方便插入各种颜色的标签，包括：<span class='p red'>红色</span>、<span class='p yellow'>黄色</span>、<span class='p green'>绿色</span>、<span class='p cyan'>青色</span>、<span class='p blue'>蓝色</span>、<span class='p gray'>灰色</span>。</li><li>超大号文字<br>文档「开始」页面中的标题部分就是超大号文字。<span class='p center logo large'>Volantis</span><span class='p center small'>A Wonderful Theme for Hexo</span></li></ul><p>##时间轴 timeline</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;% timeline %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% timenode 2020-07-24 %&#125;</span><br><span class="line"></span><br><span class="line">天气不错，适合出去约妹</span><br><span class="line"></span><br><span class="line">&#123;% endtimenode %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% timenode 2020-05-15 %&#125;</span><br><span class="line"></span><br><span class="line">下雨了，适合在房间</span><br><span class="line"></span><br><span class="line">&#123;% endtimenode %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% timenode 2020-04-20 %&#125;</span><br><span class="line"></span><br><span class="line">有空一起拉屎</span><br><span class="line"></span><br><span class="line">&#123;% endtimenode %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endtimeline %&#125;</span><br></pre></td></tr></table></figure><div class="timeline"><div class="timenode"><div class="meta"><p><p>2020-07-24</p></p></div><div class="body"><p>天气不错，适合出去约妹</p></div></div><div class="timenode"><div class="meta"><p><p>2020-05-15</p></p></div><div class="body"><p>下雨了，适合在房间</p></div></div><div class="timenode"><div class="meta"><p><p>2020-04-20</p></p></div><div class="body"><p>有空一起拉屎</p></div></div></div><p>##链接卡片 link</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% link GitHub, https://github.com, https://cdn.jsdelivr.net/gh/cpddo/p<span class="emphasis">_img@450ea647ca67bd386416a689f3eb1bc6a508b3b9/2021/01/23/f7ac7b26db76ada1704f6af09bedacbe.webp %&#125;</span></span><br></pre></td></tr></table></figure><div class="tag link"><a class="link-card" title="GitHub" href="https://github.com"><div class="left"><img src="https://cdn.jsdelivr.net/gh/cpddo/p_img@450ea647ca67bd386416a689f3eb1bc6a508b3b9/2021/01/23/f7ac7b26db76ada1704f6af09bedacbe.webp"/></div><div class="right"><p class="text">GitHub</p><p class="url">https://github.com</p></div></a></div><p>##按钮 btns<br>###1. 一组含有头像的链接：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns circle grid5 %&#125;</span><br><span class="line">&#123;% cell GitHub, https://github.com/, https://cdn.jsdelivr.net/gh/cpddo/p<span class="emphasis">_img@450ea647ca67bd386416a689f3eb1bc6a508b3b9/2021/01/23/f7ac7b26db76ada1704f6af09bedacbe.webp %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% cell 哔哩哔哩, https://www.bilibili.com/, https://cdn.jsdelivr.net/gh/cpddo/p_</span>img@e642ee265c8ae2bbd0994716aa12b3adbe07f2c4/2021/01/23/2ceca69a212d3b9988bbd2c41edc636c.webp %&#125;</span><br><span class="line">&#123;% cell Pixiv, https://www.pixiv.net/, https://cdn.jsdelivr.net/gh/cpddo/p<span class="emphasis">_img@5c4fc20944c706aa452c31d1bddbdcc672e8c6ab/2021/01/23/7658d06315d32bcf0c954b3d8e8879e0.webp %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% cell YouTube, https://www.youtube.com/, https://cdn.jsdelivr.net/gh/cpddo/p_</span>img@ff4781678ea6227f5824e3c8bfd5cc27441db4da/2021/01/23/f4e292c780275465c9150eb3cb0785a4.webp %&#125;</span><br><span class="line">&#123;% cell 今日热榜, https://tophub.today/, https://cdn.jsdelivr.net/gh/cpddo/p<span class="emphasis">_img@11fff6ed270722d709eb0ac88ce47f468c21d2ba/2021/01/23/cd22cd9d34d7d7bd58114d7d7a195822.webp %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% endbtns %&#125;</span></span><br></pre></td></tr></table></figure><div class="btns circle grid5">            <a class="button" href='https://github.com/' title='GitHub'><img src='https://cdn.jsdelivr.net/gh/cpddo/p_img@450ea647ca67bd386416a689f3eb1bc6a508b3b9/2021/01/23/f7ac7b26db76ada1704f6af09bedacbe.webp'>GitHub</a><a class="button" href='https://www.bilibili.com/' title='哔哩哔哩'><img src='https://cdn.jsdelivr.net/gh/cpddo/p_img@e642ee265c8ae2bbd0994716aa12b3adbe07f2c4/2021/01/23/2ceca69a212d3b9988bbd2c41edc636c.webp'>哔哩哔哩</a><a class="button" href='https://www.pixiv.net/' title='Pixiv'><img src='https://cdn.jsdelivr.net/gh/cpddo/p_img@5c4fc20944c706aa452c31d1bddbdcc672e8c6ab/2021/01/23/7658d06315d32bcf0c954b3d8e8879e0.webp'>Pixiv</a><a class="button" href='https://www.youtube.com/' title='YouTube'><img src='https://cdn.jsdelivr.net/gh/cpddo/p_img@ff4781678ea6227f5824e3c8bfd5cc27441db4da/2021/01/23/f4e292c780275465c9150eb3cb0785a4.webp'>YouTube</a><a class="button" href='https://tophub.today/' title='今日热榜'><img src='https://cdn.jsdelivr.net/gh/cpddo/p_img@11fff6ed270722d709eb0ac88ce47f468c21d2ba/2021/01/23/cd22cd9d34d7d7bd58114d7d7a195822.webp'>今日热榜</a>          </div><p>###2. 含有图标的按钮：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns rounded grid5 %&#125;</span><br><span class="line">&#123;% cell 下载源码, /, fas fa-download %&#125;</span><br><span class="line">&#123;% cell 查看文档, /, fas fa-book-open %&#125;</span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><div class="btns rounded grid5">            <a class="button" href='/' title='下载源码'><i class='fas fa-download'></i>下载源码</a><a class="button" href='/' title='查看文档'><i class='fas fa-book-open'></i>查看文档</a>          </div><p>###3. 圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns circle center grid5 %&#125;</span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1&#x27;</span>&gt;</span></span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&#x27;fab fa-apple&#x27;</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="name">b</span>&gt;</span></span>心率管家<span class="xml"><span class="tag">&lt;/<span class="name">b</span>&gt;</span></span></span><br><span class="line">  &#123;% p red, 专业版 %&#125;</span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png&#x27;</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1&#x27;</span>&gt;</span></span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&#x27;fab fa-apple&#x27;</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="name">b</span>&gt;</span></span>心率管家<span class="xml"><span class="tag">&lt;/<span class="name">b</span>&gt;</span></span></span><br><span class="line">  &#123;% p green, 免费版 %&#125;</span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png&#x27;</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><div class="btns circle center grid5">            <a href='https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1'>  <i class='fab fa-apple'></i>  <b>心率管家</b>  <p class='p red'>专业版</p>  <img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_pro.png'></a><a href='https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1'>  <i class='fab fa-apple'></i>  <b>心率管家</b>  <p class='p green'>免费版</p>  <img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/heartmate_lite.png'></a>          </div><p>##github 卡片 ghcard<br>###1. 用户信息卡片</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| &#123;% ghcard jerryc127 %&#125;               | &#123;% ghcard Zfour, theme=vue %&#125;              |</span><br><span class="line">| ------------------------------------ | ------------------------------------------ |</span><br><span class="line">| &#123;% ghcard Akilarlxh, theme=buefy %&#125;  | &#123;% ghcard ruanyf, theme=solarized-light %&#125; |</span><br><span class="line">| &#123;% ghcard philwebb, theme=onedark %&#125; | &#123;% ghcard zjwo, theme=solarized-dark %&#125;    |</span><br><span class="line">| &#123;% ghcard vpavic, theme=algolia %&#125;   | &#123;% ghcard bclozel, theme=calm %&#125;           |</span><br></pre></td></tr></table></figure><table><thead><tr><th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/jerryc127"><img src="https://github-readme-stats.vercel.app/api/?username=jerryc127&show_owner=true"/></a></th><th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/Zfour"><img src="https://github-readme-stats.vercel.app/api/?username=Zfour&theme=vue&show_owner=true"/></a></th></tr></thead><tbody><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/Akilarlxh"><img src="https://github-readme-stats.vercel.app/api/?username=Akilarlxh&theme=buefy&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/ruanyf"><img src="https://github-readme-stats.vercel.app/api/?username=ruanyf&theme=solarized-light&show_owner=true"/></a></td></tr><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/philwebb"><img src="https://github-readme-stats.vercel.app/api/?username=philwebb&theme=onedark&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/zjwo"><img src="https://github-readme-stats.vercel.app/api/?username=zjwo&theme=solarized-dark&show_owner=true"/></a></td></tr><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/vpavic"><img src="https://github-readme-stats.vercel.app/api/?username=vpavic&theme=algolia&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/bclozel"><img src="https://github-readme-stats.vercel.app/api/?username=bclozel&theme=calm&show_owner=true"/></a></td></tr></tbody></table><p>###2. 仓库信息卡片</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| &#123;% ghcard spring-projects/spring-boot %&#125;                 | &#123;% ghcard mybatis/mybatis-3, theme=vue %&#125;               |</span><br><span class="line">| -------------------------------------------------------- | ------------------------------------------------------- |</span><br><span class="line">| &#123;% ghcard jerryc127/hexo-theme-butterfly, theme=buefy %&#125; | &#123;% ghcard alibaba/fastjson, theme=solarized-light %&#125;    |</span><br><span class="line">| &#123;% ghcard alibaba/druid, theme=onedark %&#125;                | &#123;% ghcard alibaba/arthas, theme=solarized-dark %&#125;       |</span><br><span class="line">| &#123;% ghcard Tencent/weui, theme=algolia %&#125;                 | &#123;% ghcard volantis-x/hexo-theme-volantis, theme=calm %&#125; |</span><br></pre></td></tr></table></figure><table><thead><tr><th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/spring-projects/spring-boot"><img src="https://github-readme-stats.vercel.app/api/pin/?username=spring-projects&repo=spring-boot&show_owner=true"/></a></th><th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/mybatis/mybatis-3"><img src="https://github-readme-stats.vercel.app/api/pin/?username=mybatis&repo=mybatis-3&theme=vue&show_owner=true"/></a></th></tr></thead><tbody><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly"><img src="https://github-readme-stats.vercel.app/api/pin/?username=jerryc127&repo=hexo-theme-butterfly&theme=buefy&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/alibaba/fastjson"><img src="https://github-readme-stats.vercel.app/api/pin/?username=alibaba&repo=fastjson&theme=solarized-light&show_owner=true"/></a></td></tr><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/alibaba/druid"><img src="https://github-readme-stats.vercel.app/api/pin/?username=alibaba&repo=druid&theme=onedark&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/alibaba/arthas"><img src="https://github-readme-stats.vercel.app/api/pin/?username=alibaba&repo=arthas&theme=solarized-dark&show_owner=true"/></a></td></tr><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/Tencent/weui"><img src="https://github-readme-stats.vercel.app/api/pin/?username=Tencent&repo=weui&theme=algolia&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=calm&show_owner=true"/></a></td></tr></tbody></table><p>##github 徽标 ghbdage<br>###1. 基本参数,定义徽标左右文字和图标</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage Theme,Butterfly %&#125;</span><br><span class="line">&#123;% bdage Frame,Hexo,hexo %&#125;</span><br></pre></td></tr></table></figure><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/Butterfly-Theme-orange?logo=&color=orange&link=&"></object><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/Hexo-Frame-orange?logo=hexo&color=orange&link=&"></object><p>###2. 信息参数，定义徽标右侧内容背景色，指向链接</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage CDN,JsDelivr,jsDelivr||abcdef,https://metroui.org.ua/index.html,本站使用JsDelivr为静态资源提供CDN加速 %&#125;</span><br><span class="line">// 如果是跨顺序省略可选参数，仍然需要写个逗号,用作分割</span><br><span class="line">&#123;% bdage Source,GitHub,GitHub||,https://github.com/ %&#125;</span><br></pre></td></tr></table></figure><object class="ghbdage" style="margin-inline:5px" title="本站使用JsDelivr为静态资源提供CDN加速" standby="loading..." data="https://img.shields.io/badge/JsDelivr-CDN-orange?logo=jsDelivr&color=abcdef&link=https://metroui.org.ua/index.html&"></object><p>&#x2F;&#x2F; 如果是跨顺序省略可选参数，仍然需要写个逗号,用作分割</p><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/GitHub-Source-orange?logo=GitHub&color=orange&link=https://github.com/&"></object><p>###3. 拓展参数，支持shields的API的全部参数内容</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage Hosted,Vercel,Vercel||brightgreen,https://vercel.com/,本站采用双线部署，默认线路托管于Vercel||style=social&amp;logoWidth=20 %&#125;</span><br><span class="line">// 如果是跨顺序省略可选参数组，仍然需要写双竖线||用作分割</span><br><span class="line">&#123;% bdage Hosted,Vercel,Vercel||||style=social&amp;logoWidth=20&amp;logoColor=violet %&#125;</span><br></pre></td></tr></table></figure><object class="ghbdage" style="margin-inline:5px" title="本站采用双线部署，默认线路托管于Vercel" standby="loading..." data="https://img.shields.io/badge/Vercel-Hosted-orange?logo=Vercel&color=brightgreen&link=https://vercel.com/&style=social&logoWidth=20"></object><p>&#x2F;&#x2F; 如果是跨顺序省略可选参数组，仍然需要写双竖线||用作分割</p><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/Vercel-Hosted-orange?logo=Vercel&color=orange&link=&style=social&logoWidth=20&logoColor=violet"></object><p>##行内图片 inlineimage</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">你看我长得漂亮不</span><br><span class="line"></span><br><span class="line">我觉得很漂亮 &#123;% inlineImg https://7.dusays.com/2021/04/30/38c46d77eb99f.png 150px %&#125;</span><br></pre></td></tr></table></figure><p>你看我长得漂亮不</p><p>我觉得很漂亮 <img class="inline-img" src="https://7.dusays.com/2021/04/30/38c46d77eb99f.png" style="height:150px"/></p><p>##单张图片 image</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg, width=400px, alt=每天下课回宿舍的路，没有什么故事。 %&#125;</span><br></pre></td></tr></table></figure><div class="img-wrap"><div class="img-bg"><img class="img" src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg" alt="每天下课回宿舍的路，没有什么故事。" style="width:400px;"/></div><span class="image-caption">每天下课回宿舍的路，没有什么故事。</span></div><p>##视频 video<br>###1. 100% 宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/2d10a99a5285890815076699337/jDSbeLEbAvgA.mp4 %&#125;</span><br></pre></td></tr></table></figure><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/2d10a99a5285890815076699337/jDSbeLEbAvgA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><p>###2. 50% 宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% videos, 2 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/c271d9f55285890808619247572/cvQ5JmaxQeIA.mp4 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/8373c2325285890808618842182/0G2tZMSgFlIA.mp4 %&#125;</span><br><span class="line">&#123;% video https://bos.nj.bpc.baidu.com/tieba-smallvideo-transcode-crf/10517287<span class="emphasis">_a019c0a4655b865403740b7b9d1f0622_</span>0.mp4 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/62034a6a5285890805262027097/2ZZqs2WH2HUA.mp4 %&#125;</span><br><span class="line">&#123;% endvideos %&#125;</span><br></pre></td></tr></table></figure><div class="videos" col='2'><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/c271d9f55285890808619247572/cvQ5JmaxQeIA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/8373c2325285890808618842182/0G2tZMSgFlIA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://bos.nj.bpc.baidu.com/tieba-smallvideo-transcode-crf/10517287_a019c0a4655b865403740b7b9d1f0622_0.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/62034a6a5285890805262027097/2ZZqs2WH2HUA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div></div><p>###3. 25% 宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;% videos, 4 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/dbb2338d5285890805087414392/iPOPAzDcziQA.mp4 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/dbaff9015285890805087410150/hFeFC3ppTLsA.mp4 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/b6ccab4b5285890805073207494/crb6Lmf26tQA.mp4 %&#125;</span><br><span class="line">&#123;% video https://sf1-ttcdn-tos.pstatp.com/obj/tos-cn-v-0004/aeacaeb49b1a4bf483d93356091fad60 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/3c5d29755285890805529884127/jgf48juQ7PoA.mp4 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/63f968535285890808730645862/Gg7ng1DpoJwA.mp4 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/562fa8ea5285890808730073912/cAJgQKS0gL8A.mp4 %&#125;</span><br><span class="line">&#123;% video https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/5d54d76e5285890808730396152/z0aYxeAto3QA.mp4 %&#125;</span><br><span class="line">&#123;% endvideos %&#125;</span><br></pre></td></tr></table></figure><div class="videos" col='4'><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/dbb2338d5285890805087414392/iPOPAzDcziQA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/dbaff9015285890805087410150/hFeFC3ppTLsA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/b6ccab4b5285890805073207494/crb6Lmf26tQA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://sf1-ttcdn-tos.pstatp.com/obj/tos-cn-v-0004/aeacaeb49b1a4bf483d93356091fad60' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/3c5d29755285890805529884127/jgf48juQ7PoA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/63f968535285890808730645862/Gg7ng1DpoJwA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/562fa8ea5285890808730073912/cAJgQKS0gL8A.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://1251316161.vod2.myqcloud.com/007a649dvodcq1251316161/5d54d76e5285890808730396152/z0aYxeAto3QA.mp4' type='video/mp4'>Your browser does not support the video tag.</video></div></div><p>##折叠框 folding</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&#123;% folding 查看图片测试 %&#125;</span><br><span class="line">![<span class="string">images</span>](<span class="link">https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg</span>)</span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding cyan open, 查看默认打开的折叠框 %&#125;</span><br><span class="line">这是一个默认打开的折叠框。</span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding green, 查看代码测试 %&#125;</span><br><span class="line">​<span class="code">```java</span></span><br><span class="line"><span class="code">public class Test &#123;</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">&#125;</span></span><br><span class="line"><span class="code">​```</span></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding yellow, 查看列表测试 %&#125;</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> haha</span><br><span class="line"><span class="bullet">-</span> hehe</span><br><span class="line">  &#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding red, 查看嵌套测试 %&#125;</span><br><span class="line">&#123;% folding blue, 查看嵌套测试2 %&#125;</span><br><span class="line">&#123;% folding 查看嵌套测试3 %&#125;</span><br><span class="line">hahaha😜</span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line">&#123;% endfolding %&#125;</span><br></pre></td></tr></table></figure><details ><summary> 查看图片测试 </summary>              <div class='content'>              <p><img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg" alt="images"></p>              </div>            </details><details cyan open><summary> 查看默认打开的折叠框 </summary>              <div class='content'>              <p>这是一个默认打开的折叠框。</p>              </div>            </details><details green><summary> 查看代码测试 </summary>              <div class='content'>              <p>​&#96;&#96;&#96;java<br>public class Test {</p><p>}<br>​&#96;&#96;&#96;</p>              </div>            </details><details yellow><summary> 查看列表测试 </summary>              <div class='content'>              <ul><li>haha</li><li>hehe</li></ul>              </div>            </details><details red><summary> 查看嵌套测试 </summary>              <div class='content'>              <details blue><summary> 查看嵌套测试2 </summary>              <div class='content'>              <details ><summary> 查看嵌套测试3 </summary>              <div class='content'>              <p>hahaha😜</p>              </div>            </details>              </div>            </details>              </div>            </details><p>##分栏 tab</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test1 %&#125;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 1.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 2.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 3.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><div class="tabs" id="test1"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test1-1">test1 1</button></li><li class="tab"><button type="button" data-href="#test1-2">test1 2</button></li><li class="tab"><button type="button" data-href="#test1-3">test1 3</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test1-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test1-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test1-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><p>##诗词标签 poem</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% poem 滕王阁序,王勃 %&#125;</span><br><span class="line">飞流直下三万尺，疑是银河落十天。</span><br><span class="line">&#123;% endpoem %&#125;</span><br></pre></td></tr></table></figure><div class='poem'><div class='poem-title'>滕王阁序</div><div class='poem-author'>王勃</div><p>豫章故郡，洪都新府。星分翼轸，地接衡庐。襟三江而带五湖，控蛮荆而引瓯越。物华天宝，龙光射牛斗之墟；人杰地灵，徐孺下陈蕃之榻。雄州雾列，俊采星驰。台隍枕夷夏之交，宾主尽东南之美。都督阎公之雅望，棨戟遥临；宇文新州之懿范，襜帷暂驻。十旬休假，胜友如云；千里逢迎，高朋满座。腾蛟起凤，孟学士之词宗；紫电青霜，王将军之武库。家君作宰，路出名区；童子何知，躬逢胜饯。(豫章故郡 一作：南昌故郡；青霜 一作：清霜)</p><p>　　时维九月，序属三秋。潦水尽而寒潭清，烟光凝而暮山紫。俨骖騑于上路，访风景于崇阿。临帝子之长洲，得天人之旧馆。层峦耸翠，上出重霄；飞阁流丹，下临无地。鹤汀凫渚，穷岛屿之萦回；桂殿兰宫，即冈峦之体势。（层峦 一作：层台；即冈 一作：列冈；飞阁流丹 一作：飞阁翔丹）</p><p>　　披绣闼，俯雕甍，山原旷其盈视，川泽纡其骇瞩。闾阎扑地，钟鸣鼎食之家；舸舰弥津，青雀黄龙之舳。云销雨霁，彩彻区明。落霞与孤鹜齐飞，秋水共长天一色。渔舟唱晚，响穷彭蠡之滨，雁阵惊寒，声断衡阳之浦。(轴 通：舳；迷津 一作：弥津；云销雨霁，彩彻区明 一作：虹销雨霁，彩彻云衢)</p><p>　　遥襟甫畅，逸兴遄飞。爽籁发而清风生，纤歌凝而白云遏。睢园绿竹，气凌彭泽之樽；邺水朱华，光照临川之笔。四美具，二难并。穷睇眄于中天，极娱游于暇日。天高地迥，觉宇宙之无穷；兴尽悲来，识盈虚之有数。望长安于日下，目吴会于云间。地势极而南溟深，天柱高而北辰远。关山难越，谁悲失路之人；萍水相逢，尽是他乡之客。怀帝阍而不见，奉宣室以何年？(遥襟甫畅 一作：遥吟俯畅)</p><p>　　嗟乎！时运不齐，命途多舛。冯唐易老，李广难封。屈贾谊于长沙，非无圣主；窜梁鸿于海曲，岂乏明时？所赖君子见机，达人知命。老当益壮，宁移白首之心？穷且益坚，不坠青云之志。酌贪泉而觉爽，处涸辙以犹欢。北海虽赊，扶摇可接；东隅已逝，桑榆非晚。孟尝高洁，空余报国之情；阮籍猖狂，岂效穷途之哭！(见机 一作：安贫；以犹欢 一作：而相欢)</p><p>　　勃，三尺微命，一介书生。无路请缨，等终军之弱冠；有怀投笔，慕宗悫之长风。舍簪笏于百龄，奉晨昏于万里。非谢家之宝树，接孟氏之芳邻。他日趋庭，叨陪鲤对；今兹捧袂，喜托龙门。杨意不逢，抚凌云而自惜；钟期既遇，奏流水以何惭？</p><p>　　呜呼！胜地不常，盛筵难再；兰亭已矣，梓泽丘墟。临别赠言，幸承恩于伟饯；登高作赋，是所望于群公。敢竭鄙怀，恭疏短引；一言均赋，四韵俱成。请洒潘江，各倾陆海云尔：<br>　　滕王高阁临江渚，佩玉鸣鸾罢歌舞。<br>　　画栋朝飞南浦云，珠帘暮卷西山雨。<br>　　闲云潭影日悠悠，物换星移几度秋。<br>　　阁中帝子今何在？槛外长江空自流。</p></div><p>##label<br>臣亮言：<mark class="hl-label default">先帝</mark> 创业未半，而<mark class="hl-label blue">中道崩殂</mark> 。今天下三分，<mark class="hl-label pink">益州疲敝</mark> ，此诚<mark class="hl-label red">危急存亡之秋</mark> 也！然侍衞之臣，不懈于内；<mark class="hl-label purple">忠志之士</mark> ，忘身于外者，盖追先帝之殊遇，欲报之于陛下也。诚宜开张圣听，以光先帝遗德，恢弘志士之气；不宜妄自菲薄，引喻失义，以塞忠谏之路也。<br>宫中、府中，俱为一体；陟罚臧否，不宜异同。若有<mark class="hl-label orange">作奸</mark> 、<mark class="hl-label green">犯科</mark> ，及为忠善者，宜付有司，论其刑赏，以昭陛下平明之治；不宜偏私，使内外异法也。</p><p>##进度条</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% progress 10 red 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 30 yellow 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 50 green 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 70 cyan 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 90 blue 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 100 gray 进度条样式预览 %&#125;</span><br></pre></td></tr></table></figure><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-red"  style="width: 10%" aria-valuenow="10" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-yellow"  style="width: 30%" aria-valuenow="30" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-green"  style="width: 50%" aria-valuenow="50" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-cyan"  style="width: 70%" aria-valuenow="70" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-blue"  style="width: 90%" aria-valuenow="90" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-gray"  style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><p>##button</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">This is my website, click the button &#123;% btn &#x27;https://a-wei-y.top/&#x27;,MyBlog %&#125;</span><br><span class="line">This is my website, click the button &#123;% btn &#x27;https://a-wei-y.top/&#x27;,MyBlog,far fa-hand-point-right %&#125;</span><br><span class="line">This is my website, click the button &#123;% btn &#x27;https://a-wei-y.top/&#x27;,MyBlog,,outline %&#125;</span><br><span class="line">This is my website, click the button &#123;% btn &#x27;https://a-wei-y.top/&#x27;,MyBlog,far fa-hand-point-right,outline %&#125;</span><br><span class="line">This is my website, click the button &#123;% btn &#x27;https://a-wei-y.top/&#x27;,MyBlog,far fa-hand-point-right,larger %&#125;</span><br></pre></td></tr></table></figure><p>This is my website, click the button <a class="btn-beautify button--animated " href="https://a-wei-y.top/"   title="MyBlog"><span>MyBlog</span></a><br>This is my website, click the button <a class="btn-beautify button--animated " href="https://a-wei-y.top/"   title="MyBlog"><i class="far fa-hand-point-right"></i><span>MyBlog</span></a><br>This is my website, click the button <a class="btn-beautify button--animated outline" href="https://a-wei-y.top/"   title="MyBlog"><span>MyBlog</span></a><br>This is my website, click the button <a class="btn-beautify button--animated outline" href="https://a-wei-y.top/"   title="MyBlog"><i class="far fa-hand-point-right"></i><span>MyBlog</span></a><br>This is my website, click the button <a class="btn-beautify button--animated larger" href="https://a-wei-y.top/"   title="MyBlog"><i class="far fa-hand-point-right"></i><span>MyBlog</span></a></p>]]></content>
      
      
      <categories>
          
          <category> 博客美化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ButterFly标签 </tag>
            
            <tag> 博客正文美化 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
